<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://markusthill.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://markusthill.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-24T17:27:41+00:00</updated><id>https://markusthill.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal notes and code snippets on math, ML &amp; programming. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Hexadecimal Digit Canon Challenge</title><link href="https://markusthill.github.io/blog/2026/pe-785p100-hexadecimal-digit-canon/" rel="alternate" type="text/html" title="The Hexadecimal Digit Canon Challenge"/><published>2026-01-17T14:00:51+00:00</published><updated>2026-01-17T14:00:51+00:00</updated><id>https://markusthill.github.io/blog/2026/pe-785p100-hexadecimal-digit-canon</id><content type="html" xml:base="https://markusthill.github.io/blog/2026/pe-785p100-hexadecimal-digit-canon/"><![CDATA[<p>In the Hexadecimal Canon, every number is first stripped of all insignificant zeros and reordered into its most ‚Äúpure‚Äù form. The value of a number is not what it looks like at first glance, but what remains after this canonical purification.</p> <p>All numbers in this problem are written in <strong>base 16 (hexadecimal)</strong>. The available digits are</p> \[0,1,2,3,4,5,6,7,8,9,\text{A},\text{B},\text{C},\text{D},\text{E},\text{F},\] <p>with \(\text{A}=10,\dots,\text{F}=15.\)</p> <p><br/></p> <h3 id="the-digitcanonical-function">The Digit‚ÄìCanonical Function</h3> <p>For any <strong>positive hexadecimal integer</strong> \(d &gt; 0\), define the function \(f(d)\) as follows:</p> <ol> <li>Write \(d\) in hexadecimal.</li> <li>Sort its hexadecimal digits in <strong>ascending order</strong>.</li> <li>Remove <strong>all zero digits</strong>.</li> <li>Interpret the remaining digits again as a hexadecimal number.</li> </ol> <p>Since $d &gt; 0$, removing zero digits always leaves at least one hexadecimal digit.</p> <p><br/></p> <h3 id="examples">Examples</h3> <ul> <li> <p><strong>Example 1:</strong> \(d = \texttt{0x3A04}\). Digits: \(3, A, 0, 4.\) Sorted: \(0, 3, 4, A.\) Remove zeros ‚Üí \(3, 4, A\). Result: \(f(\texttt{0x3A04}) = \texttt{0x34A}\)</p> </li> <li> <p><strong>Example 2</strong> \(d = \texttt{0xF102}\). Digits: \(F, 1, 0, 2\). Sorted: \(0, 1, 2, F\). Remove zeros ‚Üí \(1, 2, F\). Result: \(f(\texttt{0xF102}) = \texttt{0x12F}\)</p> </li> <li> <p><strong>Example 3</strong> \(d = \texttt{0x800}\). Digits: \(8, 0, 0\). Sorted: \(0, 0, 8\). Remove zeros ‚Üí \(8\). Result: \(f(\texttt{0x800}) = \texttt{0x8}\)</p> </li> <li> <p><strong>Example 4:</strong> $d = \texttt{0xA11B0}$. Digits: $A,1,1,B,0$. Sorted: $0,1,1,A,B$. Remove zeros ‚Üí $1,1,A,B$. Result: $f(\texttt{0xA11B0}) = \texttt{0x11AB}$.</p> </li> </ul> <p><br/></p> <h3 id="the-cumulative-sum">The Cumulative Sum</h3> <p>Let \(S(n)\) denote the sum of \(f(d)\) over <strong>all positive hexadecimal integers with at most \(n\) hexadecimal digits</strong>.</p> <p>For example:</p> <ul> <li>For \(n = \texttt{0x1}\), the result is \(S(\texttt{0x1}) = \texttt{0x78}\)</li> <li>For \(n = \texttt{0x3}\), the result is \(S(\texttt{0x3}) = \texttt{0x4077C0}\)</li> </ul> <blockquote> <p><strong>Important:</strong> All values of \(S(n)\) are to be reported <strong>in hexadecimal notation</strong>.</p> </blockquote> <p><br/></p> <h2 id="challenge-levels-">Challenge Levels üèÖ</h2> <p>Compute \(S(n)\) for the following four difficulty tiers:</p> <table> <thead> <tr> <th>Tier</th> <th>Value of \(n\) (hexadecimal)</th> <th>Your Result (hexadecimal)</th> </tr> </thead> <tbody> <tr> <td>üéó Warm-up</td> <td>\(n = \; \texttt{0x5}\)</td> <td>\(S(n) = \;\_\_\_\_\_\_\_\_\_\_\_\_\)</td> </tr> <tr> <td>ü•â Bronze</td> <td>\(n = \; \texttt{0xA}\)</td> <td>\(S(n) = \;\_\_\_\_\_\_\_\_\_\_\_\_\)</td> </tr> <tr> <td>ü•à Silver</td> <td>\(n = \; \texttt{0xAA}\)</td> <td>\(S(n) = \;\_\_\_\_\_\_\_\_\_\_\_\_\, \,\) (first &amp; last 10 hex digits)</td> </tr> <tr> <td>ü•á Gold</td> <td>\(n = \; \texttt{0xAAA}\)</td> <td>\(S(n) \equiv \;\_\_\_\_\_\_\_\_\_\_\_\_ \pmod{\texttt{0x1FFFFFFFFFFFFFFF}}\)</td> </tr> </tbody> </table> <p><br/></p> <p>For the <strong>silver</strong> medal, report <strong>only</strong> the <strong>first 10</strong> and <strong>last 10</strong> hexadecimal digits of \(S(n)\).<br/> Here, ‚Äúfirst 10‚Äù means the first 10 digits of the standard hexadecimal representation of \(S(n)\), and ‚Äúlast 10‚Äù means the last 10 digits (equivalently \(S(n) \bmod 16^{10}\)). The problem is designed so that <strong>neither part has leading zeros</strong>, and both substrings consist of exactly 10 hexadecimal digits.</p> <p>For the <strong>gold</strong> medal, report your hexadecimal answer <strong>modulo</strong> \(\texttt{0x1FFFFFFFFFFFFFFF}.\)</p> <div class="answer-checker" style="border:1px solid #ddd;padding:1rem;border-radius:0.75rem;margin:1rem 0;"> <p><strong>Check your solution</strong> (enter hex, e.g. <code>34A</code> or <code>34a</code>):</p> <label>üéó Warm-up (n = 0x5):</label><br/> <input id="ans_warmup" type="text" style="width: 100%; max-width: 520px;" placeholder="your hex answer"/> <button onclick="checkAnswer('warmup')" style="margin-top:0.5rem;">Check</button> <div id="msg_warmup" style="margin-top:0.5rem;"></div> <hr style="margin:1rem 0;"/> <label>ü•â Bronze (n = 0xA):</label><br/> <input id="ans_bronze" type="text" style="width: 100%; max-width: 520px;" placeholder="your hex answer"/> <button onclick="checkAnswer('bronze')" style="margin-top:0.5rem;">Check</button> <div id="msg_bronze" style="margin-top:0.5rem;"></div> <hr style="margin:1rem 0;"/> <label>ü•à Silver (n = 0xAA):</label><br/> <div style="display:flex; align-items:center; justify-content:flex-start;"> <div style=" display:flex; gap:0.5rem; flex:0 0 520px; /* &lt;-- key: do NOT grow/shrink in the row */ width:520px; /* &lt;-- hard match with other inputs */ margin-right:0.5rem; box-sizing:border-box; "> <input id="ans_silver_prefix" type="text" style="flex:1 1 0; box-sizing:border-box;" placeholder="first 10 hex digits" maxlength="10"/> <input id="ans_silver_suffix" type="text" style="flex:1 1 0; box-sizing:border-box;" placeholder="last 10 hex digits" maxlength="10"/> </div> <button onclick="checkSilver()" style="white-space:nowrap;"> Check </button> </div> <div id="msg_silver" style="margin-top:0.5rem;"></div> <hr style="margin:1rem 0;"/> <label>ü•á Gold (n = 0xAAA) mod 0x1FFFFFFFFFFFFFFF:</label><br/> <input id="ans_gold" type="text" style="width: 100%; max-width: 520px;" placeholder="your hex answer (mod ...)"/> <button onclick="checkAnswer('gold')" style="margin-top:0.5rem;">Check</button> <div id="msg_gold" style="margin-top:0.5rem;"></div> </div> <script>
  // ------------- configure expected hashes (hex of SHA-256 digest) -------------
  const EXPECTED_SHA256_HEX = {
    warmup: "f25b4ef775f23326522a5ab6cd75aa122828a056f22a2d801d195814112056b2",
    bronze: "60a8a313a5dd7919bb5d7883c5bf356ea68c65baedcffd274782a07d8cb098c3",
    silver: "0b1afa52c14209b2fb8fa80c6deb530666c0f5efc48b4e4e0ad98978225d17cc",
    gold:   "5052d8df7c751051a386c696777e90903067ab3906eeff8d85fc200a53edc2e6",
  };

  // Optional "pepper" (not a real secret in client-side JS, but stops copy/paste hashing mistakes)
  const PEPPER = "hex-digit-canon-v1";

  function normalizeHexInput(s) {
    // Normalize common formatting differences:
    // - trim
    // - remove "0x" prefix if present
    // - remove underscores/spaces
    // - uppercase
    const t = (s ?? "").trim().replace(/^0x/i, "").replace(/[\s_]+/g, "").toUpperCase();
    // Allow empty? Treat empty as invalid.
    return t;
  }

  async function sha256Hex(text) {
    const data = new TextEncoder().encode(text);
    const digest = await crypto.subtle.digest("SHA-256", data);
    const bytes = new Uint8Array(digest);
    return Array.from(bytes, b => b.toString(16).padStart(2, "0")).join("");
  }

  async function checkAnswer(tier) {
    const input = document.getElementById(`ans_${tier}`);
    const msg = document.getElementById(`msg_${tier}`);

    const norm = normalizeHexInput(input.value);
    if (!norm) {
      msg.innerHTML = "<span style='color:#b00;'>Please enter a hex value.</span>";
      return;
    }

    // Hash normalized answer + pepper (so you hash exactly what you intend)
    const got = await sha256Hex(`${PEPPER}|${tier}|${norm}`);

    if (got === EXPECTED_SHA256_HEX[tier]) {
      msg.innerHTML = "<span style='color:#0a0;'><strong>Correct ‚úÖ</strong></span>";
    } else {
      msg.innerHTML = "<span style='color:#b00;'><strong>Not correct ‚ùå</strong></span>";
    }
  }
</script> <script>
async function checkSilver() {
  const prefixInput = document.getElementById("ans_silver_prefix");
  const suffixInput = document.getElementById("ans_silver_suffix");
  const msg = document.getElementById("msg_silver");

  const prefix = normalizeHexInput(prefixInput.value);
  const suffix = normalizeHexInput(suffixInput.value);

  if (prefix.length !== 10 || suffix.length !== 10) {
    msg.innerHTML =
      "<span style='color:#b00;'>Please enter exactly 10 hex digits for both prefix and suffix.</span>";
    return;
  }

  // Canonical message for hashing
  const payload = `${PEPPER}|silver|${prefix}|${suffix}`;
  const got = await sha256Hex(payload);

  if (got === EXPECTED_SHA256_HEX.silver) {
    msg.innerHTML = "<span style='color:#0a0;'><strong>Correct ‚úÖ</strong></span>";
  } else {
    msg.innerHTML = "<span style='color:#b00;'><strong>Not correct ‚ùå</strong></span>";
  }
}
</script> <p>üí° Solution coming soon: A full, step-by-step solution‚Äîincluding the underlying combinatorics and an efficient implementation‚Äîwill be published on this blog shortly. Stay tuned!</p>]]></content><author><name></name></author><category term="programming"/><category term="algorithms"/><category term="combinatorics"/><category term="combinatorics"/><category term="number-theory"/><category term="digit-dp"/><category term="algorithms"/><category term="optimization"/><category term="hexadecimal"/><category term="programming-puzzle"/><summary type="html"><![CDATA[A combinatorial programming challenge set in base 16: define a digit-canonical form for hexadecimal numbers and compute the cumulative sum of these canonical values across rapidly growing digit ranges. Simple to state, but requiring careful counting and optimization at scale.]]></summary></entry><entry><title type="html">Short Notes: On a Curious Prefix-Sum Problem</title><link href="https://markusthill.github.io/blog/2026/pe-818p100-generalized-parity-recurrence/" rel="alternate" type="text/html" title="Short Notes: On a Curious Prefix-Sum Problem"/><published>2026-01-10T14:00:51+00:00</published><updated>2026-01-10T14:00:51+00:00</updated><id>https://markusthill.github.io/blog/2026/pe-818p100-generalized-parity-recurrence</id><content type="html" xml:base="https://markusthill.github.io/blog/2026/pe-818p100-generalized-parity-recurrence/"><![CDATA[<p><br/></p> <h2 id="problem-description">Problem Description</h2> <p>You are given a sequence of numbers defined by a simple-looking recurrence.</p> <p>The sequence starts with a single initial value \(a_1\). Three fixed constants \(q\), \(r\), and \(t\) determine how all further values are generated.</p> <p>For every integer \(n \ge 1\), the sequence is defined by</p> \[\begin{aligned} a_{2n} &amp;= q\,a_n, \\ a_{2n+1} &amp;= r\,a_n + t\,a_{n+1}, \end{aligned}\] <p>where:</p> <ul> <li>\(a_n\) denotes the value of the sequence at position \(n\),</li> <li>\(q\) controls how values at even indices are produced,</li> <li>\(r\) and \(t\) control how values at odd indices depend on earlier terms.</li> </ul> <p>From this sequence, define the prefix sum</p> \[S(N) = a_1 + a_2 + a_3 + \dots + a_N,\] <p>which represents the sum of the first \(N\) sequence values.</p> <p>For small values of \(N\), computing \(S(N)\) is straightforward: one can generate the sequence term by term and keep a running sum.</p> <p>Now comes the riddle:</p> <blockquote> <p>Suppose \(N\) is so large that iterating from \(1\) to \(N\) is completely infeasible‚Äîthink of values like \(10^{12}\), \(10^{15}\), or even larger.</p> <p>Is it still possible to compute the exact value of \(S(N)\), and if so, how?</p> </blockquote> <p>At first glance, the task seems impossible: the definition of \(S(N)\) involves \(N\) terms, and \(N\) may be astronomically large.</p> <p>The challenge is to uncover a hidden structure in the recurrence that makes this computation feasible without ever touching most of the terms involved.</p> <p>Before diving into the solution, here is a brief roadmap of how we will approach the problem:</p> <ol> <li>We first analyze the recurrence algebraically and derive closed relations for the prefix sums.</li> <li>These relations reveal a binary self-similarity that allows us to reduce large indices recursively.</li> <li>We then translate this structure into an efficient $O(\log N)$ algorithm.</li> <li>Finally, we implement the method in code, verify it against a naive solution, and demonstrate its power on enormous values of $N$.</li> </ol> <p><br/></p> <h2 id="formal-problem-statement">Formal Problem Statement</h2> <p>Let $(a_n)_{n\ge 1}$ be a sequence defined by constants</p> \[a_1,q,r,t \in \mathbb{R} \;(\text{or } \mathbb{Z})\] <p>and the recurrence</p> \[\begin{aligned} a_{2n} &amp;= q\,a_n, \\ a_{2n+1} &amp;= r\,a_n + t\,a_{n+1}, \qquad (n \ge 1). \end{aligned}\] <p>Define the prefix sums \(S(n) := \sum_{k=1}^{n} a_k, \qquad S(0):=0.\)</p> <blockquote> <p><strong>Problem.</strong> Compute \(S(N)\) for a given \(N \ge 1\).</p> </blockquote> <p>Throughout this discussion, ‚Äúexact‚Äù means exact arithmetic with respect to the chosen number system. In particular:</p> <ul> <li>If $a_1,q,r,t$ are integers, then all values of $a_n$ and $S(N)$ are integers, and the algorithm computes them exactly using arbitrary-precision arithmetic.</li> <li>If floating-point values are used instead, the algorithm still applies, but the result is subject to the usual floating-point rounding behavior.</li> </ul> <p><br/></p> <h2 id="from-the-recurrence-to-prefix-sum-relations">From the Recurrence to Prefix-Sum Relations</h2> <p>We now turn to the key analytical step: translating the defining recurrence of the sequence into corresponding recurrences for its prefix sums. The central objective is to express \(S(2n)\) and \(S(2n+1)\) in terms of <em>smaller indices</em>, involving only \(S(n)\), \(S(n-1)\), and a small amount of local sequence information. This is precisely what will later allow an efficient algorithm.</p> <hr/> <p><br/></p> <h2 id="deriving-a-formula-for-s2n">Deriving a Formula for \(S(2n)\)</h2> <p>Because the recurrence defining $(a_n)$ distinguishes between even and odd indices, it is natural to analyze the prefix sums by separating contributions from even and odd positions. We begin directly from the definition:</p> \[S(2n) = \sum_{k=1}^{2n} a_k.\] <p>Since the recurrence distinguishes between even and odd indices, it is natural to split the sum accordingly:</p> \[S(2n) = \sum_{k=1}^{n} a_{2k} \;+\; \sum_{k=0}^{n-1} a_{2k+1}.\] <p><br/></p> <h3 id="even-index-contribution">Even-index contribution</h3> <p>Using the recurrence \(a_{2k}=q\,a_k\), the even part simplifies immediately:</p> \[\sum_{k=1}^{n} a_{2k} = \sum_{k=1}^{n} q\,a_k = q \sum_{k=1}^{n} a_k = q\,S(n).\] <p><br/></p> <h3 id="odd-index-contribution">Odd-index contribution</h3> <p>For the odd indices, first separate the base term:</p> \[\sum_{k=0}^{n-1} a_{2k+1} = a_1 + \sum_{k=1}^{n-1} a_{2k+1}.\] <p>For \(k \ge 1\), substitute the recurrence \(a_{2k+1}=r\,a_k+t\,a_{k+1}\):</p> \[\sum_{k=1}^{n-1} a_{2k+1} = r\sum_{k=1}^{n-1} a_k \;+\; t\sum_{k=1}^{n-1} a_{k+1}.\] <p>Both sums can be rewritten in terms of prefix sums:</p> \[\sum_{k=1}^{n-1} a_k = S(n-1), \qquad \sum_{k=1}^{n-1} a_{k+1} = \sum_{j=2}^{n} a_j = S(n)-a_1.\] <p>Combining these expressions gives</p> \[\sum_{k=0}^{n-1} a_{2k+1} = a_1 + rS(n-1) + t\bigl(S(n)-a_1\bigr) = rS(n-1) + tS(n) + (1-t)a_1.\] <p><br/></p> <h3 id="combining-both-parts">Combining both parts</h3> <p>Adding the even and odd contributions yields the closed recurrence</p> \[S(2n) = qS(n) + rS(n-1) + tS(n) + (1-t)a_1 = (q+t)S(n) + rS(n-1) + (1-t)a_1.\] <p>Notably, \(S(2n)\) depends only on prefix sums at smaller indices and a constant term determined by \(a_1\).</p> <hr/> <p><br/></p> <h2 id="deriving-a-formula-for-s2n1">Deriving a Formula for \(S(2n+1)\)</h2> <p>The odd case follows immediately from the definition:</p> \[S(2n+1) = S(2n) + a_{2n+1}.\] <p>Substituting the recurrence for \(a_{2n+1}\) gives</p> \[S(2n+1) = S(2n) + r\,a_n + t\,a_{n+1}.\] <p>Inserting the expression for \(S(2n)\) obtained above leads to</p> \[S(2n+1) = (q+t)S(n) + rS(n-1) + (1-t)a_1 \;+\; r\,a_n + t\,a_{n+1}.\] <hr/> <p><br/></p> <h2 id="summary-of-the-induced-prefix-sum-recurrences">Summary of the Induced Prefix-Sum Recurrences</h2> <p>We have derived the following identities for all \(n \ge 1\):</p> \[\begin{aligned} S(2n) &amp;= (q+t)\,S(n) + r\,S(n-1) + (1-t)\,a_1, \\ S(2n+1) &amp;= (q+t)\,S(n) + r\,S(n-1) + (1-t)\,a_1 \;+\; r\,a_n + t\,a_{n+1}. \end{aligned}\] <p>The key observation is the asymmetry between the two cases:</p> <ul> <li>The <strong>even</strong> prefix sum \(S(2n)\) closes purely in terms of prefix sums.</li> <li>The <strong>odd</strong> prefix sum \(S(2n+1)\) additionally depends on the local pair \((a_n,a_{n+1})\).</li> </ul> <p>This structure is exactly what enables an efficient algorithm.</p> <hr/> <p><br/></p> <h2 id="a-binary-state-algorithm-for-computing-sn">A Binary-State Algorithm for Computing \(S(N)\)</h2> <p>The identities above suggest a strategy based on repeatedly halving the index. Instead of summing \(N\) terms, we process the binary representation of \(N\) and update a constant-size state at each step.</p> <p><br/></p> <h3 id="state-representation">State Representation</h3> <p>At any stage, we maintain the state</p> \[\bigl(S(n-1),\; S(n),\; a_n,\; a_{n+1}\bigr),\] <p>which contains precisely the information required to compute \(S(2n),\; S(2n+1)\) and the next sequence values. The initial state corresponds to \(n=1\):</p> \[\bigl(S(0), S(1), a_1, a_2\bigr) = \bigl(0,\; a_1,\; a_1,\; q\,a_1\bigr),\] <p>since \(a_2=a_{2\cdot 1}=q\,a_1\).</p> <p><br/></p> <h3 id="binary-decomposition-of-the-index">Binary Decomposition of the Index</h3> <p>Write the target index \(N\) in binary:</p> \[N = (1 b_{k-1} b_{k-2} \dots b_0)_2.\] <p>The leading \(1\) initializes the state at \(n=1\). Each subsequent bit determines the transition:</p> <ul> <li>bit \(0\) corresponds to \(n \mapsto 2n\),</li> <li>bit \(1\) corresponds to \(n \mapsto 2n+1\).</li> </ul> <p>Processing the bits from left to right repeatedly reduces the problem size by a factor of two.</p> <p><br/></p> <h3 id="precomputation-step">Precomputation Step</h3> <p>From the current state, compute the quantities</p> \[\begin{aligned} S(2n) &amp;= (q+t)S(n) + rS(n-1) + (1-t)a_1, \\ S(2n+1) &amp;= S(2n) + r\,a_n + t\,a_{n+1}, \end{aligned}\] <p>as well as the sequence values</p> \[\begin{aligned} a_{2n} &amp;= q\,a_n, \\ a_{2n+1} &amp;= r\,a_n + t\,a_{n+1}, \\ a_{2n+2} &amp;= q\,a_{n+1}. \end{aligned}\] <p>All of these depend only on the current state and can be computed in constant time.</p> <p><br/></p> <h3 id="even-transition-bit-0">Even Transition (bit \(0\))</h3> <p>If the next bit is \(0\), update \(n \mapsto 2n\). The new state is</p> <p>\(\bigl(S(2n)-a_{2n},\; S(2n),\; a_{2n},\; a_{2n+1}\bigr),\) using the identity \(S(2n-1)=S(2n)-a_{2n}\).</p> <p><br/></p> <h3 id="odd-transition-bit-1">Odd Transition (bit \(1\))</h3> <p>If the next bit is \(1\), update \(n \mapsto 2n+1\). The new state is</p> \[\bigl(S(2n),\; S(2n+1),\; a_{2n+1},\; a_{2n+2}\bigr).\] <p><br/></p> <h3 id="termination-and-result">Termination and Result</h3> <p>After all binary digits of \(N\) have been processed, the maintained index equals \(n=N\). The desired prefix sum is therefore \(S(N)\).</p> <hr/> <p><br/></p> <h3 id="interpretation">Interpretation</h3> <p>The entire computation runs in \(O(\log N)\) time. Only a constant-size state is maintained, and each binary digit of \(N\) triggers a single constant-time update. The apparent necessity of summing \(N\) terms disappears once the binary self-similarity of the recurrence is made explicit.</p> <hr/> <p><br/></p> <h2 id="from-theory-to-code">From Theory to Code</h2> <h3 id="a-naive-reference-implementation">A Naive Reference Implementation</h3> <p>Before turning to the optimized algorithm, it is useful to establish a simple baseline. For small values of \(N\), the most straightforward approach is to <strong>explicitly generate the sequence</strong> using its defining recurrence and then compute the prefix sums by a single pass over the resulting list. While this method runs in \(O(N)\) time and quickly becomes infeasible for large \(N\), it has two important roles:</p> <ul> <li>it makes the recurrence concrete and easy to experiment with, and</li> <li>it serves as a reliable <strong>reference implementation</strong> for verifying the correctness of the optimized method later on.</li> </ul> <p>The following code implements this direct approach.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Sequence</span>


<span class="k">def</span> <span class="nf">build_sequence_naive</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">a1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Build a[1..N+1] for the parity recurrence (naive O(N) reference).

    Recurrence:
        a_{2n}   = q * a_n
        a_{2n+1} = r * a_n + t * a_{n+1}

    We compute up to a_{N+1} so that the step for odd indices (2n+1) can safely
    reference a_{n+1} for all n needed.

    Args:
        N: Target maximum index (must satisfy N &gt;= 1).
        a1: Initial value a_1.
        q, r, t: Recurrence constants.

    Returns:
        A list `a` of length N+2 using 1-based indexing:
            - a[0] is unused (kept as 0),
            - a[k] == a_k for k = 1..N+1.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">N must be &gt;= 1</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># 1-based array: keep index 0 unused for clarity.
</span>    <span class="c1"># We compute up to N+1 because the recurrence needs a_{n+1}.
</span>    <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span>

    <span class="c1"># Fill values in increasing n. Each step writes a_{2n} and a_{2n+1}.
</span>    <span class="c1"># The upper bound ensures we don't write beyond N+1.
</span>    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">a</span>


<span class="k">def</span> <span class="nf">prefix_sums_naive</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Compute prefix sums S(k) = sum_{i=1}^k a_i from a 1-based sequence.

    Args:
        a: 1-based sequence array with a[0] unused.

    Returns:
        1-based prefix sums array S with:
            - S[0] = 0,
            - S[k] = sum_{i=1}^k a[i].
    </span><span class="sh">"""</span>
    <span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">running</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
        <span class="n">running</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">running</span>
    <span class="k">return</span> <span class="n">S</span>
</code></pre></div></div> <p>The first function, <code class="language-plaintext highlighter-rouge">build_sequence_naive</code>, constructs the sequence values \(a_1,a_2,\dots,a_{N+1}\) directly from the recurrence by iterating over all indices up to \(N\). The second function, <code class="language-plaintext highlighter-rouge">prefix_sums_naive</code>, then computes the prefix sums \(S(k)=\sum_{i=1}^k a_i\) in a single linear pass. Together, these two routines implement the most direct solution to the problem: explicit sequence generation followed by explicit summation. This approach is simple and transparent, but its runtime grows linearly with \(N\), which makes it unsuitable for very large inputs. The following example demonstrates how the naive implementation can be used for moderate values of \(N\). We generate the sequence, inspect the first few terms, and finally compute the prefix sum \(S(N)\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">a1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="o">-</span><span class="mi">11</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="o">-</span><span class="mi">33</span>

<span class="n">a</span> <span class="o">=</span> <span class="nf">build_sequence_naive</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">a1</span><span class="o">=</span><span class="n">a1</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

<span class="c1"># Show the first few terms a_1..a_20 (ignore a[0]).
</span><span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">21</span><span class="p">])</span>

<span class="c1"># Prefix sums S(k) = sum_{i=1}^k a_i in O(N).
</span><span class="n">S</span> <span class="o">=</span> <span class="nf">prefix_sums_naive</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># This is S(N).
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">S(N):</span><span class="sh">"</span><span class="p">,</span> <span class="n">S</span><span class="p">[</span><span class="n">N</span><span class="p">])</span>
</code></pre></div></div> <p>In this example, we fix concrete values for the parameters \(a_1\), \(q\), \(r\), and \(t\), and choose a moderately large bound \(N = 10\,000\). The sequence is first generated explicitly up to index \(N+1\) using the naive construction. Printing the first few terms provides a quick sanity check that the recurrence is applied as expected. Afterwards, the prefix sums are computed in linear time, and the final value \(S(N)\) is obtained by a simple array lookup. For values of \(N\) on this scale, this approach works well and is easy to understand‚Äîbut it already hints at why a different strategy is needed when \(N\) becomes much larger. Before moving on to the optimized algorithm, it is helpful to <em>see</em> what this recurrence produces.</p> <p>The following plot shows the sequence values \(a_n\) and their corresponding prefix sums \(S(n)\) for a fixed choice of parameters and a moderate range of indices. Even at this scale, the behavior is far from simple: the sequence oscillates, changes magnitude rapidly, and the prefix sums reflect a complex accumulation of these effects. This visual complexity hints at why a naive summation strategy quickly becomes unwieldy‚Äîand why exploiting the structure of the recurrence is essential.</p> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-01-10-pe-818p100-generalized-parity-recurrence/2026-01-10-pe-818p100-generalized-parity-recurrence-480.webp 480w,/assets/img/2026-01-10-pe-818p100-generalized-parity-recurrence/2026-01-10-pe-818p100-generalized-parity-recurrence-800.webp 800w,/assets/img/2026-01-10-pe-818p100-generalized-parity-recurrence/2026-01-10-pe-818p100-generalized-parity-recurrence-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-01-10-pe-818p100-generalized-parity-recurrence/2026-01-10-pe-818p100-generalized-parity-recurrence.png" class="img-fluid rounded z-depth-1 imgcenter" width="95%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 1:</b> The parity-defined sequence $a_n$ (top) and its prefix sums $S(n)$ (bottom) for fixed parameters. Even at moderate sizes, the behavior is highly irregular. </figcaption> </figure> <p><br/></p> <h3 id="an-optimized-prefix-sum-algorithm">An Optimized Prefix-Sum Algorithm</h3> <p>The naive approach makes the difficulty of the problem clear: explicitly generating the sequence and summing its terms requires time proportional to \(N\), which quickly becomes infeasible.</p> <p>We now turn to an optimized algorithm that avoids this bottleneck entirely. Instead of iterating over all indices up to \(N\), the method exploits the parity-based structure of the recurrence derived earlier.</p> <p>The key idea is to process the binary representation of \(N\) from left to right, maintaining only a <strong>constant-size state</strong> that contains just enough information to update the prefix sum when the index is doubled or incremented by one. At each step, the problem size is effectively cut in half.</p> <p>At a high level, the algorithm can be summarized as follows:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initialize state at n = 1
for each remaining binary digit of N (left to right):
    precompute S(2n), S(2n+1), a_{2n}, a_{2n+1}, a_{2n+2}
    if bit == 0:
        update state to n = 2n
    else:
        update state to n = 2n + 1
return S(N)
</code></pre></div></div> <p>The full implementation below is a direct and faithful translation of this idea.</p> <p>As a result, the number of operations grows only with the number of binary digits of \(N\), making it possible to compute \(S(N)\) even for astronomically large values of \(N\). The following implementation is a direct translation of the derivation above into code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">prefix_sum_a</span><span class="p">(</span>
    <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">a1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">r</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">t</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute the prefix sum S(N) for a parity-defined recurrence in O(log N).

    We consider a sequence $(a_n)_{n</span><span class="se">\\</span><span class="s">ge 1}$ defined by constants $q, r, t$:

    $$
    </span><span class="se">\\</span><span class="s">begin{aligned}
    a_{2n}   &amp;= q</span><span class="se">\\</span><span class="s">,a_n, </span><span class="se">\\\\</span><span class="s">
    a_{2n+1} &amp;= r</span><span class="se">\\</span><span class="s">,a_n + t</span><span class="se">\\</span><span class="s">,a_{n+1},
    </span><span class="se">\\</span><span class="s">end{aligned}
    </span><span class="se">\\</span><span class="s">qquad (n</span><span class="se">\\</span><span class="s">ge 1),
    $$

    with given initial value $a_1 = a1$.

    The associated prefix sums are

    $$
    S(n) = </span><span class="se">\\</span><span class="s">sum_{k=1}^{n} a_k, </span><span class="se">\\</span><span class="s">qquad S(0)=0.
    $$

    Using the derived identities

    $$
    S(2n)   = (q+t)</span><span class="se">\\</span><span class="s">,S(n) + r</span><span class="se">\\</span><span class="s">,S(n-1) + (1-t)</span><span class="se">\\</span><span class="s">,a_1,
    $$
    $$
    S(2n+1) = S(2n) + r</span><span class="se">\\</span><span class="s">,a_n + t</span><span class="se">\\</span><span class="s">,a_{n+1},
    $$

    we can compute $S(N)$ by walking through the binary representation of $N$
    and maintaining the constant-size state

        (S(n-1), S(n), a_n, a_{n+1})

    which is updated according to whether the next binary digit corresponds to
    $n </span><span class="se">\\</span><span class="s">mapsto 2n$ (bit 0) or $n </span><span class="se">\\</span><span class="s">mapsto 2n+1$ (bit 1).

    Args:
        N:
            Target index (must satisfy N &gt;= 1).
        a1:
            Initial value $a_1$ of the sequence.
        q:
            Constant in the even-index recurrence $a_{2n} = q</span><span class="se">\\</span><span class="s">,a_n$.
        r:
            Constant in the odd-index recurrence $a_{2n+1} = r</span><span class="se">\\</span><span class="s">,a_n + t</span><span class="se">\\</span><span class="s">,a_{n+1}$.
        t:
            Constant in the odd-index recurrence $a_{2n+1} = r</span><span class="se">\\</span><span class="s">,a_n + t</span><span class="se">\\</span><span class="s">,a_{n+1}$.

    Returns:
        The prefix sum $S(N) = </span><span class="se">\\</span><span class="s">sum_{k=1}^{N} a_k$.

    Raises:
        ValueError: If N &lt; 1.

    Notes:
        - This implementation is numerically and type-wise </span><span class="sh">"</span><span class="s">honest</span><span class="sh">"</span><span class="s">: it supports
          `int` and `float` (and mixtures thereof) and returns `int | float`.
        - Runtime is O(log N) and memory usage is O(1).
    </span><span class="sh">"""</span>
    <span class="c1"># Guard against invalid input: the sequence is defined for N &gt;= 1.
</span>    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">N must be &gt;= 1</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Process the binary digits of N from left to right.
</span>    <span class="c1"># The leading '1' corresponds to n = 1, which we encode in the initial state,
</span>    <span class="c1"># so we iterate over the remaining digits only.
</span>    <span class="n">bits</span> <span class="o">=</span> <span class="nf">format</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tail_bits</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="c1"># Maintain the state at the current index n:
</span>    <span class="c1">#
</span>    <span class="c1">#   S_nm1 = S(n-1) = sum_{k=1}^{n-1} a_k
</span>    <span class="c1">#   S_n   = S(n)   = sum_{k=1}^{n}   a_k
</span>    <span class="c1">#   a_n   = a_n
</span>    <span class="c1">#   a_np1 = a_{n+1}
</span>    <span class="c1">#
</span>    <span class="c1"># Initialize at n = 1:
</span>    <span class="c1">#   S(0) = 0
</span>    <span class="c1">#   S(1) = a1
</span>    <span class="c1">#   a_1  = a1
</span>    <span class="c1">#   a_2  = q * a1
</span>    <span class="n">S_nm1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">S_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">a1</span>
    <span class="n">a_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">a1</span>
    <span class="n">a_np1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">a1</span>

    <span class="c1"># Update the state for each remaining bit.
</span>    <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">tail_bits</span><span class="p">:</span>
        <span class="c1"># Precompute prefix sums at doubled indices:
</span>        <span class="c1">#
</span>        <span class="c1">#   S(2n)   = (q+t) S(n) + r S(n-1) + (1-t) a1
</span>        <span class="c1">#   S(2n+1) = S(2n) + r a_n + t a_{n+1}
</span>        <span class="c1">#
</span>        <span class="c1"># These are computed unconditionally; which one we "land on" depends on
</span>        <span class="c1"># the next binary digit.
</span>        <span class="n">S_2n</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">S_nm1</span> <span class="o">+</span> <span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">S_n</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">a1</span>
        <span class="n">S_2np1</span> <span class="o">=</span> <span class="n">S_2n</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">a_n</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">a_np1</span>

        <span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># Bit 1: n -&gt; 2n + 1
</span>            <span class="c1">#
</span>            <span class="c1"># Sequence updates:
</span>            <span class="c1">#   a_{2n+1} = r a_n + t a_{n+1}
</span>            <span class="c1">#   a_{2n+2} = q a_{n+1}
</span>            <span class="n">new_a_n</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">a_n</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">a_np1</span>
            <span class="n">new_a_np1</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">a_np1</span>

            <span class="c1"># Prefix sum updates:
</span>            <span class="c1">#   S((2n+1)-1) = S(2n)
</span>            <span class="c1">#   S(2n+1)     = S_2np1
</span>            <span class="n">S_nm1</span><span class="p">,</span> <span class="n">S_n</span> <span class="o">=</span> <span class="n">S_2n</span><span class="p">,</span> <span class="n">S_2np1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Bit 0: n -&gt; 2n
</span>            <span class="c1">#
</span>            <span class="c1"># Sequence updates:
</span>            <span class="c1">#   a_{2n}   = q a_n
</span>            <span class="c1">#   a_{2n+1} = r a_n + t a_{n+1}
</span>            <span class="n">new_a_n</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">a_n</span>
            <span class="n">new_a_np1</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">a_n</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">a_np1</span>

            <span class="c1"># Prefix sum updates:
</span>            <span class="c1">#   S(2n)   = S_2n
</span>            <span class="c1">#   S(2n-1) = S(2n) - a_{2n}
</span>            <span class="n">S_nm1</span><span class="p">,</span> <span class="n">S_n</span> <span class="o">=</span> <span class="n">S_2n</span> <span class="o">-</span> <span class="n">new_a_n</span><span class="p">,</span> <span class="n">S_2n</span>

        <span class="c1"># Advance the sequence state.
</span>        <span class="n">a_n</span><span class="p">,</span> <span class="n">a_np1</span> <span class="o">=</span> <span class="n">new_a_n</span><span class="p">,</span> <span class="n">new_a_np1</span>

    <span class="c1"># After all bits have been processed, S_n = S(N).
</span>    <span class="k">return</span> <span class="n">S_n</span>
</code></pre></div></div> <p>The function <code class="language-plaintext highlighter-rouge">prefix_sum_a</code> computes the prefix sum \(S(N)\) without ever constructing the full sequence.</p> <p>It walks through the binary digits of \(N\) and maintains the four values \(\bigl(S(n-1), S(n), a_n, a_{n+1}\bigr)\) as its internal state. For each bit, the state is updated according to whether the index transition is \(n \mapsto 2n\) or \(n \mapsto 2n+1\), using the closed-form recurrences derived earlier.</p> <p>Because each binary digit of \(N\) triggers only a constant amount of work, the overall runtime is proportional to the bit-length of \(N\). This allows exact prefix sums to be computed for values of \(N\) that are far beyond the reach of any direct summation approach.</p> <p>As a quick sanity check, we can compare the naive result (computed by explicitly building the sequence and its prefix sums) with the optimized function. For a moderate value of \(N\) both methods are feasible, so they should agree exactly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compare naive approach with optimized code for a (small) N.
</span><span class="n">slow</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">N</span><span class="p">]</span>
<span class="n">fast</span> <span class="o">=</span> <span class="nf">prefix_sum_a</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">naive S(N):</span><span class="sh">"</span><span class="p">,</span> <span class="n">slow</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">fast  S(N):</span><span class="sh">"</span><span class="p">,</span> <span class="n">fast</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">slow</span> <span class="o">==</span> <span class="n">fast</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mismatch between naive and optimized result!</span><span class="sh">"</span>
</code></pre></div></div> <p>To further increase confidence in the implementation, we can perform a full verification on a small range of indices. For a fixed, moderately sized bound \(N\), the naive method allows us to compute all prefix sums \(S(1), S(2), \dots, S(N)\) explicitly.</p> <p>The following loop compares these reference values against the optimized algorithm for <em>every</em> index up to \(N\). If the derivation and implementation are correct, all results must match exactly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Verify that the optimized O(log n) algorithm matches the naive result
# for all n &lt;= N.
</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">fast</span> <span class="o">=</span> <span class="nf">prefix_sum_a</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">slow</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">fast</span> <span class="o">!=</span> <span class="n">slow</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">AssertionError</span><span class="p">(</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">Mismatch at n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s">: </span><span class="sh">"</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">fast=</span><span class="si">{</span><span class="n">fast</span><span class="si">}</span><span class="s">, slow=</span><span class="si">{</span><span class="n">slow</span><span class="si">}</span><span class="sh">"</span>
        <span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Verification passed: optimized and naive prefix sums match for all n &lt;= </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h3 id="demonstrating-the-power-of-the-optimized-algorithm">Demonstrating the Power of the Optimized Algorithm</h3> <p>The true strength of the approach becomes apparent when we push it far beyond the range where any naive method could possibly work.</p> <p>Since the optimized algorithm performs one constant-time update per binary digit of \(N\), its runtime depends only on the <strong>bit-length</strong> of \(N\), not on \(N\) itself. This makes it feasible to compute prefix sums for indices that are astronomically large.</p> <p>The following example evaluates \(S(N)\) for a selection of increasingly large values of \(N\)‚Äîranging from millions to numbers with dozens of digits. Even the so-called ‚Äúworst-case‚Äù bit patterns, where all binary digits are equal to one, are handled just as easily.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Demonstration: huge N is easy with the O(log N) method.
</span>
<span class="n">a1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="o">-</span><span class="mi">11</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="o">-</span><span class="mi">33</span>

<span class="n">test_Ns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">9</span><span class="p">,</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">15</span><span class="p">,</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">18</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">60</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>   <span class="c1"># ~1.15e18 (all 1-bits, "worst-case" bit-pattern)
</span>    <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">80</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="mi">10</span><span class="o">**</span><span class="mi">30</span><span class="p">,</span>          <span class="c1"># absurdly large, still fine
</span><span class="p">]</span>

<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="n">test_Ns</span><span class="p">:</span>
    <span class="n">S_N</span> <span class="o">=</span> <span class="nf">prefix_sum_a</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">N=</span><span class="si">{</span><span class="n">N</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">32</span><span class="si">}</span><span class="s">  bits=</span><span class="si">{</span><span class="n">N</span><span class="p">.</span><span class="nf">bit_length</span><span class="p">()</span><span class="si">:</span><span class="o">&gt;</span><span class="mi">3</span><span class="si">}</span><span class="s">  S(N)=</span><span class="si">{</span><span class="n">S_N</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This final demonstration highlights the central takeaway: once the hidden structure of the recurrence is exposed, computing a sum of \(N\) terms no longer requires work proportional to \(N\)‚Äîonly to the number of bits needed to write \(N\) down.</p> <p><br/></p> <h2 id="summary">Summary</h2> <p>We started with a simple recurrence that defines a sequence by splitting indices into even and odd cases. While computing individual terms is easy, the task of summing the first \(N\) terms quickly becomes infeasible when \(N\) is large.</p> <p>By carefully analyzing how the recurrence behaves under index doubling, we derived closed formulas for the prefix sums \(S(2n)\) and \(S(2n+1)\). These relations reveal a hidden binary structure: prefix sums at large indices can be expressed entirely in terms of much smaller ones, together with a small amount of local state.</p> <p>This insight leads directly to an efficient algorithm that processes the binary representation of \(N\) and maintains only a constant-size state. As a result, the prefix sum \(S(N)\) can be computed exactly in time proportional to the number of bits of \(N\), rather than to \(N\) itself.</p> <p>The contrast between the naive and optimized implementations highlights a recurring theme in algorithmic mathematics: once the right structure is identified, problems that initially appear intractable can often be solved with surprising efficiency.</p>]]></content><author><name></name></author><category term="programming"/><category term="algorithms"/><category term="mathematics"/><category term="recurrence-relations"/><category term="prefix-sums"/><category term="discrete-math"/><category term="algorithms"/><category term="optimization"/><category term="python"/><summary type="html"><![CDATA[A deceptively simple recurrence leads to an unexpected challenge when computing its prefix sums. Solving it efficiently requires looking at the problem from a different angle.]]></summary></entry><entry><title type="html">Short Notes: Equal Partitions, Products, and Decimal Structure</title><link href="https://markusthill.github.io/blog/2026/pe-83p100-quanity-equal-parts/" rel="alternate" type="text/html" title="Short Notes: Equal Partitions, Products, and Decimal Structure"/><published>2026-01-10T14:00:51+00:00</published><updated>2026-01-10T14:00:51+00:00</updated><id>https://markusthill.github.io/blog/2026/pe-83p100-quanity-equal-parts</id><content type="html" xml:base="https://markusthill.github.io/blog/2026/pe-83p100-quanity-equal-parts/"><![CDATA[<p><br/></p> <h2 id="problem">Problem</h2> <p>Dividing a fixed quantity into equal parts often leads to interesting trade-offs between the size of each part and the number of factors involved. In this problem, we investigate how such a division influences not only the magnitude of a product, but also its arithmetic properties.</p> <p>Let \(N \ge 2\) be a positive integer. Suppose \(N\) is divided into \(k\) equal real parts, each of size \(r = N/k\), so that</p> \[N = \underbrace{r + r + \cdots + r}_{k\text{ times}}.\] <p>Define the corresponding product</p> \[P = r^k = \left(\frac{N}{k}\right)^k.\] <p>For a fixed value of \(N\), let \(M(N)\) denote the maximum possible value of \(P\) over all integers \(k \ge 1\).</p> <p><br/></p> <h3 id="decimal-behavior">Decimal Behavior</h3> <p>A rational number is said to have a terminating decimal expansion (also called being <em>finite in base 10</em>) if its decimal representation ends after finitely many digits.</p> <p>For each \(N\), the value \(M(N)\) is a rational number, and its decimal expansion may or may not terminate.</p> <p><br/></p> <h3 id="non-termination-weight">Non-Termination Weight</h3> <p>Define a function \(W(N)\) as follows:</p> <ul> <li>If \(M(N)\) has a terminating decimal expansion, set \(W(N) = 0\).</li> <li>Otherwise, write \(M(N)\) in lowest terms and let \(W(N)\) be the smallest prime divisor of its denominator.</li> </ul> <p><br/></p> <h3 id="task">Task</h3> <p>Compute</p> \[\sum_{N = 5}^{10^6} W(N).\] <p><br/></p> <h2 id="solution-sketch">Solution Sketch</h2> <h3 id="step-1-set-up-the-function">Step 1: Set up the function</h3> <p>For a fixed positive integer \(N\) and a (for now) real variable \(k&gt;0\), define</p> \[f(k) = \left(\frac{N}{k}\right)^k.\] <p>We will maximize \(f(k)\) over real \(k\) first, and later worry about the fact that in the original problem \(k\) must be an integer.</p> <p><br/></p> <h3 id="step-2-rewrite-using-exponentials-and-logarithms">Step 2: Rewrite using exponentials and logarithms</h3> <p>A standard trick is to rewrite powers via the exponential function:</p> \[a^k = e^{k\ln(a)}.\] <p>So here,</p> \[f(k) = \left(\frac{N}{k}\right)^k = e^{k\ln\left(\frac{N}{k}\right)}.\] <p>Now expand the logarithm:</p> \[\ln\left(\frac{N}{k}\right) = \ln(N) - \ln(k).\] <p>Therefore,</p> \[f(k) = e^{k(\ln N - \ln k)} = e^{k\ln N - k\ln k}.\] <p><br/></p> <h3 id="step-3-differentiate-step-by-step">Step 3: Differentiate step by step</h3> <p>Let</p> \[g(k) = k\ln\left(\frac{N}{k}\right).\] <p>Then</p> \[f(k) = e^{g(k)}.\] <p>By the chain rule,</p> \[f'(k) = e^{g(k)} \cdot g'(k) = f(k)\,g'(k).\] <p>So we only need \(g'(k)\).</p> <p>Then, start from the expanded form:</p> \[g(k) = k(\ln N - \ln k) = k\ln N - k\ln k.\] <p>Differentiate term by term:</p> <p>1) Since \(\ln N\) is constant (with respect to \(k\)),</p> \[\frac{d}{dk}(k\ln N) = \ln N.\] <p>2) For \(k\ln k\) use the product rule:</p> \[\frac{d}{dk}(k\ln k) = \frac{d}{dk}(k)\cdot \ln k + k \cdot \frac{d}{dk}(\ln k) = 1\cdot \ln k + k\cdot \frac{1}{k} = \ln k + 1.\] <p>Hence</p> \[\frac{d}{dk}(-k\ln k) = -(\ln k + 1).\] <p>Putting both pieces together:</p> \[g'(k) = \ln N - (\ln k + 1) = \ln N - \ln k - 1.\] <p>Then, plug back into \(f'(k)\)</p> <p>Recall \(f'(k) = f(k)\,g'(k)\), so</p> \[f'(k) = \left(\frac{N}{k}\right)^k (\ln N - \ln k - 1).\] <p><br/></p> <h3 id="step-4-solve-the-critical-point-equation">Step 4: Solve the critical point equation</h3> <p>A maximum (in the continuous setting) occurs at critical points where \(f'(k)=0\):</p> \[\left(\frac{N}{k}\right)^k (\ln N - \ln k - 1) = 0.\] <p>The factor \(\left(\frac{N}{k}\right)^k\) is strictly positive for \(k&gt;0\), so the only way the product can be zero is</p> \[\ln N - \ln k - 1 = 0.\] <p>Rearrange:</p> \[\ln N - \ln k = 1 \quad\Longleftrightarrow\quad \ln\left(\frac{N}{k}\right) = 1.\] <p>Exponentiate both sides:</p> \[\frac{N}{k} = e^1 = e.\] <p>Finally solve for \(k\):</p> \[k = \frac{N}{e}.\] <p><br/></p> <h4 id="conclusion-continuous-optimum">Conclusion (continuous optimum)</h4> <p>In the continuous version of the problem, \(f(k)=\left(\frac{N}{k}\right)^k\) is maximized when</p> \[k = \frac{N}{e}.\] <p><br/></p> <h3 id="step-5-restricting-to-integer-values-of-k">Step 5: Restricting to integer values of \(k\)</h3> <p>The previous calculation identifies the maximizer of</p> \[f(k) = \left(\frac{N}{k}\right)^k\] <p>when \(k\) is allowed to vary over the positive real numbers. In the original problem, however, the number of parts must be an integer.</p> <p>Let</p> \[k^* = \frac{N}{e}\] <p>denote the continuous maximizer. Since \(f(k)\) is smooth and unimodal around its maximum, its largest value among integer arguments must be attained at one of the two integers closest to \(k^*\).</p> <p>Therefore, it suffices to compare the values</p> \[k_1 = \lfloor k^* \rfloor, \qquad k_2 = \lceil k^* \rceil,\] <p>and select the one that yields the larger product.Equivalently, since the logarithm is strictly increasing, one may compare</p> \[k \bigl(\ln N - \ln k\bigr)\] <p>for \(k \in \{k_1, k_2\}\) instead of evaluating the product directly. The integer value chosen in this way determines the maximizing split and hence the value of \(M(N)\).</p> <h2 id="implementation-notes">Implementation Notes</h2> <p>To turn the mathematical reasoning into an efficient program, we proceed in a few clearly separated steps. For each value of \(N\), the first task is to determine the integer \(k\) that maximizes the product</p> \[\left(\frac{N}{k}\right)^k.\] <p>As shown above, the continuous maximizer lies at \(k^* = N/e\), so it is sufficient to compare the two nearest integers,</p> \[\lfloor k^* \rfloor \quad \text{and} \quad \lceil k^* \rceil.\] <p>To avoid numerical overflow, this comparison is carried out in logarithmic form. Once the optimal integer \(k\) has been selected, we consider the value</p> \[M(N) = \left(\frac{N}{k}\right)^k = \frac{N^k}{k^k}.\] <p>Instead of forming this potentially huge rational number explicitly, we analyze its denominator in lowest terms. After cancelling common factors between \(N\) and \(k\), the reduced denominator is</p> \[\left(\frac{k}{\gcd(N,k)}\right)^k.\] <p>This observation allows us to decide whether \(M(N)\) has a terminating decimal expansion using only elementary number theory: a rational number terminates in base 10 if and only if its reduced denominator has no prime factors other than \(2\) and \(5\). If the decimal expansion terminates, the contribution \(W(N)\) is zero. Otherwise, the contribution is defined as the smallest prime divisor of the reduced denominator, which can be extracted directly from \(k / \gcd(N,k)\). The following code implements these steps and computes the required sum</p> \[\sum_{N=5}^{10^6} W(N).\] <p>It should complete in less than 10 seconds.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">math</span> <span class="kn">import</span> <span class="n">gcd</span>


<span class="k">def</span> <span class="nf">reduce_fraction</span><span class="p">(</span><span class="n">numer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">denom</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Reduce the rational numer/denom to lowest terms.

    Note:
        We only need the reduced denominator for the decimal-termination test
        and for extracting prime factors.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">denom</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ZeroDivisionError</span><span class="p">(</span><span class="sh">"</span><span class="s">Denominator must be non-zero.</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">g</span> <span class="o">=</span> <span class="nf">gcd</span><span class="p">(</span><span class="n">numer</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">numer</span> <span class="o">//</span> <span class="n">g</span><span class="p">,</span> <span class="n">denom</span> <span class="o">//</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">is_terminating_decimal_of_rational</span><span class="p">(</span><span class="n">numer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">denom</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Return True iff numer/denom (in lowest terms) has a terminating decimal.

    A rational number terminates in base 10 iff its reduced denominator has
    no prime factors other than 2 and 5.
    </span><span class="sh">"""</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nf">reduce_fraction</span><span class="p">(</span><span class="n">numer</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>

    <span class="c1"># Strip all factors of 2 and 5 from the reduced denominator.
</span>    <span class="k">while</span> <span class="n">d</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">//=</span> <span class="mi">2</span>
    <span class="k">while</span> <span class="n">d</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">//=</span> <span class="mi">5</span>

    <span class="c1"># If nothing remains, the denominator was of the form 2^a * 5^b.
</span>    <span class="k">return</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">smallest_prime_factor</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Return the smallest prime factor of n (for n &gt;= 2).

    Note that this function is pretty slow and should be replaced for large numbers `n`.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">n must be &gt;= 2</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Handle 2 quickly, then try odd divisors up to sqrt(n).
</span>    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span>

    <span class="n">p</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">while</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">p</span>
        <span class="n">p</span> <span class="o">+=</span> <span class="mi">2</span>

    <span class="c1"># If no divisor up to sqrt(n), n itself is prime.
</span>    <span class="k">return</span> <span class="n">n</span>


<span class="k">def</span> <span class="nf">W_of_N</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute the non-termination weight W(N) for the adapted blog problem.

    Recap of the adapted definition:
      1) Choose k (integer) that maximizes f(k) = (N/k)^k.
      2) Let M(N) = (N/k)^k in lowest terms.
      3) If M(N) is a terminating decimal =&gt; W(N) = 0.
         Otherwise W(N) is the smallest prime divisor of the reduced denominator.

    Key simplification:
      M(N) = (N/k)^k = N^k / k^k.
      In lowest terms, its denominator is (k / gcd(N, k))^k.
      So the </span><span class="sh">"</span><span class="s">obstructing</span><span class="sh">"</span><span class="s"> primes come entirely from k after canceling gcd(N, k).
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">N must be &gt;= 2</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Continuous maximizer from calculus: k* = N/e
</span>    <span class="n">k_star</span> <span class="o">=</span> <span class="n">N</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">e</span>

    <span class="c1"># Since k must be an integer, it suffices to check the nearest integers:
</span>    <span class="c1"># floor(k*) and ceil(k*). (For N&gt;=2 this never hits k=0.)
</span>    <span class="n">k_candidates</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="n">k_star</span><span class="p">),</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">k_star</span><span class="p">))</span>

    <span class="c1"># Choose the better k by comparing log(f(k)) instead of f(k) directly.
</span>    <span class="c1"># This avoids huge numbers because f(k) grows/decays exponentially.
</span>    <span class="k">def</span> <span class="nf">log_f</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="c1"># For our N range, k is always &gt;= 1, but guard anyway.
</span>        <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-inf</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>

    <span class="n">best_k</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">k_candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">log_f</span><span class="p">)</span>

    <span class="c1"># Reduce (N/best_k)^best_k = N^k / k^k to lowest terms.
</span>    <span class="c1"># Cancelling gcd(N, k) once is enough because the denominator becomes:
</span>    <span class="c1">#   (k / gcd(N,k))^k
</span>    <span class="n">g</span> <span class="o">=</span> <span class="nf">gcd</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">best_k</span><span class="p">)</span>
    <span class="n">reduced_base_denom</span> <span class="o">=</span> <span class="n">best_k</span> <span class="o">//</span> <span class="n">g</span>  <span class="c1"># denominator of (N/k) in lowest terms
</span>
    <span class="c1"># If reduced_base_denom has only prime factors 2 and 5, then its k-th power does too,
</span>    <span class="c1"># hence M(N) is a terminating decimal.
</span>    <span class="k">if</span> <span class="nf">is_terminating_decimal_of_rational</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">best_k</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="c1"># Otherwise, the smallest "new" prime in the reduced denominator base is exactly the
</span>    <span class="c1"># smallest obstructing prime for the full denominator as well (powers don't change
</span>    <span class="c1"># the set of prime factors).
</span>    <span class="k">return</span> <span class="nf">smallest_prime_factor</span><span class="p">(</span><span class="n">reduced_base_denom</span><span class="p">)</span>


<span class="c1"># -----------------------------
# Compute the required sum
# -----------------------------
</span><span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1_000_000</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="nc">W_of_N</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">total</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="mathematics"/><category term="algorithms"/><category term="number-theory"/><category term="programming"/><category term="optimization"/><category term="number-theory"/><category term="discrete-math"/><category term="calculus"/><category term="python"/><category term="computational-math"/><summary type="html"><![CDATA[We study how splitting an integer into equal parts affects the maximum attainable product and how arithmetic properties of the optimum emerge. In particular, we relate a continuous optimization problem to the decimal structure of the resulting rational values.]]></summary></entry><entry><title type="html">Short Notes: Summing Non-Isolated Divisors Across All Subsets</title><link href="https://markusthill.github.io/blog/2026/pe-844p100-divisors-sets/" rel="alternate" type="text/html" title="Short Notes: Summing Non-Isolated Divisors Across All Subsets"/><published>2026-01-06T09:00:51+00:00</published><updated>2026-01-06T09:00:51+00:00</updated><id>https://markusthill.github.io/blog/2026/pe-844p100-divisors-sets</id><content type="html" xml:base="https://markusthill.github.io/blog/2026/pe-844p100-divisors-sets/"><![CDATA[<p><br/></p> <h2 id="problem-description">Problem Description</h2> <p>Take the numbers from \(1\) up to some number \(n\) and consider <strong>all possible subsets</strong> of these numbers.</p> <p>Inside any chosen subset, some numbers may be <em>isolated</em> while others are <em>connected</em> to the rest of the set by divisibility:</p> <ul> <li>A number is <strong>isolated</strong> if it does <strong>not</strong> divide any other <em>different</em> number in the subset.</li> <li>A number is <strong>non-isolated</strong> if it divides at least one other element of the same subset.</li> </ul> <p>For each subset, we add up all <strong>non-isolated</strong> numbers.<br/> Finally, we sum these values over <strong>all</strong> subsets of \(\{1,2,\dots,n\}\).</p> <p>Our task is to compute the same quantity for a very large value of \(n\) (e.g., \(10^{12}\)), and report the answer modulo a given number.</p> <p><br/></p> <h3 id="examples">Examples</h3> <ul> <li> <p>Subset \(A=\{2,3\}\)<br/> Neither number divides the other, so both are isolated.<br/> Contribution: \(0\)</p> </li> <li> <p>Subset \(A=\{2,4\}\)<br/> The number \(2\) divides \(4\), so \(2\) is non-isolated.<br/> The number \(4\) does not divide any other element.<br/> Contribution: \(2\)</p> </li> <li> <p>Subset \(A=\{1,3,6\}\)<br/> The number \(1\) divides both \(3\) and \(6\).<br/> The number \(3\) divides \(6\).<br/> All except \(6\) are non-isolated.<br/> Contribution: \(1 + 3 = 4\)</p> </li> <li> <p>Subset \(A=\{5\}\)<br/> A single element cannot divide another <em>distinct</em> element.<br/> Contribution: \(0\)</p> </li> </ul> <p><br/></p> <h3 id="more-formal-problem-description">More Formal Problem Description</h3> <p>Let \(A\) be a finite subset of the positive integers.</p> <p>An element \(x \in A\) is included in the sum for \(A\) if there exists another element \(y \in A\) with \(y \neq x\) such that \(x \mid y.\)</p> <p>Define the function</p> \[f(A) = \sum_{\substack{x \in A \\ \exists\, y \in A,\; y \neq x,\; x \mid y}} x,\] <p>that is, the sum of all elements of \(A\) that divide at least one other element of \(A\).</p> <p>For a positive integer \(n\), define</p> \[S(n) = \sum_{A \subseteq \{1,2,\dots,n\}} f(A),\] <p>where the sum ranges over all subsets of \(\{1,2,\dots,n\}\).</p> <hr/> <p><br/></p> <h2 id="a-straightforward-baseline-solution">A Straightforward Baseline Solution</h2> <p>The code below implements a straight-forward, brute-force solution that follows the problem definition literally: it explicitly enumerates all subsets of \(\{1,\dots,n\}\), checks within each subset which elements divide at least one other element, sums those elements, and finally aggregates the result over all subsets. While this approach is easy to understand and closely mirrors the mathematical definition of \(S(n)\), it quickly becomes infeasible, as we will see below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>


<span class="k">def</span> <span class="nf">powerset</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
    <span class="sh">"""</span><span class="s">Return all subsets of A with size &gt;= 2 (since size 0/1 cannot contribute).</span><span class="sh">"""</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">set</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">items</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">r</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Sum of all x in A that divide at least one other (distinct) element of A.</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">any</span><span class="p">((</span><span class="n">y</span> <span class="o">%</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">A</span> <span class="o">-</span> <span class="p">{</span><span class="n">x</span><span class="p">}):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">total</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Naively compute S(n) by enumerating all subsets of {1, ..., n}.</span><span class="sh">"""</span>
    <span class="n">universe</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">f</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="nf">powerset</span><span class="p">(</span><span class="n">universe</span><span class="p">))</span>


<span class="n">n</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">result</span> <span class="o">=</span> <span class="nc">S</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Solution:</span><span class="sh">"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">result</span> <span class="o">==</span> <span class="mi">9855</span>  <span class="c1"># expected answer
</span></code></pre></div></div> <p><strong>What this code does</strong></p> <p>The goal is to compute \(S(n)\), defined as follows:</p> <ol> <li>Consider <strong>all subsets</strong> \(A \subseteq \{1,2,\dots,n\}\).</li> <li>For each subset \(A\), compute \(f(A)\): <ul> <li>For every \(x \in A\), check whether there exists a <em>different</em> element \(y \in A\) such that \(x \mid y\).</li> <li>If such a \(y\) exists, then \(x\) is counted <strong>once</strong> (even if it divides multiple elements).</li> <li>Sum all counted \(x\) values to obtain \(f(A)\).</li> </ul> </li> <li>Sum \(f(A)\) over all subsets \(A\) to get \(S(n)\).</li> </ol> <p><strong>How the functions map to this definition</strong></p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">powerset(A)</code><br/> Generates all subsets of <code class="language-plaintext highlighter-rouge">A</code> with size at least 2.<br/> (Subsets of size 0 or 1 cannot contribute to \(f(A)\), because there is no <em>distinct</em> second element to be divisible by.)</p> </li> <li><code class="language-plaintext highlighter-rouge">f(A)</code><br/> Implements the rule ‚Äúsum all elements that divide at least one other distinct element in the subset‚Äù. The line <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">any</span><span class="p">((</span><span class="n">y</span> <span class="o">%</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">A</span> <span class="o">-</span> <span class="p">{</span><span class="n">x</span><span class="p">})</span>
</code></pre></div> </div> <p>checks whether \(x\) divides at least one other element \(y\) in the subset.</p> </li> <li><code class="language-plaintext highlighter-rouge">S(n)</code> Builds the universe set<br/> \(\{1, \dots, n\}\), enumerates all relevant subsets, applies \(f\) to each, and sums the results.</li> </ul> <p>The snippet runs this for \(n = 11\) and verifies the known value \(S(11) = 9855\).</p> <p><br/></p> <h3 id="the-above-code-is-slow">The above Code is slow</h3> <p>However, this becomes infeasible quickly. Here is why:</p> <ol> <li>The number of subsets explodes exponentially. The set \(\{1,2,\dots,n\}\) has \(2^n\) subsets in total. As explained in <a href="#why-are-there-2n-subsets">Why are there (2^n) subsets?</a>, the exponential growth comes from independent include/exclude decisions. Even though the code skips subsets of size 0 and 1, the count is still essentially \(2^n - (n + 1),\) which is still exponential growth. Concrete sizes: <ul> <li>\(n = 20 \Rightarrow 2^{20} = 1{,}048{,}576\) subsets (about one million)</li> <li>\(n = 30 \Rightarrow 2^{30} \approx 1.07 \cdot 10^9\) subsets (about a billion)</li> <li>\(n = 40 \Rightarrow 2^{40} \approx 1.10 \cdot 10^{12}\) subsets (a trillion) So just looping over all subsets becomes impossible very quickly.</li> </ul> </li> <li>Each subset also has internal work. For each subset \(A\), the function \(f(A)\) performs nested checks: <ul> <li>It loops over each \(x \in A\).</li> <li>For each \(x\), it scans the other elements \(A \setminus \{x\}\) until it finds a multiple. In the worst case (when few divisibility relations exist), this is roughly quadratic in the subset size: \(\text{work per subset} \sim O(|A|^2).\) So the total runtime behaves roughly like: \(\sum_{A \subseteq \{1,\dots,n\}} O(|A|^2),\) which is exponential overall, with a large constant factor coming from the inner checks.</li> </ul> </li> <li>Memory usage can also become a bottleneck. The function <code class="language-plaintext highlighter-rouge">powerset(...)</code> returns a full <code class="language-plaintext highlighter-rouge">list[set[int]]</code> of <em>all</em> subsets at once.<br/> This means the program stores every subset in memory before it even starts summing. For moderately large \(n\), this alone can exhaust available RAM, independent of runtime.</li> </ol> <p>This code is a simple and functioning baseline, but it is fundamentally limited because it performs an exhaustive enumeration over roughly \(2^n\) subsets and then does additional work inside each subset. That exponential growth is why values like \(S(20)\) already become slow, and why computing \(S(10^{12})\) requires a completely different, non-enumerative approach.</p> <p>In the following, we will attempt to find an efficient approach, step-by-step.</p> <hr/> <p><br/></p> <h2 id="from-brute-force-to-a-combinatorial-viewpoint">From Brute Force to a Combinatorial Viewpoint</h2> <p>The brute-force approach mirrors the definition of \(S(n)\) directly, but its exponential complexity makes it unsuitable for large values of \(n\). To make progress, we need to rethink <em>what</em> we are counting and <em>how</em> we count it.</p> <p>This section takes a first step in that direction by changing perspective. Instead of iterating over all subsets and computing their contributions one by one, we begin to analyze the problem <strong>element-wise</strong>: we fix a single number \(x\) and ask how often it contributes across all subsets. This shift lays the groundwork for a fully combinatorial formulation that avoids explicit subset enumeration.</p> <p><br/></p> <h3 id="counting-contributions-per-element-still-brute-force">Counting Contributions per Element (Still Brute Force)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Try different values of n and x to see how many subsets a single x contributes to
# for n=11 and x=3, expected: 768 subsets
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">universe</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">subsets</span> <span class="o">=</span> <span class="nf">powerset</span><span class="p">(</span><span class="n">universe</span><span class="p">)</span>

<span class="n">count_contributing_subsets</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">subsets</span><span class="p">:</span>
    <span class="n">contributes</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">A</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">any</span><span class="p">((</span><span class="n">y</span> <span class="o">!=</span> <span class="n">x</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">y</span> <span class="o">%</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">A</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">contributes</span><span class="p">:</span>
        <span class="n">count_contributing_subsets</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> contributes in </span><span class="si">{</span><span class="n">count_contributing_subsets</span><span class="si">}</span><span class="s"> subsets of </span><span class="si">{</span> <span class="si">{</span><span class="mi">1</span><span class="p">,...,</span><span class="si">{</span><span class="n">n</span><span class="si">}}</span> <span class="si">}</span><span class="s">. In total 2^n-(n+1)=</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">subsets</span><span class="p">)</span><span class="si">}</span><span class="s"> subsets.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Expected output:</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3 contributes in 768 subsets of {1,...,11}. In total 2^n-(n+1)=2036 subsets.
</code></pre></div></div> <p><strong>What this computation is counting</strong></p> <p>The idea is to <strong>fix a single value</strong> \(x\) and ask:</p> <p>Among all subsets \(A \subseteq \{1,2,\dots,n\}\), in how many of them does \(x\) actually <em>contribute</em> to the overall sum \(S(n)\)?</p> <p>An element \(x\) contributes to a subset \(A\) exactly when:</p> <ol> <li>\(x \in A\), and</li> <li>there exists some <em>other</em> element \(y \in A\) with \(y \neq x\) such that<br/> \(x \mid y.\)</li> </ol> <p>In words: the subset must contain \(x\) <strong>and</strong> at least one <em>distinct multiple</em> of \(x\).</p> <p><strong>Why this is useful for a combinatorial approach</strong></p> <p>The naive brute-force approach computes \(S(n)\) by iterating over all subsets and summing contributions inside each subset.</p> <p>Here we flip the perspective:</p> <ul> <li>Instead of asking ‚Äúwhat is the contribution of this subset?‚Äù,</li> <li>we ask ‚Äúin how many subsets does this particular \(x\) contribute?‚Äù.</li> </ul> <p>If we can count the number of subsets where \(x\) contributes, then the total contribution of \(x\) to \(S(n)\) is simply:</p> \[x \cdot \#\{A \subseteq \{1,\dots,n\} : x \in A \text{ and } \exists\, y \in A,\ y \neq x,\ x \mid y\}.\] <p>Summing this quantity over all \(x \in \{1,\dots,n\}\) reconstructs \(S(n)\), but in a way that is much more amenable to mathematical counting arguments (and avoids enumerating all subsets explicitly).</p> <p><strong>About the ‚Äútotal number of subsets‚Äù mentioned</strong></p> <p>The enumeration in this experiment typically ignores subsets of size 0 or 1, because such subsets can never contain a pair of distinct elements where one divides the other. So the total number of considered subsets is</p> \[2^n - (n+1),\] <p>i.e. all subsets minus the empty set and the \(n\) singletons.</p> <p><br/></p> <h3 id="counting-contributing-subsets-via-binomial-coefficients">Counting Contributing Subsets via Binomial Coefficients</h3> <p>Having identified how often a fixed element \(x\) contributes across all subsets, we now replace explicit subset enumeration by a counting argument based on binomial coefficients.<br/> The code below implements this idea in a direct and still-naive form, preparing the ground for a fully closed-form combinatorial solution.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">factorial</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute n! for n &gt;= 0 (naive iterative implementation).</span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">0</span>
    <span class="n">fac</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">fac</span> <span class="o">*=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">fac</span>


<span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute the binomial coefficient </span><span class="sh">'</span><span class="s">n choose k</span><span class="sh">'</span><span class="s"> (naive implementation).</span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">&lt;=</span> <span class="n">n</span>
    <span class="k">return</span> <span class="nf">factorial</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="nf">factorial</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="nf">factorial</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># We want to count how many subsets A ‚äÜ {1,...,n} satisfy:
#   (1) x ‚àà A, and
#   (2) there exists some y ‚àà A with y ‚â† x and x | y.
#
# In other words: A must contain x and at least one *distinct multiple* of x.
</span>
<span class="c1"># Count the multiples of x in {1,...,n}, excluding x itself.
# Multiples of x are: x, 2x, 3x, ..., floor(n/x)*x.
# That is floor(n/x) many multiples total, and removing x leaves:
</span><span class="n">num_multiples_of_x_excluding_x</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># Partition the universe {1,...,n} into:
#   - the required element {x}
#   - the "good" elements: multiples of x (excluding x), from which we must pick ‚â• 1
#   - the "free" elements: all remaining numbers (not equal to x and not a multiple of x),
#     from which we may pick any amount (including 0)
</span><span class="n">m</span> <span class="o">=</span> <span class="n">num_multiples_of_x_excluding_x</span>               <span class="c1"># "good" pool size
</span><span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>                                    <span class="c1"># "free" pool size (everything else)
</span>
<span class="c1"># Now count subsets by two independent choices:
#   - choose i &gt;= 1 elements from the m "good" multiples (to ensure x divides something)
#   - choose j &gt;= 0 elements from the r "free" elements (arbitrary)
#
# Each pair (i, j) yields: choose(m, i) * choose(r, j) subsets,
# and we always include x itself (so x ‚àà A is guaranteed).
#
# This is still a brute-force sum over i and j, but it avoids enumerating subsets A.
</span>
<span class="c1"># Sanity check targets for (n=11):
#   x=1 -&gt; 1023
#   x=2 -&gt; 960
#   x=3 -&gt; 768
</span><span class="n">count_contributing_subsets</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>            <span class="c1"># must pick at least one multiple of x
</span>    <span class="n">ways_pick_multiples</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>        <span class="c1"># may pick any number of remaining elements
</span>        <span class="n">ways_pick_rest</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="n">count_contributing_subsets</span> <span class="o">+=</span> <span class="n">ways_pick_multiples</span> <span class="o">*</span> <span class="n">ways_pick_rest</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> contributes in </span><span class="si">{</span><span class="n">count_contributing_subsets</span><span class="si">}</span><span class="s"> subsets of </span><span class="si">{</span> <span class="si">{</span><span class="mi">1</span><span class="p">,...,</span><span class="si">{</span><span class="n">n</span><span class="si">}}</span> <span class="si">}</span><span class="s">. In total 2^n-(n+1)=</span><span class="si">{</span><span class="mi">2</span><span class="o">**</span><span class="n">n</span><span class="o">-</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s"> subsets.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Expected output:</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3 contributes in 768 subsets of {1,...,11}. In total 2^n-(n+1)=2036 subsets.
</code></pre></div></div> <p>This code counts, for fixed values of \(n\) and \(x\), how many subsets \(A \subseteq \{1,\dots,n\}\) satisfy the condition that <strong>\(x\) contributes</strong> to the sum \(S(n)\), without explicitly enumerating all subsets.</p> <p>The helper functions</p> <ul> <li><code class="language-plaintext highlighter-rouge">factorial(n)</code> computes \(n!\),</li> <li><code class="language-plaintext highlighter-rouge">choose(n, k)</code> computes the binomial coefficient \(\binom{n}{k},\) which counts the number of ways to choose \(k\) elements from a set of size \(n\). These are used as basic counting primitives.</li> </ul> <p>A subset \(A\) contributes for a given \(x\) if and only if:</p> <ol> <li>\(x \in A\), and</li> <li>\(A\) contains at least one <em>distinct multiple</em> of \(x\).</li> </ol> <p>To exploit this structure, the set \(\{1,\dots,n\}\) is partitioned into three disjoint parts:</p> <ul> <li>the fixed element \(\{x\}\), which must always be included,</li> <li>the set of <em>multiples of</em> \(x\) greater than \(x\) itself,</li> <li>all remaining elements, which are neither equal to \(x\) nor divisible by \(x\).</li> </ul> <p>If there are \(m\) multiples of \(x\) larger than \(x\) and \(r\) remaining elements, then every contributing subset is uniquely determined by:</p> <ul> <li>choosing at least one element from the \(m\) multiples, and</li> <li>choosing any number of elements from the \(r\) remaining elements.</li> </ul> <p>For a fixed choice of:</p> <ul> <li>\(i \ge 1\) multiples of \(x\), and</li> <li>\(j \ge 0\) remaining elements,</li> </ul> <p>there are</p> \[\binom{m}{i} \cdot \binom{r}{j}\] <p>distinct subsets, and each of them automatically contains \(x\).</p> <p>The double loop over \(i\) and \(j\) sums these contributions, effectively counting all subsets in which \(x\) contributes, but without ever listing the subsets themselves.</p> <p>This approach still performs explicit summation, but it has replaced subset enumeration by pure combinatorial counting. It demonstrates how the contribution of a single element \(x\) can be expressed in terms of binomial coefficients, a small step toward deriving a closed-form expression for \(S(n)\).</p> <p><br/></p> <h3 id="element-wise-computation-of-sn-via-combinatorial-counting">Element-Wise Computation of \(S(n)\) via Combinatorial Counting</h3> <p>Building on the contribution-counting idea for a fixed element \(x\), the following implementation assembles the full value of \(S(n)\) by summing the weighted contributions of all \(x \in \{1,\dots,n\}\). While still computationally naive, it avoids explicit subset enumeration and makes the underlying combinatorial structure of the problem explicit.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via element-wise combinatorial counting (still naive).</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Number of remaining elements (neither x nor a multiple of x)
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Count how many subsets A ‚äÜ {1,...,n} make x contribute
</span>        <span class="n">count_contributing_subsets</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Choose at least one multiple of x
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">ways_pick_multiples</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

            <span class="c1"># Choose any number of remaining elements
</span>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">ways_pick_rest</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
                <span class="n">count_contributing_subsets</span> <span class="o">+=</span> <span class="n">ways_pick_multiples</span> <span class="o">*</span> <span class="n">ways_pick_rest</span>

        <span class="c1"># Each such subset contributes x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">count_contributing_subsets</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">11</span>
<span class="nc">S</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># should be 9855
</span></code></pre></div></div> <p><em>Expected output:</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9855
</code></pre></div></div> <p>This function computes \(S(n)\) by summing the contributions of each individual element \(x \in \{1,\dots,n\}\) separately, rather than iterating over all subsets.</p> <p>For a fixed value of \(x\), the code counts how many subsets \(A \subseteq \{1,\dots,n\}\) satisfy the condition that \(x\) contributes, i.e. that \(A\) contains \(x\) and at least one other element divisible by \(x\). This is done by partitioning the universe into multiples of \(x\) and all remaining elements, and then counting valid subsets using binomial coefficients.</p> <p>Once the number of contributing subsets for \(x\) has been determined, it is multiplied by \(x\) itself, since each such subset contributes exactly \(x\) to the overall sum. Summing this quantity over all \(x\) reconstructs \(S(n)\).</p> <p>With this element-wise counting approach, we can already evaluate \(S(n)\) for moderately larger values of \(n\) than before. For example, we can compute \(S(20):\)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div> <p><em>Expected output:</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>18626559
</code></pre></div></div> <p>And even \(S(200)\):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div></div> <p><em>Expected output:</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2664683200606651329234017512985870589238327005040580368858611711
</code></pre></div></div> <p>These results confirm the correctness of the combinatorial reformulation and demonstrate a significant improvement over the original subset enumeration (try it yourself for different small values of \(n\) and compare with our initial naive solution).</p> <p>Nevertheless, the implementation still performs explicit summations for each \(x \in \{1,\dots,n\}\) and relies on repeated evaluations of binomial coefficients. As a result, the runtime grows too quickly, and the method becomes impractical for large values of \(n\)‚Äîin particular for the target value \(n = 10^{14}\). A further simplification to closed-form expressions is therefore required.</p> <hr/> <p><br/></p> <h2 id="step-by-step-combinatorial-simplifications">Step-by-Step Combinatorial Simplifications</h2> <h3 id="first-simplification-collapsing-the-inner-sum">First Simplification: Collapsing the Inner Sum</h3> <p>As a first optimization, we focus on the innermost loop of the combinatorial expression. By recognizing that the sum over all binomial coefficients corresponding to the ‚Äúfree‚Äù elements does not depend on how many multiples of \(x\) are chosen, this inner summation can be factored out and simplified. This reduces redundant computation and reveals a familiar combinatorial pattern that will be exploited further in the next steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via element-wise combinatorial counting (inner sum simplified).</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># r = number of remaining elements (neither x nor a multiple of x)
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Precompute the inner sum over j (still written as a sum here).
</span>        <span class="c1"># This counts all ways to choose any subset of the r "free" elements.
</span>        <span class="n">sum_over_rest</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">choose</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">count_contributing_subsets</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># must pick at least one multiple of x
</span>            <span class="n">ways_pick_multiples</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">count_contributing_subsets</span> <span class="o">+=</span> <span class="n">ways_pick_multiples</span> <span class="o">*</span> <span class="n">sum_over_rest</span>

        <span class="c1"># Each contributing subset adds x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">count_contributing_subsets</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <p><br/></p> <h3 id="second-simplification-replacing-the-inner-binomial-sum-by-a-power-of-two">Second Simplification: Replacing the Inner Binomial Sum by a Power of Two</h3> <p>The inner sum over the ‚Äúfree‚Äù elements counts all ways to choose an arbitrary subset from a pool of size \(r\). In combinatorial terms, this is exactly the size of a power set:</p> \[\sum_{j=0}^{r} \binom{r}{j} = 2^{r}.\] <p>Using the notation from above, we have:</p> <ul> <li>\(m = \left\lfloor \frac{n}{x} \right\rfloor - 1\) (multiples of \(x\) larger than \(x\)),</li> <li>\(r = n - m - 1\) (all remaining ‚Äúfree‚Äù elements).</li> </ul> <p>The inner sum runs over all possible choices of elements from the \(r\) ‚Äúfree‚Äù elements. For each fixed \(j\), the binomial coefficient \(\binom{r}{j}\) counts the number of subsets of size exactly \(j\). Summing over all values of \(j\) therefore counts <strong>all subsets</strong> of a set with \(r\) elements.</p> <p>Equivalently, each of the \(r\) elements can be either included or excluded independently, giving \(2 \cdot 2 \cdots 2 = 2^{r}\) possible subsets.</p> <p>Another intuitive way to see this identity is through a binary encoding: Consider the \(r\) ‚Äúfree‚Äù elements and fix an ordering of them. Any subset of these elements can be represented by a binary string of length \(r\): the \(k\)-th bit is set to \(1\) if the \(k\)-th element is included in the subset, and set to \(0\) otherwise. Each bit has two independent choices (on or off), so the total number of such binary strings is \(2 \cdot 2 \cdots 2 = 2^{r}.\) This gives a one-to-one correspondence between subsets and binary strings of length \(r\), showing that there are exactly \(2^{r}\) subsets. Grouping these subsets by the number of bits set to \(1\) recovers the binomial sum: there are \(\binom{r}{j}\) binary strings with exactly \(j\) ones, and summing over \(j = 0,\dots,r\) yields</p> \[\sum_{j=0}^{r} \binom{r}{j} = 2^{r}.\] <p>which in code becomes a single exponentiation. This removes an entire loop and makes the structure of the count much clearer: for each valid choice of at least one multiple of \(x\), the remaining elements can be included or excluded independently, giving \(2^{r}\) possibilities.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via element-wise combinatorial counting (inner sum = power of 2).</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># r = number of remaining elements (neither x nor a multiple of x)
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># All subsets of the r "free" elements: sum_{j=0}^r C(r,j) = 2^r
</span>        <span class="n">sum_over_rest</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">r</span>

        <span class="n">count_contributing_subsets</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># must pick at least one multiple of x
</span>            <span class="n">ways_pick_multiples</span> <span class="o">=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">count_contributing_subsets</span> <span class="o">+=</span> <span class="n">ways_pick_multiples</span> <span class="o">*</span> <span class="n">sum_over_rest</span>

        <span class="c1"># Each contributing subset adds x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">count_contributing_subsets</span>

    <span class="k">return</span> <span class="n">total</span>


<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <p><br/></p> <h3 id="third-simplification-factoring-the-independent-choices">Third Simplification: Factoring the Independent Choices</h3> <p>In this step, we make the independence of the two combinatorial choices explicit. The selection of multiples of \(x\) and the selection of the remaining elements are handled separately: we first sum over all nonempty choices of multiples of \(x\), and only then multiply once by the number of possible choices for the remaining elements. This removes another unnecessary dependency inside the loop and brings the expression closer to a closed-form count.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via element-wise combinatorial counting (factor out 2^r).

    Improvement vs. the previous version:
      - We no longer multiply by 2^r inside the i-loop.
      - Instead, we first compute the i-sum: sum_{i=1..m} C(m,i),
        and only then multiply once by 2^r.
    </span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># r = number of remaining elements (neither x nor a multiple of x)
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Count ways to pick at least one element from the m "good" multiples:
</span>        <span class="c1">#   sum_{i=1..m} C(m, i)
</span>        <span class="n">count_nonempty_multiple_choices</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">count_nonempty_multiple_choices</span> <span class="o">+=</span> <span class="nf">choose</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="c1"># Independent choices for the r "free" elements:
</span>        <span class="c1">#   sum_{j=0..r} C(r, j) = 2^r
</span>        <span class="n">count_free_choices</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">r</span>

        <span class="c1"># Each valid subset contributes x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">count_nonempty_multiple_choices</span> <span class="o">*</span> <span class="n">count_free_choices</span>

    <span class="k">return</span> <span class="n">total</span>


<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <p><br/></p> <h3 id="fourth-simplification-eliminating-the-final-inner-sum">Fourth Simplification: Eliminating the Final Inner Sum</h3> <p>At this stage, both remaining summations can be replaced by closed-form expressions. The number of ways to select at least one multiple of \(x\) becomes \(2^{m}-1\), while the number of ways to select any subset of the remaining elements is \(2^{r}\). This removes all inner loops and yields a compact formula for the contribution of each \(x\), bringing the computation of \(S(n)\) close to its final combinatorial form.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via element-wise combinatorial counting (closed form for both sums).

    Improvement vs. the previous version:
      - The sum over i = 1..m of C(m, i) is replaced by its closed form 2^m - 1.
      - Together with the already simplified factor 2^r, this removes the inner loop
        entirely.
    </span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># r = number of remaining elements (neither x nor a multiple of x)
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Number of nonempty choices of multiples of x:
</span>        <span class="c1">#   sum_{i=1..m} C(m, i) = 2^m - 1
</span>        <span class="n">count_nonempty_multiple_choices</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">m</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Number of choices for the remaining "free" elements:
</span>        <span class="c1">#   sum_{j=0..r} C(r, j) = 2^r
</span>        <span class="n">count_free_choices</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">r</span>

        <span class="c1"># Each valid subset contributes x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">count_nonempty_multiple_choices</span> <span class="o">*</span> <span class="n">count_free_choices</span>

    <span class="k">return</span> <span class="n">total</span>


<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <p><br/></p> <h3 id="fifth-simplification-collapsing-the-expression-into-a-single-power-difference">Fifth Simplification: Collapsing the Expression into a Single Power Difference</h3> <p>In this final simplification step, the remaining combinatorial factors are combined algebraically. The product \((2^{m} - 1)\cdot 2^{r}\) is rewritten as a difference of two powers of two, yielding a compact expression for the number of subsets in which a given element \(x\) contributes. As a result, the contribution of each \(x\) can now be computed in constant time, leaving only a single loop over \(x \in \{1,\dots,n\}\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) via a closed-form contribution per element x.</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Using:
</span>        <span class="c1">#   (2^m - 1) * 2^r
</span>        <span class="c1"># with r = n - m - 1,
</span>        <span class="c1"># this simplifies algebraically to:
</span>        <span class="c1">#   2^(n-1) - 2^r
</span>        <span class="n">contribution_count</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Each such subset contributes x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <p><br/></p> <h3 id="sixth-simplification-reusing-the-global-power-term">Sixth Simplification: Reusing the Global Power Term</h3> <p>In this step, we observe that the term \(2^{\,n-1}\) is independent of \(x\) and can be computed once and reused for all iterations. Only the subtraction term \(2^{\,n-m-1}\) depends on \(x\) through the number of multiples of \(x\).</p> <p>This change does not alter the mathematical structure of the formula, but it removes redundant recomputation and clarifies the separation between the global contribution shared by all elements and the correction term specific to each \(x\). The result is a cleaner and slightly more efficient implementation of the final closed-form expression for \(S(n)\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) using a shared power-of-two term.</span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># This term does not depend on x and can be reused
</span>    <span class="n">base_power</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Contribution count for x:
</span>        <span class="c1">#   2^(n-1) - 2^(n-m-1)
</span>        <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Each such subset contributes x exactly once
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="grouping-values-of-x-by-constant-quotients">Grouping Values of \(x\) by Constant Quotients</h2> <p>In the previous formula, the contribution of each element \(x\) depends on the value</p> \[m = \left\lfloor \frac{n}{x} \right\rfloor - 1,\] <p>rather than on \(x\) itself. A closer inspection reveals that this quantity remains constant over large contiguous ranges of \(x\). The following code illustrates this phenomenon by grouping values of \(x\) according to the same value of \(m\). Making this structure explicit is the key to the next optimization step, where we replace the loop over individual \(x\) by a loop over ranges with identical combinatorial behavior.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">q</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># number of x-values per printed row
</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">base_power</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">current_m</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">row</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># m = number of multiples of x in {1,...,n}, excluding x itself
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="c1"># Collect (x, m) pairs for printing
</span>    <span class="k">if</span> <span class="n">current_m</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">current_m</span> <span class="o">=</span> <span class="n">m</span>

    <span class="k">if</span> <span class="n">m</span> <span class="o">!=</span> <span class="n">current_m</span> <span class="ow">or</span> <span class="nf">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">==</span> <span class="n">q</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">m = </span><span class="si">{</span><span class="n">current_m</span><span class="si">}</span><span class="s">: </span><span class="sh">"</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>
        <span class="n">row</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_m</span> <span class="o">=</span> <span class="n">m</span>

    <span class="n">row</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># print remaining row
</span><span class="k">if</span> <span class="n">row</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">m = </span><span class="si">{</span><span class="n">current_m</span><span class="si">}</span><span class="s">: </span><span class="sh">"</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>

</code></pre></div></div> <p><em>Output</em>:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>m = 99:  [1]
m = 49:  [2]
m = 32:  [3]
m = 24:  [4]
m = 19:  [5]
m = 15:  [6]
m = 13:  [7]
m = 11:  [8]
m = 10:  [9]
m = 9:  [10]
m = 8:  [11]
m = 7:  [12]
m = 6:  [13, 14]
m = 5:  [15, 16]
m = 4:  [17, 18, 19, 20]
m = 3:  [21, 22, 23, 24, 25]
m = 2:  [26, 27, 28, 29, 30, 31, 32, 33]
m = 1:  [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
m = 0:  [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
</code></pre></div></div> <p>This output groups values of \(x\) by the value of</p> \[m = \left\lfloor \frac{n}{x} \right\rfloor - 1\] <p>for \(n = 100\). For small values of \(x\), the quotient \(\left\lfloor \frac{n}{x} \right\rfloor\) changes rapidly, so each value of \(x\) tends to produce a distinct value of \(m\). As \(x\) increases, the quotient varies more slowly, and the same value of \(m\) applies to increasingly large ranges of \(x\).</p> <p>This is why the early groups contain only one element (for example, \(x = 1\) gives \(m = 99\)), while later groups grow wider and wider. In particular, once \(x &gt; \frac{n}{2}\), we have \(\left\lfloor \frac{n}{x} \right\rfloor = 1\), so \(m = 0\) and all remaining values of \(x\) fall into a single large group.</p> <p>This behavior reflects a general property of integer division and is the reason why the final summation over \(x\) can be optimized by iterating over ranges with constant \(m\) rather than over individual values of \(x\).</p> <p>Below you find a small helper snippet that makes the structure behind the quantity \(m = \left\lfloor \frac{n}{x} \right\rfloor - 1\) explicit.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Explore ranges of x for which
#     m = floor(n / x) - 1
# is constant.
</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Fix a value of m
</span><span class="n">m</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># We want all x such that:
#     floor(n / x) - 1 = m
# ‚áî  floor(n / x) = m + 1
#
# This inequality is equivalent to:
#     m + 1 ‚â§ n / x &lt; m + 2
#
# Solving for x gives:
#     n / (m + 2) &lt; x ‚â§ n / (m + 1)
</span>
<span class="n">left</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>   <span class="c1"># smallest integer x satisfying n/x &lt; m+2
</span><span class="n">right</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>     <span class="c1"># largest integer x satisfying n/x ‚â• m+1
</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_range</span> <span class="c1"># should match the result obtained above
</span></code></pre></div></div> <p><em>Output</em>:</p> <pre><code class="language-test">range(26, 34) 
</code></pre> <p>By fixing a value of \(m\) and solving the corresponding inequality for \(x\), we can explicitly determine all values of \(x\) for which the combinatorial contribution depends on the same parameter \(m\). The resulting interval shows that many consecutive values of \(x\) share identical behavior, especially for larger \(x\).</p> <p>This observation is crucial for a further optimization: instead of summing contributions for each \(x\) individually, we can group values of \(x\) with the same \(m\) and handle their total contribution in one step.</p> <p><br/></p> <h3 id="summing-contributions-over-ranges-of-x">Summing Contributions over Ranges of \(x\)</h3> <p>Having identified that the quantity</p> \[m = \left\lfloor \frac{n}{x} \right\rfloor - 1\] <p>remains constant over contiguous intervals of \(x\), we can now exploit this structure directly in the computation of \(S(n)\). Instead of iterating over each value of \(x\) individually, the following code groups together all \(x\) sharing the same \(m\) and accumulates their contributions in bulk. This marks a decisive shift from element-wise summation to interval-wise aggregation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) by grouping x-values with the same m = n//x - 1 (range batching).

    Conceptually unchanged vs. the previous closed-form version:
      - We still use that each x contributes: x * (2^(n-1) - 2^(n-m-1)).
      - The only change in viewpoint is that we do not iterate over every x in 1..n.
        Instead, we iterate over values of m and add the contributions of *ranges* of x
        where m is constant.

    Note:
      - This is still not fully optimized: the summation sum(range(...)) is a bit slow
        and will be replaced later by an O(1) arithmetic-series formula.
    </span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">base_power</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># -------------------------------------------------------------------------
</span>    <span class="c1"># Phase 1: Iterate over possible m-values and find intervals [x_left, x_right]
</span>    <span class="c1"># such that for all x in that interval:
</span>    <span class="c1">#     m = floor(n / x) - 1
</span>    <span class="c1">#
</span>    <span class="c1"># Solving floor(n/x) = m+1 gives:
</span>    <span class="c1">#     n/(m+2) &lt; x &lt;= n/(m+1)
</span>    <span class="c1"># Hence:
</span>    <span class="c1">#     x_left  = floor(n/(m+2)) + 1
</span>    <span class="c1">#     x_right = floor(n/(m+1))
</span>    <span class="c1"># -------------------------------------------------------------------------
</span>    <span class="n">last_left</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">last_right</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">x_left</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">x_right</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Once the interval collapses to a single x, we stop the batching phase
</span>        <span class="c1"># and handle the remaining small x-values individually.
</span>        <span class="k">if</span> <span class="n">x_left</span> <span class="o">==</span> <span class="n">x_right</span><span class="p">:</span>
            <span class="n">last_left</span> <span class="o">=</span> <span class="n">x_left</span>
            <span class="n">last_right</span> <span class="o">=</span> <span class="n">x_right</span>
            <span class="k">break</span>

        <span class="c1"># For this entire interval, m is constant, so the contribution factor is constant.
</span>        <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Sum contributions over all x in [x_left, x_right]:
</span>        <span class="c1">#   sum_{x=x_left..x_right} x * contribution_count
</span>        <span class="c1"># = (sum of x over the interval) * contribution_count
</span>        <span class="c1">#
</span>        <span class="c1"># Still computed "naively" as in your snippet (we'll optimize this later).
</span>        <span class="n">total</span> <span class="o">+=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">x_left</span><span class="p">,</span> <span class="n">x_right</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="c1"># -------------------------------------------------------------------------
</span>    <span class="c1"># Phase 2: Handle the remaining x-values individually.
</span>    <span class="c1">#
</span>    <span class="c1"># The batching above stops when x_left == x_right, i.e. the next interval has
</span>    <span class="c1"># length 1. From that point downward, intervals are tiny, so we just compute
</span>    <span class="c1"># x * factor directly for x = x_left, x_left-1, ..., 1.
</span>    <span class="c1"># -------------------------------------------------------------------------
</span>    <span class="k">assert</span> <span class="n">last_left</span> <span class="o">==</span> <span class="n">last_right</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">last_left</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1234567891234567891234567891</span> <span class="o">==</span> <span class="mi">156208504659765295659151493</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1234567891234567891234567891</span> <span class="o">==</span> <span class="mi">617453040502786108363224072</span>
</code></pre></div></div> <p>Note that we can now also compute \(S(2000)\) or \(S(2 \cdot 10^5)\) in feasible time. However, larger values of \(n\) are still hard for our approach.</p> <p><br/></p> <h3 id="replacing-interval-summation-by-a-closed-form-formula">Replacing Interval Summation by a Closed-Form Formula</h3> <p>In this optimization step, we eliminate the remaining inner summation over intervals of \(x\) by using the closed-form formula for an arithmetic series. Once values of \(x\) are grouped by a constant value of</p> \[m = \left\lfloor \frac{n}{x} \right\rfloor - 1,\] <p>their total contribution can be computed in constant time. This removes all residual linear work inside the main loop and yields an efficient, fully combinatorial implementation of \(S(n)\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) using range batching + O(1) arithmetic-series interval sums.

    We use the closed-form per-element contribution:
        x * (2^(n-1) - 2^(n-m-1)),
    where
        m = n//x - 1
    is constant over contiguous ranges of x.

    Compared to the previous batching version:
      - we compute powers via pow(2, k),
      - and replace sum(range(x_left, x_right+1)) by the O(1) interval sum formula.
    </span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">base_power</span> <span class="o">=</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Phase 1: iterate over m-values and aggregate whole x-intervals at once
</span>    <span class="n">stop_x</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="c1"># Interval of x where floor(n/x) - 1 == m:
</span>        <span class="c1">#   n/(m+2) &lt; x &lt;= n/(m+1)
</span>        <span class="n">x_left</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">x_right</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_left</span> <span class="o">==</span> <span class="n">x_right</span><span class="p">:</span>
            <span class="n">stop_x</span> <span class="o">=</span> <span class="n">x_left</span>
            <span class="k">break</span>

        <span class="c1"># Constant factor for all x in this interval (depends only on m)
</span>        <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Sum_{x=x_left..x_right} x  (arithmetic series, computed in O(1))
</span>        <span class="n">sum_x_interval</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_left</span> <span class="o">+</span> <span class="n">x_right</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="n">total</span> <span class="o">+=</span> <span class="n">sum_x_interval</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="c1"># Phase 2: handle the remaining small x-values individually
</span>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">stop_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1234567891234567891234567891</span> <span class="o">==</span> <span class="mi">156208504659765295659151493</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1234567891234567891234567891</span> <span class="o">==</span> <span class="mi">617453040502786108363224072</span>
</code></pre></div></div> <p><br/></p> <h3 id="optimized-implementation-with-optional-modulus">Optimized Implementation with Optional Modulus</h3> <p>We now arrive at a optimized version of the algorithm that combines all previous combinatorial insights. By batching values of \(x\) with identical \(m = \left\lfloor \frac{n}{x} \right\rfloor - 1,\) using closed-form arithmetic-series sums, and supporting fast modular exponentiation, the computation of \(S(n)\) becomes feasible even for very large values of \(n\).</p> <p>This implementation allows the use of an optional modulus to keep intermediate values manageable. When a modulus is provided, all arithmetic is performed modulo that value; otherwise, the exact (and potentially enormous) integer result is computed. The remaining runtime depends mainly on the number of distinct values of \(m\), which grows on the order of \(O(\sqrt{n})\), making this approach scalable to inputs far beyond what brute-force methods could handle.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>


<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">modulus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Compute S(n) using range batching and closed-form interval sums.

    This version supports an optional modulus:
      - If `modulus` is None, all computations are done with exact integers.
      - If `modulus` is given, all arithmetic is performed modulo `modulus`.

    Core idea (unchanged):
      - Each x contributes: x * (2^(n-1) - 2^(n-m-1)),
        where m = n//x - 1.
      - Values of x are grouped into intervals where m is constant.
      - Contributions over each interval are summed using the arithmetic series formula.
    </span><span class="sh">"""</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Precompute the global power term (modular or exact)
</span>    <span class="k">if</span> <span class="n">modulus</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">base_power</span> <span class="o">=</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_power</span> <span class="o">=</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">modulus</span><span class="p">)</span>

    <span class="c1"># ------------------------------------------------------------------
</span>    <span class="c1"># Phase 1: batch ranges of x where m = floor(n/x) - 1 is constant
</span>    <span class="c1"># ------------------------------------------------------------------
</span>    <span class="n">stop_x</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="c1"># All x satisfying floor(n/x) - 1 = m lie in:
</span>        <span class="c1">#   n/(m+2) &lt; x &lt;= n/(m+1)
</span>        <span class="n">x_left</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">x_right</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_left</span> <span class="o">==</span> <span class="n">x_right</span><span class="p">:</span>
            <span class="n">stop_x</span> <span class="o">=</span> <span class="n">x_left</span>
            <span class="k">break</span>

        <span class="c1"># Contribution factor for this whole interval
</span>        <span class="k">if</span> <span class="n">modulus</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">modulus</span><span class="p">)</span>
            <span class="n">contribution_count</span> <span class="o">%=</span> <span class="n">modulus</span>

        <span class="c1"># Sum of x over the interval [x_left, x_right]
</span>        <span class="n">interval_sum</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_left</span> <span class="o">+</span> <span class="n">x_right</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">modulus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">interval_sum</span> <span class="o">%=</span> <span class="n">modulus</span>

        <span class="n">total</span> <span class="o">+=</span> <span class="n">interval_sum</span> <span class="o">*</span> <span class="n">contribution_count</span>
        <span class="k">if</span> <span class="n">modulus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">%=</span> <span class="n">modulus</span>

    <span class="c1"># ------------------------------------------------------------------
</span>    <span class="c1"># Phase 2: handle the remaining small x-values individually
</span>    <span class="c1"># ------------------------------------------------------------------
</span>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">stop_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">modulus</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">contribution_count</span> <span class="o">=</span> <span class="n">base_power</span> <span class="o">-</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">modulus</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">contribution_count</span>
            <span class="n">total</span> <span class="o">%=</span> <span class="n">modulus</span>

    <span class="k">return</span> <span class="n">total</span>

<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9855</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">18626559</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2664683200606651329234017512985870589238327005040580368858611711</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1234567891234567891234567891</span><span class="p">)</span> <span class="o">==</span> <span class="mi">156208504659765295659151493</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1234567891234567891234567891</span><span class="p">)</span> <span class="o">==</span> <span class="mi">617453040502786108363224072</span>
<span class="k">assert</span> <span class="nc">S</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1234567891234567891234567891</span><span class="p">)</span> <span class="o">==</span> <span class="mi">481424271854145029777746921</span>
</code></pre></div></div> <p>Now, also solutions for \(S(2 \cdot 10^{10})\) can be computed without problems (considering a modulus which is sufficiently small). Computing \(S(10^{14}, 1234567891)\) needs around 2-3 minutes.</p> <hr/> <p><br/></p> <h2 id="appendix">Appendix</h2> <h3 id="why-are-there-2n-subsets">Why are there \(2^n\) subsets?</h3> <p>Think of building a subset \(A \subseteq \{1,2,\dots,n\}\) as making <strong>\(n\) independent yes/no decisions</strong>:</p> <ul> <li>For element \(1\): <strong>in</strong> the subset (bit = 1) or <strong>not in</strong> the subset (bit = 0)?</li> <li>For element \(2\): in (1) or out (0)?</li> <li>‚Ä¶</li> <li>For element \(n\): in (1) or out (0)?</li> </ul> <p>So every subset can be represented by a binary vector of length \(n\). This is exactly the same idea as a truth table in logic (‚ÄúWahrheitstafel‚Äù): each row is one assignment of \(n\) bits.</p> <h4 id="small-example-n--3">Small example: \(n = 3\)</h4> <p>Universe: \(\{1,2,3\}\)</p> <p>Each subset corresponds to one row in this table:</p> <table> <thead> <tr> <th style="text-align: right">bit for 1</th> <th style="text-align: right">bit for 2</th> <th style="text-align: right">bit for 3</th> <th style="text-align: left">subset</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">0</td> <td style="text-align: right">0</td> <td style="text-align: right">0</td> <td style="text-align: left">\(\emptyset\)</td> </tr> <tr> <td style="text-align: right">0</td> <td style="text-align: right">0</td> <td style="text-align: right">1</td> <td style="text-align: left">\(\{3\}\)</td> </tr> <tr> <td style="text-align: right">0</td> <td style="text-align: right">1</td> <td style="text-align: right">0</td> <td style="text-align: left">\(\{2\}\)</td> </tr> <tr> <td style="text-align: right">0</td> <td style="text-align: right">1</td> <td style="text-align: right">1</td> <td style="text-align: left">\(\{2,3\}\)</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0</td> <td style="text-align: right">0</td> <td style="text-align: left">\(\{1\}\)</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0</td> <td style="text-align: right">1</td> <td style="text-align: left">\(\{1,3\}\)</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">1</td> <td style="text-align: right">0</td> <td style="text-align: left">\(\{1,2\}\)</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">1</td> <td style="text-align: right">1</td> <td style="text-align: left">\(\{1,2,3\}\)</td> </tr> </tbody> </table> <p>How many rows are there?</p> <ul> <li>3 bit positions,</li> <li>each bit can be 0 or 1 (2 choices),</li> <li>choices are independent,</li> </ul> <p>so the total number is:</p> \[2 \cdot 2 \cdot 2 = 2^3.\] <h4 id="general-case-n">General case: \(n\)</h4> <p>By the same logic, there are \(n\) independent bits, each with 2 possibilities, hence</p> \[\underbrace{2 \cdot 2 \cdot \dots \cdot 2}_{n\text{ times}} = 2^n\] <p>different bit patterns, and therefore \(2^n\) subsets.</p> <p>That‚Äôs why brute-forcing ‚Äúall subsets‚Äù explodes so quickly: increasing \(n\) by 1 <strong>doubles</strong> the number of subsets.</p> <p><em>(And if we exclude the empty set and all singletons, we subtract \(1 + n\), leaving \(2^n - (n+1)\) subsets.)</em></p>]]></content><author><name></name></author><category term="programming"/><category term="math"/><category term="combinatorics"/><category term="number-theory"/><category term="divisibility"/><category term="powerset"/><category term="optimization"/><category term="python"/><summary type="html"><![CDATA[A step-by-step combinatorial derivation of an efficient algorithm to compute S(n): the total sum of subset elements that divide another element in the same subset. The post shows how a brute-force exponential problem can be transformed into a fast method using number-theoretic structure and closed-form counting.]]></summary></entry><entry><title type="html">Backpropagation from Scratch: Feed-Forward Neural Networks in Matrix Notation</title><link href="https://markusthill.github.io/blog/2025/neural-nets-and-backprop/" rel="alternate" type="text/html" title="Backpropagation from Scratch: Feed-Forward Neural Networks in Matrix Notation"/><published>2025-12-16T08:00:51+00:00</published><updated>2025-12-16T08:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/neural-nets-and-backprop</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/neural-nets-and-backprop/"><![CDATA[<p>\( \renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}} \def\matr#1{\boldsymbol{\mathbf{#1}}} \def\Wmatr{\matr{W}} \newcommand{\din}{\mathord{d_0}} \newcommand{\batch}{\mathord{N}} \newcommand{\yhat}{\vec{\hat{y}}} \)</p> <h1 id="introduction">Introduction</h1> <p>Neural networks have been explained a thousand times ‚Äî so why another intro?</p> <p>Because for many people (my past self included) the hard part isn‚Äôt ‚Äúdeep math‚Äù, it‚Äôs getting the <strong>nuts and bolts</strong> of training straight: what exactly happens in the forward pass, where the gradients come from, and how <em>backpropagation</em> is really just an efficient, structured application of the chain rule that makes gradient descent practical.</p> <p>The basics are also the best entry point: once you understand a plain fully connected feed-forward network end to end, the same ideas carry over to CNNs, recurrent nets, and even transformers. In my experience, neural networks are not conceptually difficult - the main obstacle is notation. At the end of the day it‚Äôs often <strong>‚Äúnotation, notation, notation‚Äù</strong> (and much less multivariate calculus than it first appears).</p> <p>That‚Äôs why this post takes a deliberately ‚Äúfrom scratch‚Äù approach: we build a mathematical formulation step by step (single weight ‚Üí vectors/matrices ‚Üí batches), derive backpropagation in a way that matches an implementation, and then implement a small network accordingly. Along the way we also cover practical details that matter in real code, such as weight initialization and gradient checking, to make sure training works and the derivations actually hold up in practice.</p> <hr/> <h1 id="notation-guide">Notation Guide</h1> <p>The following table summarizes all symbols and notation used throughout this post. Vectors are written in <strong>bold lowercase</strong>, matrices in <strong>bold uppercase</strong>, and scalars in regular font. Unless stated otherwise, vectors are column vectors.</p> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$L$</td> <td>scalar</td> <td>Number of parameterized (trainable) layers in the network</td> </tr> <tr> <td>$\ell$</td> <td>index</td> <td>Layer index, $\ell = 0,1,\ldots,L$</td> </tr> <tr> <td>$\din$</td> <td>scalar</td> <td>Input dimension of the network</td> </tr> <tr> <td>$d_L$</td> <td>scalar</td> <td>Output dimension of the network</td> </tr> <tr> <td>$d_\ell$</td> <td>scalar</td> <td>Number of neurons in layer $\ell$</td> </tr> <tr> <td>$\batch$</td> <td>scalar</td> <td>Batch size (number of training examples processed in parallel)</td> </tr> <tr> <td>$\vec{x}$</td> <td>vector</td> <td>Single input vector $\vec{x} \in \mathbb{R}^{\din}$</td> </tr> <tr> <td>$\vec{x}_n$</td> <td>vector</td> <td>$n$-th input vector in a batch</td> </tr> <tr> <td>$\matr X$</td> <td>matrix</td> <td>Input batch matrix, $\matr X \in \mathbb{R}^{\din \times \batch}$</td> </tr> <tr> <td>$a_i^{(\ell)}$</td> <td>scalar</td> <td>Activation of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec a^{(\ell)}$</td> <td>vector</td> <td>Activation vector of layer $\ell$ for a single example</td> </tr> <tr> <td>$\vec a_n^{(\ell)}$</td> <td>vector</td> <td>Activation vector at layer $\ell$ for the $n$-th example</td> </tr> <tr> <td>$\matr A^{(\ell)}$</td> <td>matrix</td> <td>Activation matrix at layer $\ell$, $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$</td> </tr> <tr> <td>$z_i^{(\ell)}$</td> <td>scalar</td> <td>Pre-activation of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec z^{(\ell)}$</td> <td>vector</td> <td>Pre-activation vector of layer $\ell$ (single example)</td> </tr> <tr> <td>$\matr Z^{(\ell)}$</td> <td>matrix</td> <td>Pre-activation matrix of layer $\ell$ (batch)</td> </tr> <tr> <td>$w_{i,j}^{(\ell)}$</td> <td>scalar</td> <td>Weight connecting neuron $j$ in layer $\ell-1$ to neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec w^{(\ell)}_{i}$</td> <td>vector</td> <td>Incoming weight vector of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\Wmatr^{(\ell)}$</td> <td>matrix</td> <td>Weight matrix of layer $\ell$, $\Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}$</td> </tr> <tr> <td>$b_i^{(\ell)}$</td> <td>scalar</td> <td>Bias of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec b^{(\ell)}$</td> <td>vector</td> <td>Bias vector of layer $\ell$, $\vec b^{(\ell)} \in \mathbb{R}^{d_\ell}$</td> </tr> <tr> <td>$\sigma(\cdot)$</td> <td>function</td> <td>Activation function (applied element-wise)</td> </tr> <tr> <td>$\sigma‚Äô(\cdot)$</td> <td>function</td> <td>Derivative of the activation function</td> </tr> <tr> <td>$\hat{\vec y}$</td> <td>vector</td> <td>Network output for a single input</td> </tr> <tr> <td>$\hat{\vec y}_n$</td> <td>vector</td> <td>Predicted output for the $n$-th training example</td> </tr> <tr> <td>$\matr{\hat{Y}}$</td> <td>matrix</td> <td>Output matrix of the network for a batch</td> </tr> <tr> <td>$\vec y_n^*$</td> <td>vector</td> <td>True target/output for the $n$-th training example</td> </tr> <tr> <td>$\matr Y^*$</td> <td>matrix</td> <td>Target output matrix for a batch</td> </tr> <tr> <td>$\mathcal{L}_n$</td> <td>scalar</td> <td>Loss for the $n$-th training example</td> </tr> <tr> <td>$\mathcal{L}(\vec y^*, \hat{\vec y})$</td> <td>scalar</td> <td>Loss function for a single example</td> </tr> <tr> <td>$J(\Theta)$</td> <td>scalar</td> <td>Cost function (average loss over a batch)</td> </tr> <tr> <td>$\Theta$</td> <td>set</td> <td>Set of all trainable parameters of the network</td> </tr> <tr> <td>$\delta_i^{(\ell)}$</td> <td>scalar</td> <td>Error signal (loss derivative w.r.t. $z_i^{(\ell)}$)</td> </tr> <tr> <td>$\vec{\delta}^{(\ell)}$</td> <td>vector</td> <td>Error vector at layer $\ell$ (single example)</td> </tr> <tr> <td>$\matr\Delta^{(\ell)}$</td> <td>matrix</td> <td>Error matrix at layer $\ell$ for a batch</td> </tr> <tr> <td>$\otimes$</td> <td>operator</td> <td>Hadamard (element-wise) product</td> </tr> <tr> <td>$\vec 1$</td> <td>vector</td> <td>Vector of ones, $\vec 1 \in \mathbb{R}^{\batch}$</td> </tr> <tr> <td>$\eta$</td> <td>scalar</td> <td>Learning rate</td> </tr> <tr> <td>$\frac{\partial J}{\partial \Wmatr^{(\ell)}}$</td> <td>matrix</td> <td>Gradient of the cost w.r.t. weights of layer $\ell$</td> </tr> <tr> <td>$\frac{\partial J}{\partial \vec b^{(\ell)}}$</td> <td>vector</td> <td>Gradient of the cost w.r.t. biases of layer $\ell$</td> </tr> <tr> <td>$(\cdot)^\top$</td> <td>operator</td> <td>Matrix or vector transpose</td> </tr> </tbody> </table> <p><br/> <strong>Conventions</strong></p> <ul> <li>All vectors are column vectors unless explicitly stated otherwise.</li> <li>Batch data is stacked <strong>column-wise</strong>.</li> <li>Activation functions and their derivatives are applied element-wise.</li> <li>Gradients are derived using the denominator-layout convention for matrix calculus.</li> </ul> <hr/> <h1 id="classical-feed-forward-neural-network-architecture">Classical Feed-Forward Neural Network Architecture</h1> <p>Figure 1 illustrates the general structure of a typical fully connected feed-forward neural network. The network consists of $L$ parameterized feed-forward layers with trainable weights, starting with the first hidden layer at $\ell = 1$ and ending with the output layer at $\ell = L$.</p> <p>In total, the network comprises $L + 1$ layers, since the input is treated as an additional layer at $\ell = 0$. This input layer does not contain trainable parameters but serves to provide the input values to the network. The layer at $\ell = L$ is the output layer, while the layers with indices $1 \le \ell &lt; L$ are referred to as hidden layers. The input to the network is given by the vector $\vec{x} = (x_1, x_2, \ldots, x_{\din})^\top \in \mathbb{R}^{\din}$, where $\din$ denotes the input dimension. In the input layer, the components of $\vec{x}$ are identified with the activations $a_i^{(0)} = x_i$. Throughout this work, a superscript $(\ell)$ is used to indicate that a quantity is associated with layer $\ell$.</p> <p>Each layer $\ell$ consists of $d_\ell$ neurons, where $d_0 = \din$ corresponds to the input dimension and $d_L$ denotes the output dimension of the network. In Figure 1, neurons are depicted as circles with incoming arrows. A neuron (also referred to as a node or unit) is the fundamental computational element of a neural network. It receives inputs from the preceding layer, forms a weighted sum of these inputs, adds a bias term, and subsequently applies a nonlinear activation function to produce its output. The output of the $i$-th neuron in layer $\ell$ is typically referred to as the activation $a_i^{(\ell)}$.</p> <p>The neurons of adjacent layers are fully connected, meaning that each neuron in layer $\ell - 1$ is connected to every neuron in layer $\ell$. The weight $w^{(\ell)}_{i,j}$ denotes the trainable parameter connecting the $j$-th neuron of layer $\ell - 1$ to the $i$-th neuron of layer $\ell$. Bias terms are associated with each neuron in layers $\ell \ge 1$. For a given layer $\ell$, the bias term $b_i^{(\ell)}$ corresponds to the $i$-th neuron and is added to the weighted sum of its inputs before the activation function is applied.</p> <p>The activations of the output layer, $a_i^{(L)}$, constitute the network output and are commonly denoted by $\hat{y}_i$, as indicated in Figure 1.</p> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig1-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig1-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig1.png" class="img-fluid rounded z-depth-1 imgcenter" width="95%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 1:</b> Typical structure of a feed-forward neural network. </figcaption> </figure> <h1 id="the-feed-forward-pass">The Feed-Forward Pass</h1> <h2 id="the-activation-of-a-neuron">The Activation of a Neuron</h2> <p>Figure 2 illustrates the internal structure of a neuron, including the weighted summation of its inputs, the addition of a bias term, and the subsequent application of a (nonlinear) activation function.</p> <p>The neuron receives as inputs the activations \(a^{(\ell-1)}_1, a^{(\ell-1)}_2, \ldots, a^{(\ell-1)}_{d_{\ell-1}}\) from all neurons in the preceding layer $\ell - 1$. Each input activation \(a^{(\ell-1)}_j\) is multiplied by a corresponding trainable weight $w^{(\ell)}_{i,j}$, where the index $i$ denotes the neuron in the current layer and $j$ denotes the neuron in the previous layer. These multiplications are indicated by the multiplication symbols ($\times$) in the diagram.</p> <p>The weighted inputs are then summed, as represented by the summation node ($\sum$). In addition, a bias term $b_i^{(\ell)}$, which is specific to the $i$-th neuron in layer $\ell$, is added to this sum. The resulting quantity,</p> \[z_i^{(\ell)} = \sum_{j=1}^{d_{\ell-1}} w^{(\ell)}_{i,j} \, a^{(\ell-1)}_j + b_i^{(\ell)},\] <p>is referred to as the pre-activation of the neuron.</p> <p>Finally, the pre-activation $z_i^{(\ell)}$ is passed through a nonlinear activation function $\sigma(\cdot)$, producing the output (or activation)</p> \[\begin{equation} a_i^{(\ell)} = \sigma\!\left(z_i^{(\ell)}\right) = \sigma \Bigg( \sum_{j=1}^{d_{\ell-1}} {w_{i,j}^{(\ell)} \cdot a_j^{(\ell-1)} } + b_i^{(\ell)} \Bigg) \label{eq:activation} \end{equation}\] <p>This activation constitutes the output of the neuron and serves as an input to the neurons in the subsequent layer $\ell + 1$.</p> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig2-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig2-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig2.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 2:</b> Representation of a typical neuron in a neural net. </figcaption> </figure> <h2 id="the-activation-of-a-layer-of-neurons">The Activation of a Layer of Neurons</h2> <p>The figure thus highlights the three fundamental steps performed by a neuron during the forward pass: weighted summation of inputs, addition of a bias term, and application of a nonlinear activation function.</p> <p>In matrix form, Eq. \eqref{eq:activation} can be written as</p> \[\begin{equation} \vec a^{(\ell)} = \sigma\left(\vec z^{(\ell)}\right) = \sigma\left( \Wmatr^{(\ell)} \vec a^{(\ell-1)} + \vec b^{(\ell)} \right), \label{eq:activation3} \end{equation}\] <p>where all vectors are interpreted as column vectors:</p> \[\begin{align} \vec a^{(\ell)}, \vec z^{(\ell)}, \vec b^{(\ell)} &amp;\in \mathbb{R}^{d_\ell}, \\ \Wmatr^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times d_{\ell-1}}, \\ \vec a^{(\ell-1)} &amp;\in \mathbb{R}^{d_{\ell-1}}. \label{eq:activation4} \end{align}\] <p>The activation function $\sigma(\cdot)$ is applied element-wise to its vector argument.</p> <p>It is also possible to pass multiple input vectors through the network simultaneously by stacking them into a matrix. Let</p> \[\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_\batch\] <p>denote $\batch$ input examples, where $\batch$ is referred to as the batch size. These inputs are arranged column-wise into the input matrix</p> \[\begin{align} \matr X &amp;= \big( \vec{x}_1 \ \vec{x}_2 \ \ldots \ \vec{x}_\batch \big), \\ \matr X &amp;\in \mathbb{R}^{\din \times \batch}. \label{eq:input-matrix} \end{align}\] <p>This procedure is commonly referred to as batching. A matrix containing multiple input vectors is called a batch.</p> <blockquote> <p>In some implementations, input vectors are stacked row-wise, with each row representing one example. Here, we adopt the column-wise convention, which is consistent with the preceding vector notation and the matrix formulations used throughout.</p> </blockquote> <p>For each input example $\vec{x}_n$, a corresponding activation vector $\vec{a}_n^{(\ell)}$ is produced at layer $\ell$. These activations can be collected into the activation matrix</p> \[\matr A^{(\ell)} = \big( \vec a_1^{(\ell)} \ \vec a_2^{(\ell)} \ \ldots \ \vec a_\batch^{(\ell)} \big) \in \mathbb{R}^{d_\ell \times \batch}.\] <p>Using matrix notation, the forward pass for a batch of inputs can be written as</p> \[\begin{align} \matr Z^{(\ell)} &amp;= \begin{pmatrix} \vec b^{(\ell)} &amp; \Wmatr^{(\ell)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr A^{(\ell-1)} \end{pmatrix}, \\ &amp;= \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top \\ \matr A^{(\ell)} &amp;= \sigma\!\left( \matr Z^{(\ell)} \right), \label{eq:input-matrix1} \end{align}\] <p>where $\vec{1} \in \mathbb{R}^{\batch}$ denotes a vector of ones.</p> <p>Note that the inclusion of the bias vector into the weight matrix is a purely notational convenience. From a mathematical perspective, the bias term can equivalently be added as a separate vector after the weighted sum has been computed. By augmenting the input with a constant component equal to one and concatenating the bias vector to the weight matrix, the affine transformation can be written as a single matrix multiplication. Throughout this post, this augmented representation is used where it simplifies the notation, but it does not constitute an additional modeling assumption.</p> <hr/> <h3 id="unpacking-the-batch-forward-pass-column-wise-view">Unpacking the Batch Forward Pass (Column-wise View)</h3> <p>To see explicitly how the augmented matrix multiplication works, we expand Eq. \eqref{eq:input-matrix1} using block-matrix multiplication:</p> \[\matr A^{(\ell-1)} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec a^{(\ell-1)}_{1} &amp; \vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} \in \mathbb{R}^{d_{\ell-1}\times \batch}.\] <p>Then the matrix multiplication expands as</p> \[\Wmatr^{(\ell)}\matr A^{(\ell-1)} = \Wmatr^{(\ell)} \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec a^{(\ell-1)}_{1} &amp; \vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>To add the bias to every column, we replicate it across the batch:</p> \[\vec b^{(\ell)}\vec 1^\top = \vec b^{(\ell)} \begin{pmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \end{pmatrix} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec b^{(\ell)} &amp; \vec b^{(\ell)} &amp; \cdots &amp; \vec b^{(\ell)} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>Hence the pre-activation matrix becomes</p> \[\matr Z^{(\ell)} = \Wmatr^{(\ell)}\matr A^{(\ell-1)} + \vec b^{(\ell)}\vec 1^\top = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} + \vec b^{(\ell)} &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} + \vec b^{(\ell)} &amp; \cdots &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} + \vec b^{(\ell)} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>Applying $\sigma(\cdot)$ element-wise yields the activation matrix</p> \[\begin{align*} \matr A^{(\ell)} = \sigma\!\left(\matr Z^{(\ell)}\right) &amp;= \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} + \vec b^{(\ell)}\big) &amp; \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} + \vec b^{(\ell)}\big) &amp; \cdots &amp; \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} + \vec b^{(\ell)}\big) \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} \\[12pt] &amp;= \big( \vec a_1^{(\ell)} \ \vec a_2^{(\ell)} \ \ldots \ \vec a_\batch^{(\ell)} \big) \end{align*}\] <p>So each column is exactly the single-example relation we already wrote:</p> \[\vec a^{(\ell)}_{n} = \sigma\!\left(\vec z^{(\ell)}_{n}\right) = \sigma\!\left(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{n} + \vec b^{(\ell)}\right), \qquad n=1,\ldots,\batch.\] <hr/> <h3 id="computing-the-activation-of-the-first-hidden-layer">Computing the Activation of the first Hidden Layer</h3> <p>Using the augmented representation and $\matr A^{(0)}= \matr X$, the affine transformation of the first hidden layer can be written as</p> \[\begin{equation} \matr A^{(1)} = \sigma\!\left( \begin{pmatrix} \vec b^{(1)} &amp; \Wmatr^{(1)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr X \end{pmatrix} \right). \label{eq:input-matrix2} \end{equation}\] <p>Here, the augmented weight matrix</p> \[\begin{pmatrix} \vec b^{(1)} &amp; \Wmatr^{(1)} \end{pmatrix} \in \mathbb{R}^{d_1 \times (\din + 1)}\] <p>is formed by concatenating the bias vector $\vec b^{(1)} \in \mathbb{R}^{d_1}$ and the weight matrix $\Wmatr^{(1)} \in \mathbb{R}^{d_1 \times \din}$, while the augmented input matrix</p> <p>\(\begin{pmatrix} \vec{1}^\top \\ \matr X \end{pmatrix} \in \mathbb{R}^{(\din + 1) \times \batch}\) contains a row of ones and the input batch $\matr X \in \mathbb{R}^{\din \times \batch}$. As a result, the matrix product is well defined and yields</p> \[\matr A^{(1)} \in \mathbb{R}^{d_1 \times \batch}.\] <hr/> <h2 id="summary-forward-pass-through-the-network">Summary: Forward Pass Through the Network</h2> <p>We summarize the steps required to compute a forward pass for a batch of inputs through a fully connected feed-forward neural network. This provides a compact, algorithmic view of the computations introduced so far and serves as a reference for later gradient derivations.</p> <h3 id="inputs-and-parameters">Inputs and Parameters</h3> <ul> <li><strong>Network architecture</strong> <ul> <li>Number of layers: $L$</li> <li>Layer sizes: $d_0=\din, d_1, \ldots, d_L$</li> </ul> </li> <li><strong>Trainable parameters (for each layer $\ell = 1,\ldots,L$)</strong> <ul> <li>Weight matrix: $ \Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}} $</li> <li>Bias vector: $ \vec b^{(\ell)} \in \mathbb{R}^{d_\ell} $</li> </ul> </li> <li><strong>Activation function</strong> <ul> <li>Nonlinear function $\sigma(\cdot)$, applied element-wise</li> </ul> </li> <li><strong>Batch input</strong> <ul> <li>$\batch$ input vectors stacked column-wise: \(\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}\)</li> </ul> </li> </ul> <hr/> <h3 id="forward-pass">Forward Pass</h3> <ol> <li> <p><strong>Initialization</strong> \(\matr A^{(0)} := \matr X\)</p> </li> <li><strong>Layer-wise propagation</strong><br/> For each layer $\ell = 1,2,\ldots,L$: <ul> <li> <p>Pre-activations: \(\matr Z^{(\ell)} = \begin{pmatrix} \vec b^{(\ell)} &amp; \Wmatr^{(\ell)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr A^{(\ell-1)} \end{pmatrix}, \qquad \matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch},\)</p> <p>where $\vec 1 \in \mathbb{R}^{\batch}$ denotes a vector of ones.</p> <p>Equivalently: $\matr Z^{(\ell)} = \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top$.</p> </li> <li> <p>Activations: \(\matr A^{(\ell)} = \sigma\!\left(\matr Z^{(\ell)}\right), \qquad \matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}\)</p> </li> </ul> </li> <li> <p><strong>Network output</strong> \(\matr A^{(L)} = \big( \hat{\vec y}_1 \ \hat{\vec y}_2 \ \ldots \ \hat{\vec y}_\batch \big) ,\qquad \matr A^{(L)} \in \mathbb{R}^{d_L \times \batch},\)</p> <p>where $\hat{\vec y}_n \in \mathbb{R}^{d_L}$ denotes the output of the network for the $n$-th input example.</p> </li> </ol> <p>Each column of $\matr A^{(\ell)}$ corresponds to the activations produced by one training example, and the same weight matrices and bias vectors are shared across all columns.</p> <hr/> <h3 id="remarks">Remarks</h3> <ul> <li>The batch forward pass is mathematically equivalent to performing $\batch$ independent forward passes in parallel.</li> <li>Writing the computation in matrix form enables efficient implementations using linear algebra routines.</li> <li>All intermediate matrices \(\{\matr A^{(\ell)}, \matr Z^{(\ell)}\}_{\ell=1}^L\) must be retained, as they are required later during backpropagation.</li> </ul> <p>This completes the forward-pass specification for batched inputs.</p> <hr/> <h1 id="the-backpropagation-algorithm">The Backpropagation Algorithm</h1> <h2 id="cost-function-and-gradient-descent">Cost Function and Gradient Descent</h2> <p>To train the neural network, a suitable differentiable cost (sometimes objective) function $J(\Theta)$ is required, where $\Theta$ denotes the set of all trainable parameters of the network, including both the weight matrices and the bias vectors of all layers. Explicitly, we write</p> \[\Theta = \left\{ \Wmatr^{(1)}, \vec b^{(1)}, \Wmatr^{(2)}, \vec b^{(2)}, \ldots, \Wmatr^{(L)}, \vec b^{(L)} \right\}.\] <p>We denote the loss for a single ($n$-th) training example by $\mathcal{L}(\vec y_n^*, \hat{\vec y_n})$. A commonly used choice is the mean squared error (MSE) loss, which measures the prediction error for a single training example. For an individual example $n$, the loss is defined as</p> \[\mathcal{L}_n = \frac{1}{2}\left\| \vec{y}_n^* - \hat{\vec y}_n \right\|^2,\] <p>where $\vec y_n^* \in \mathbb{R}^{d_L}$ denotes the true output vector of the $n$-th example and $\hat{\vec y}_n = \vec a_n^{(L)} \in \mathbb{R}^{d_L}$ is the corresponding predicted output of the network.</p> <blockquote> <p><strong>Note.</strong><br/> The factor $\tfrac{1}{2}$ is included purely for convenience. When differentiating the squared error, it cancels the factor $2$ arising from the derivative of the square and thus simplifies the resulting gradient expressions. Importantly, this factor does not affect the location of the minimum of the cost function.</p> </blockquote> <p>The cost function $J(\Theta)$ is obtained by aggregating the loss over a batch of $\batch$ training examples. Using the mean squared error, the cost function is given by</p> \[J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n = \frac{1}{\batch} \sum_{n=1}^{\batch} \frac{1}{2}\left\| \vec{y}_n^* - \hat{\vec y}_n \right\|^2. \label{eq:mse}\] <p>In Figure 3, the local structure of two consecutive layers in a feed-forward neural network is illustrated in a form that is particularly useful for deriving the backpropagation algorithm. The figure highlights a single neuron $i$ in layer $\ell$, its pre-activation $z_i^{(\ell)}$, and its activation</p> \[a_i^{(\ell)} = \sigma\left(z_i^{(\ell)}\right),\] <p>as well as how this activation is propagated forward to all neurons in the subsequent layer $\ell + 1$.</p> <p>Specifically, the activation $a_i^{(\ell)}$ serves as an input to each neuron in layer $\ell + 1$, where it is multiplied by the corresponding weights $w_{k,i}^{(\ell+1)}$, summed together with the other incoming weighted activations, and combined with the bias terms $b_k^{(\ell+1)}$ to form the pre-activations $z_k^{(\ell+1)}$. This explicit depiction makes clear how a single activation influences multiple downstream neurons and, conversely (as we will see below), how gradients computed at layer $\ell + 1$ will later be propagated backward through the same weighted connections to layer $\ell$.</p> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig3-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig3-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig3.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 3:</b> Zoom into the neural network. </figcaption> </figure> <p>Gradient descent can be used to minimize the cost function $J(\Theta)$. This requires computing the partial derivatives of $J$ with respect to all trainable parameters contained in $\Theta$. Due to the layered structure of a neural network, these derivatives cannot be computed directly in a single step. Figure 3 illustrates this dependency structure for a specific weight $w_{i,j}^{(\ell)}$.</p> <p>At this point, it is convenient to distinguish between the <strong>loss</strong> $\mathcal{L}(\vec y^*, \hat{\vec y})$ of a single training example and the <strong>cost function</strong> $J(\Theta)$, which aggregates the loss over a batch of $\batch$ examples. The derivation of backpropagation is carried out at the single-sample level, since differentiation is a linear operation. In particular, if</p> \[J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n(\Theta),\] <p>then the gradient of the cost function is given by</p> \[\frac{\partial J}{\partial \Theta} = \frac{1}{\batch} \sum_{n=1}^{\batch} \frac{\partial \mathcal{L}_n}{\partial \Theta}.\] <p>Thus, the gradient of $J$ is simply the average of the gradients of the individual losses. For this reason, we first derive the gradients of the loss for a single training example and later extend the result to a batch by averaging.</p> <p>Assume that we want to compute the partial derivative of the loss function with respect to the weight $w_{i,j}^{(\ell)}$. Since this weight influences the loss only indirectly through the pre-activation $z_i^{(\ell)}$ of neuron $i$ in layer $\ell$, the chain rule yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \frac{\partial z_i^{(\ell)}}{\partial w_{i,j}^{(\ell)}}. \label{eq:partial1} \end{align}\] <p>From the definition of the pre-activation</p> \[z_i^{(\ell)} = \sum_{j} w_{i,j}^{(\ell)} a_j^{(\ell-1)} + b_i^{(\ell)},\] <p>it follows immediately that</p> \[\begin{align} \frac{\partial z_i^{(\ell)}}{\partial w_{i,j}^{(\ell)}} = a_j^{(\ell-1)}. \label{eq:partial2} \end{align}\] <p>Substituting this result into Eq. \eqref{eq:partial1} yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} = \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}. \end{align}\] <p>So far, we have considered the partial derivative of the loss with respect to the weights $w_{i,j}^{(\ell)}$. An analogous and even simpler expression is obtained for the bias parameters $b_i^{(\ell)}$.</p> <p>Since the bias enters the pre-activation additively, we have</p> \[\frac{\partial z_i^{(\ell)}}{\partial b_i^{(\ell)}} = 1.\] <p>Therefore, by the chain rule,</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \frac{\partial z_i^{(\ell)}}{\partial b_i^{(\ell)}} \\ &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \end{align}\] <p>In summary, we have:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}, \label{eq:partial3} \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \label{eq:partial3_b} \end{align}\] <p>The remaining partial derivative $\partial \mathcal{L} / \partial z_i^{(\ell)}$ cannot be evaluated directly. However, it can be expressed recursively by applying the chain rule, which forms the basis of the backpropagation algorithm.</p> <hr/> <h2 id="interlude-the-chain-rule-and-nested-dependencies">Interlude: The Chain Rule and Nested Dependencies</h2> <p>Before deriving the backpropagation algorithm, it is helpful to briefly recall the chain rule for derivatives, which plays a central role in computing gradients in neural networks. In particular, neural networks are composed of nested functions, so changes in a parameter typically affect the output only indirectly through a sequence of intermediate variables.</p> <p>Consider a function $z = f(x, y)$ that depends on two intermediate variables $x$ and $y$, which in turn both depend on a common variable $t$, i.e.</p> \[x = g(t), \qquad y = h(t).\] <p>In this case, the total derivative of $z$ with respect to $t$ is given by the chain rule as</p> \[\begin{align} z &amp;= f(x, y), \quad x = g(t), \quad y = h(t), \\ \frac{d z}{d t} &amp;= \frac{\partial z}{\partial x} \frac{d x}{d t} + \frac{\partial z}{\partial y} \frac{d y}{d t}. \label{eq:chainrule1} \end{align}\] <p>This expression shows that the influence of $t$ on $z$ is obtained by summing all paths through which $t$ affects the intermediate variables $x$ and $y$.</p> <p>Consider the composite function</p> \[z = \sin(x^2 + y^2), \qquad x = e^t, \qquad y = t^2.\] <p>Since $z$ depends on $t$ only through the intermediate variables $x$ and $y$, the chain rule gives</p> \[\begin{align} \frac{\partial z}{\partial x} &amp;= 2x \cos(x^2 + y^2), \\ \frac{\partial z}{\partial y} &amp;= 2y \cos(x^2 + y^2), \\ \frac{d x}{d t} &amp;= e^t, \\ \frac{d y}{d t} &amp;= 2t. \end{align}\] <p>Therefore, the total derivative of $z$ with respect to $t$ is</p> \[\begin{align} \frac{d z}{d t} &amp;= 2x \cos(x^2 + y^2)\,\frac{d x}{d t} + 2y \cos(x^2 + y^2)\,\frac{d y}{d t} \\ &amp;= 2x \cos(x^2 + y^2)\,e^t + 2y \cos(x^2 + y^2)\,2t \\ &amp;= 2\cos(x^2 + y^2)\left(x\,e^t + 2yt\right) \\ &amp;= 2\cos\left(e^{2t} + t^4\right)\left(e^{2t} + 2t^3\right). \end{align}\] <p>Using direct differentiation, we obtain</p> \[\begin{align} z &amp;= \sin(e^{2t} + t^4), \\ \frac{d z}{d t} &amp;= \cos(e^{2t} + t^4)\left(2 e^{2t} + 4 t^3\right) \\ &amp;= 2\cos(e^{2t} + t^4)\left(e^{2t} + 2 t^3\right), \end{align}\] <p>which is identical to the result obtained via the chain rule.</p> <p>Similarly, an analogous form of the chain rule applies when the intermediate variables are vector-valued. Let $\vec t$ be a vector and let $z = f(\vec x)$ with $\vec x = g(\vec t)$. Then the partial derivative of $z$ with respect to the $i$-th component of $\vec t$ is given by</p> \[\begin{align} z &amp;= f(\vec{x}), \qquad \vec{x} = g(\vec{t}), \\ \frac{\partial z}{\partial t_i} &amp;= \frac{\partial z}{\partial x_1} \frac{\partial x_1}{\partial t_i} + \frac{\partial z}{\partial x_2} \frac{\partial x_2}{\partial t_i} + \cdots \\ &amp;= \sum_{j} \frac{\partial z}{\partial x_j} \frac{\partial x_j}{\partial t_i}. \label{eq:chainrule2} \end{align}\] <p>This expression shows that the influence of a single component $t_i$ on $z$ is obtained by summing over all paths through which $t_i$ affects the intermediate variables $\vec x$.</p> <hr/> <h2 id="backpropagation-via-the-chain-rule">Backpropagation via the Chain Rule</h2> <p>We can now use the chain rule to compute the partial derivative $\partial \mathcal{L} / \partial z_i^{(\ell)}$ in Eq. \eqref{eq:partial3} &amp; Eq. \eqref{eq:partial3_b} in a recursive manner. As illustrated in Figure 3, the pre-activations $z_k^{(\ell+1)}$ of the next layer can be regarded as functions of the pre-activations of the current layer, in particular of $z_i^{(\ell)}$, i.e., $z_k^{(\ell+1)} = z_k^{(\ell+1)}(\ldots, z_i^{(\ell)}, \ldots)$. Consequently, the loss function depends on $z_i^{(\ell)}$ only indirectly through its influence on all downstream pre-activations $z_k^{(\ell+1)}$. Formally, one may therefore view the loss function as a composite function of the form $\mathcal{L} = \mathcal{L}\big(\ldots, z_k^{(\ell+1)}(\ldots, z_i^{(\ell)}, \ldots), \ldots\big)$, which naturally leads to a recursive application of the chain rule.</p> <p>Applying the chain rule, we obtain</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_1^{(\ell+1)}} \frac{\partial z_1^{(\ell+1)}}{\partial z_i^{(\ell)}} + \frac{\partial \mathcal{L}}{\partial z_2^{(\ell+1)}} \frac{\partial z_2^{(\ell+1)}}{\partial z_i^{(\ell)}} + \cdots \\ &amp;= \sum_{k=1}^{d_{\ell+1}} \frac{\partial \mathcal{L}}{\partial z_k^{(\ell+1)}} \frac{\partial z_k^{(\ell+1)}}{\partial z_i^{(\ell)}}. \label{eq:chainrule3} \end{align}\] <p>From the definition</p> \[z_k^{(\ell+1)} = \sum_j w^{(\ell+1)}_{k,j} a_j^{(\ell)} + b_k^{(\ell+1)}, \qquad a_j^{(\ell)} = \sigma\!\left(z_j^{(\ell)}\right),\] <p>it follows that</p> \[\frac{\partial z_k^{(\ell+1)}}{\partial z_i^{(\ell)}} = w^{(\ell+1)}_{k,i} \, \sigma'\!\left(z_i^{(\ell)}\right),\] <p>where $\sigma‚Äô(z)$ denotes the derivative of the activation function $\sigma(\cdot)$ with respect to its argument.</p> <p>Substituting this result into Eq. \eqref{eq:chainrule3} yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} = \sigma'\!\left(z_i^{(\ell)}\right) \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i} \frac{\partial \mathcal{L}}{\partial z_k^{(\ell+1)}}. \label{eq:backprop-recursion} \end{align}\] <p>Note the recursive structure of Eq. \eqref{eq:backprop-recursion}. For notational convenience, we define the error signal (also called the delta term), i.e. the gradient of the loss with respect to the pre-activation, at neuron $i$ in layer $\ell$ as</p> \[\delta_i^{(\ell)} := \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}.\] <p>With this definition, Eq. \eqref{eq:backprop-recursion} can be written in the recursive form</p> \[\begin{align} \delta_i^{(\ell)} &amp;= \sigma'\!\left(z_i^{(\ell)}\right) \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i}\, \delta_k^{(\ell+1)}. \label{eq:chainrule4} \end{align}\] <p>Equation \eqref{eq:chainrule4} can be written in a more compact vector form. To this end, we introduce the error vectors</p> \[\begin{align} \vec{\delta}^{(\ell)} &amp;\in \mathbb{R}^{d_\ell }, \\ \vec{\delta}^{(\ell+1)} &amp;\in \mathbb{R}^{d_{\ell+1} }, \end{align}\] <p>and the weight matrix</p> \[\begin{align} \Wmatr^{(\ell+1)} \in \mathbb{R}^{d_{\ell+1} \times d_\ell}. \label{eq:chainrule6} \end{align}\] <p>In Eq.\eqref{eq:chainrule4}, the summation is carried out over the index $k$, which corresponds to the first index of the weight $w_{k,i}^{(\ell+1)}$. This implies that, in matrix notation, the transpose of $\Wmatr^{(\ell+1)}$ must be used. Indeed, we have</p> \[\begin{align} (\Wmatr^{(\ell+1)})^\top &amp;\in \mathbb{R}^{d_\ell \times d_{\ell+1}}, \\ (\Wmatr^{(\ell+1)})^\top \vec{\delta}^{(\ell+1)} &amp;\in \mathbb{R}^{d_\ell }. \label{eq:chainrule7} \end{align}\] <p>To see how the transpose arises explicitly, write the weight matrix row-wise as</p> \[\Wmatr^{(\ell+1)} = \begin{pmatrix} (\vec w^{(\ell+1)}_{1})^\top \\ (\vec w^{(\ell+1)}_{2})^\top \\ \vdots \\ (\vec w^{(\ell+1)}_{d_{\ell+1}})^\top \end{pmatrix}, \qquad \vec w^{(\ell+1)}_{k} \in \mathbb{R}^{d_\ell},\] <p>i.e., the $k$-th row contains the incoming weights of neuron $k$ in layer $\ell+1$. Taking the transpose yields</p> \[(\Wmatr^{(\ell+1)})^\top = \begin{pmatrix} \vec w^{(\ell+1)}_{1} &amp; \vec w^{(\ell+1)}_{2} &amp; \cdots &amp; \vec w^{(\ell+1)}_{d_{\ell+1}} \end{pmatrix}.\] <p>Now let</p> \[\vec\delta^{(\ell+1)} = \begin{pmatrix} \delta^{(\ell+1)}_{1}\\ \delta^{(\ell+1)}_{2}\\ \vdots\\ \delta^{(\ell+1)}_{d_{\ell+1}} \end{pmatrix}.\] <p>Then the matrix-vector product can be interpreted as a weighted sum of the (transposed) row vectors:</p> \[(\Wmatr^{(\ell+1)})^\top \vec\delta^{(\ell+1)} = \sum_{k=1}^{d_{\ell+1}} \vec w^{(\ell+1)}_{k}\,\delta^{(\ell+1)}_{k}.\] <p>Looking at the $i$-th component of this vector gives</p> \[\big[(\Wmatr^{(\ell+1)})^\top \vec\delta^{(\ell+1)}\big]_i = \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i}\,\delta^{(\ell+1)}_{k},\] <p>which is exactly the summation term appearing in Eq. \eqref{eq:chainrule4}.</p> <p>Using this notation (following the <a href="https://en.wikipedia.org/wiki/Matrix_calculus">denominator-layout convention</a>) and writing the derivative of the activation function element-wise, the backpropagation recursion can be expressed as</p> \[\begin{align} \vec{\delta}^{(\ell)} := \frac{\partial \mathcal{L}}{\partial \vec z^{(\ell)}} = \Big( (\Wmatr^{(\ell+1)})^\top \vec{\delta}^{(\ell+1)} \Big) \otimes \sigma'\!\left(\vec z^{(\ell)}\right), \label{eq:chainrule8} \end{align}\] <p>where $\otimes$ denotes the Hadamard (element-wise) product and the derivative $\sigma‚Äô(\cdot)$ is applied component-wise to the vector $\vec z^{(\ell)}$.</p> <p>Recall Eqs. \eqref{eq:partial3} and \eqref{eq:partial3_b}, which give the gradients of the loss with respect to the weights and biases of layer $\ell$:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}, \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \end{align}\] <p>By comparison with Eq. \eqref{eq:chainrule8}, we immediately obtain</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \delta_i^{(\ell)} \, a_j^{(\ell-1)}, \label{eq:finalbackprop} \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \delta_i^{(\ell)} \label{eq:finalbackprop_b} \end{align}\] <p>Thus, once the error signal $\vec{\delta}^{(\ell)}$ has been computed for a given layer, the gradients with respect to both the weights and the biases follow directly. These observations naturally lead to compact matrix expressions for the gradients with respect to the weight matrices and bias vectors, which we derive next.</p> <p>From the component-wise relations</p> \[\frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} = \delta_i^{(\ell)}\, a_j^{(\ell-1)}, \qquad \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} = \delta_i^{(\ell)},\] <p>we can derive compact matrix-vector expressions for the gradients with respect to the weight matrix $\Wmatr^{(\ell)}$ and the bias vector $\vec b^{(\ell)}$.</p> <hr/> <h3 id="gradient-with-respect-to-the-weights-vectorized-form">Gradient with respect to the weights (vectorized form)</h3> <p>Collecting all partial derivatives $\partial \mathcal{L}/\partial w_{i,j}^{(\ell)}$ into a matrix yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial \Wmatr^{(\ell)}} &amp;= \vec{\delta}^{(\ell)} \, \big(\vec a^{(\ell-1)}\big)^\top. \label{eq:finalbackprop2} \end{align}\] <p>The transpose arises because $\vec{\delta}^{(\ell)}$ and $\vec a^{(\ell-1)}$ are column vectors. With the (column-vector) convention used throughout,</p> \[\vec{\delta}^{(\ell)} \in \mathbb{R}^{d_\ell \times 1}, \qquad \vec a^{(\ell-1)} \in \mathbb{R}^{d_{\ell-1} \times 1}.\] <p>To obtain a matrix in $\mathbb{R}^{d_\ell \times d_{\ell-1}}$, we must form an outer product: the first factor stays a column vector, while the second is transposed into a row vector:</p> \[\big(\vec a^{(\ell-1)}\big)^\top \in \mathbb{R}^{1 \times d_{\ell-1}}.\] <p>Thus the dimensions match as</p> \[\begin{align} \vec{\delta}^{(\ell)} \, \big(\vec a^{(\ell-1)}\big)^\top &amp;\in \mathbb{R}^{d_\ell \times 1}\;\cdot\;\mathbb{R}^{1 \times d_{\ell-1}} = \mathbb{R}^{d_\ell \times d_{\ell-1}}. \end{align}\] <p>Equivalently, the $(i,j)$-th entry of this outer product is</p> \[\left[\vec{\delta}^{(\ell)} \, (\vec a^{(\ell-1)})^\top\right]_{i,j} = \delta_i^{(\ell)}\, a_j^{(\ell-1)},\] <p>which matches the component-wise gradient $\partial \mathcal{L}/\partial w_{i,j}^{(\ell)}$.</p> <hr/> <h3 id="gradient-with-respect-to-the-biases-vectorized-form">Gradient with respect to the biases (vectorized form)</h3> <p>The bias gradient collects the partial derivatives $\partial \mathcal{L}/\partial b_i^{(\ell)}$ into a vector:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial \vec b^{(\ell)}} &amp;= \vec{\delta}^{(\ell)} \in \mathbb{R}^{d_\ell \times 1}. \label{eq:finalbackprop2_b} \end{align}\] <p>Hence, once the error vector $\vec{\delta}^{(\ell)}$ has been computed via the backpropagation recursion \eqref{eq:chainrule8}, both gradients $\partial \mathcal{L}/\partial \Wmatr^{(\ell)}$ and $\partial \mathcal{L}/\partial \vec b^{(\ell)}$ follow immediately.</p> <p>As before, we can propagate a batch of training examples through the network by collecting them column-wise into the input matrix</p> \[\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}.\] <p>During the forward pass, each layer produces an activation matrix $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$, where the $n$-th column corresponds to the activations generated by the $n$-th training example. Proceeding layer by layer, we eventually obtain the output matrix $\matr A^{(L)}$.</p> <p>In the derivation above, the gradients were computed for a single training example. To perform a batch update of the parameters, these gradients must be aggregated over all $\batch$ examples in the batch. Since differentiation is linear, this aggregation is achieved by summing (or averaging) the per-example gradients.</p> <p>To this end, we collect the error vectors $\vec{\delta}^{(\ell)}$ of all training examples at layer $\ell$ into the error matrix</p> \[\matr\Delta^{(\ell)} = \big( \vec{\delta}_1^{(\ell)} \; \vec{\delta}_2^{(\ell)} \; \ldots \; \vec{\delta}_\batch^{(\ell)} \big) \in \mathbb{R}^{d_\ell \times \batch}.\] <p>Analogously, we denote by \(\matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}\) the matrix of pre-activations at layer $\ell$.</p> <hr/> <h2 id="batch-backpropagation-recursion">Batch Backpropagation Recursion</h2> <p>Applying the single-sample recursion \eqref{eq:chainrule8} column-wise to all examples yields the batch version of backpropagation:</p> \[\begin{align} \matr\Delta^{(\ell)} &amp;= \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \;\otimes\; \sigma'\!\left(\matr Z^{(\ell)}\right), \label{eq:matrixBackprop} \end{align}\] <p>where the derivative $\sigma‚Äô(\cdot)$ is applied element-wise and $\otimes$ denotes the Hadamard product.</p> <p>The dimensions involved are</p> \[\begin{align} (\Wmatr^{(\ell+1)})^\top &amp;\in \mathbb{R}^{d_\ell \times d_{\ell+1}}, \\ \matr\Delta^{(\ell+1)} &amp;\in \mathbb{R}^{d_{\ell+1} \times \batch}, \\ \matr Z^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times \batch}, \end{align}\] <p>so that</p> \[(\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \in \mathbb{R}^{d_\ell \times \batch},\] <p>and the element-wise multiplication with $\sigma‚Äô(\matr Z^{(\ell)})$ is well defined.</p> <hr/> <h3 id="batch-gradients-for-weights-and-biases">Batch gradients for weights and biases</h3> <p>Using the batch error matrix $\matr\Delta^{(\ell)}$, the gradients of the loss with respect to the parameters of layer $\ell$ can be written compactly as</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \label{eq:J_W_ell} \\[6pt] \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \label{eq:J_b_ell} \end{align}\] <p>with the following dimensions:</p> \[\begin{align} \matr\Delta^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times \batch}, \\ \matr A^{(\ell-1)} &amp;\in \mathbb{R}^{d_{\ell-1} \times \batch}, \\ \big(\matr A^{(\ell-1)}\big)^\top &amp;\in \mathbb{R}^{\batch \times d_{\ell-1}}, \\ \vec 1 &amp;\in \mathbb{R}^{\batch}, \\ \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;\in \mathbb{R}^{d_\ell \times d_{\ell-1}}, \\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;\in \mathbb{R}^{d_\ell}. \end{align}\] <p>Here, the weight gradient has the same shape as the weight matrix $\Wmatr^{(\ell)}$, and the bias gradient has the same shape as the bias vector $\vec b^{(\ell)}$, as required for gradient-based optimization.</p> <p>The first expression Eq. \eqref{eq:J_W_ell} corresponds to summing the outer products $\vec{\delta}_n^{(\ell)} (\vec a_n^{(\ell-1)})^\top$ over all training examples, while the second expression in Eq. \eqref{eq:J_b_ell} reflects the fact that each bias gradient is obtained by summing the corresponding error signals across the batch.</p> <p>These batch-wise expressions allow all gradients to be computed efficiently using matrix operations, which is crucial for practical implementations of backpropagation.</p> <p>To make the batch expressions more explicit, we write out the matrices $\matr\Delta^{(\ell)}$ and $\matr A^{(\ell-1)}$ in terms of their column vectors.</p> <p>The activation matrix of layer $\ell-1$ is given by</p> \[\matr A^{(\ell-1)} = \begin{pmatrix} \vec a^{(\ell-1)}_1 &amp; \vec a^{(\ell-1)}_2 &amp; \cdots &amp; \vec a^{(\ell-1)}_\batch \end{pmatrix} \in \mathbb{R}^{d_{\ell-1} \times \batch},\] <p>where</p> \[\vec a^{(\ell-1)}_n \in \mathbb{R}^{d_{\ell-1}}\] <p>denotes the activation vector produced by the $n$-th training example at layer $\ell-1$.</p> <p>Similarly, the error matrix at layer $\ell$ is</p> \[\matr\Delta^{(\ell)} = \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \in \mathbb{R}^{d_\ell \times \batch},\] <p>where each column</p> \[\vec\delta^{(\ell)}_n \in \mathbb{R}^{d_\ell}\] <p>is the error signal corresponding to the $n$-th training example.</p> <p>Taking the transpose of the activation matrix yields</p> \[\big(\matr A^{(\ell-1)}\big)^\top = \begin{pmatrix} (\vec a^{(\ell-1)}_1)^\top \\ (\vec a^{(\ell-1)}_2)^\top \\ \vdots \\ (\vec a^{(\ell-1)}_\batch)^\top \end{pmatrix} \in \mathbb{R}^{\batch \times d_{\ell-1}}.\] <hr/> <h3 id="weight-gradient-as-a-sum-of-outer-products">Weight gradient as a sum of outer products</h3> <p>Using these explicit forms, the batch gradient with respect to the weight matrix can be expanded as</p> \[\begin{align} \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top &amp;= \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \begin{pmatrix} (\vec a^{(\ell-1)}_1)^\top \\ (\vec a^{(\ell-1)}_2)^\top \\ \vdots \\ (\vec a^{(\ell-1)}_\batch)^\top \end{pmatrix} \\[6pt] &amp;= \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n \, (\vec a^{(\ell-1)}_n)^\top. \end{align}\] <p>Thus, the gradient of the cost function with respect to the weights is</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top \\ &amp;= \frac{1}{\batch} \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n \, (\vec a^{(\ell-1)}_n)^\top, \end{align}\] <p>which is exactly the average of the single-sample gradients derived earlier.</p> <hr/> <h3 id="bias-gradient-as-a-sum-of-error-signals">Bias Gradient as a Sum of Error Signals</h3> <p>For the bias parameters, we multiply the error matrix by a vector of ones,</p> \[\vec 1 = \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} \in \mathbb{R}^{\batch}.\] <p>This yields</p> \[\begin{align} \matr\Delta^{(\ell)} \vec 1 &amp;= \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} = \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n. \end{align}\] <p>Therefore, the batch gradient with respect to the bias vector is</p> \[\begin{align} \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1 \\ &amp;= \frac{1}{\batch} \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n. \end{align}\] <hr/> <p>These expansions make explicit that the matrix-based batch formulas implement nothing more than an efficient summation (or averaging) of the per-example gradients, fully consistent with the definition of the cost function \(J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n(\Theta).\)</p> <hr/> <h2 id="gradient-descent-parameter-updates">Gradient Descent Parameter Updates</h2> <p>Once the gradients of the cost function $J(\Theta)$ with respect to all parameters have been computed (typically for a full batch or mini-batch; see the section on practial considerations for details), the weights and biases can be updated using a gradient-based optimizer. For standard (batch) gradient descent with learning rate $\eta&gt;0$, the update rule for layer $\ell$ is</p> \[\begin{align} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \, \frac{\partial J}{\partial \Wmatr^{(\ell)}}, \label{eq:gd-update_1} \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \, \frac{\partial J}{\partial \vec b^{(\ell)}}. \label{eq:gd-update} \end{align}\] <p>Using the batch-gradient expressions derived above, we have</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \end{align}\] <p>where $\matr\Delta^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ collects the error vectors of layer $\ell$ for all $\batch$ examples and $\vec 1 \in \mathbb{R}^{\batch}$ is a vector of ones. Substituting these gradients into \eqref{eq:gd-update_1} &amp; \eqref{eq:gd-update} yields the explicit batch update rules</p> \[\begin{align} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \,\frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \,\frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1. \label{eq:gd-update-batch} \end{align}\] <p>These updates are applied for $\ell = 1,2,\ldots,L$. In practice, $J$ is often computed over a mini-batch, so $\batch$ denotes the mini-batch size.</p> <blockquote> <p><strong>Implementation note (when to update).</strong><br/> In a standard backpropagation implementation, one first performs the full backward pass to compute all error signals $\matr\Delta^{(\ell)}$ (and thus all gradients) before updating any parameters. Updating weights prematurely would mix parameters from different stages of the backward pass and can lead to incorrect gradients.</p> <p>That said, with careful bookkeeping (e.g., storing the required activations and error signals, or recomputing them deterministically), updates can be scheduled in different ways. The safest approach is: compute all gradients first, then update. Implement this approach first and make sure everything is working as intended, before considering optimizations of the backpropagation algorithm.</p> </blockquote> <h2 id="initialization-of-backpropagation-at-the-output-layer">Initialization of Backpropagation at the Output Layer</h2> <p>The recursive relations in Eqs. \eqref{eq:chainrule4} and \eqref{eq:chainrule8} express the error signal in layer $\ell$ in terms of the error signals in the subsequent layer $\ell+1$. Consequently, this recursion cannot be evaluated forward and must instead be initialized at the output layer $\ell = L$ and then propagated backward through the network.</p> <p>This backward flow of error signals (from the output layer toward the input layer) is the origin of the term <em>backpropagation</em>.</p> <hr/> <h3 id="output-layer-error-signal-single-training-example">Output-layer Error Signal (single training example)</h3> <p>We begin by computing the error signal for the output layer $\ell = L$. For the mean squared error (MSE) loss and a single training example, the error signal of the $i$-th output neuron is defined as</p> \[\begin{align} \delta_i^{(L)} &amp;:= \frac{\partial \mathcal{L}}{\partial z_i^{(L)}} \\[4pt] &amp;= \frac{\partial}{\partial z_i^{(L)}} \left[ \frac{1}{2} \big(y_i^* - a_i^{(L)}\big)^2 \right] \\[4pt] &amp;= \frac{\partial}{\partial z_i^{(L)}} \left[ \frac{1}{2} \big(y_i^* - \sigma(z_i^{(L)})\big)^2 \right] \\[4pt] &amp;= -\big(y_i^* - \sigma(z_i^{(L)})\big)\, \sigma'\!\left(z_i^{(L)}\right) \\[4pt] &amp;= \big(a_i^{(L)} - y_i^*\big)\, \sigma'\!\left(z_i^{(L)}\right)\\[4pt] &amp;= \big(\yhat_i - y_i^*\big)\, \sigma'\!\left(z_i^{(L)}\right). \end{align}\] <p>This expression provides the starting point for the recursive computation of error signals in all preceding layers.</p> <hr/> <blockquote> <p><strong>Remark (Cross-entropy loss).</strong><br/> For the binary cross-entropy loss in combination with a sigmoid activation function in the output layer (or for the generalized cross-entropy loss with a softmax activation), the derivative of the activation function cancels with a corresponding term from the loss derivative. In this case, the output-layer error signal simplifies to $\delta_i^{(L)} = a_i^{(L)} - y_i^*.$ This simplification is one of the main reasons why cross-entropy loss is commonly preferred over mean squared error for classification tasks.</p> </blockquote> <hr/> <h3 id="vectorized-form-single-training-example">Vectorized Form (single training example)</h3> <p>Collecting the error signals of all output neurons into a vector yields</p> \[\begin{align} \vec{\delta}^{(L)} &amp;= \big(\vec a^{(L)} - \vec y^*\big) \otimes \sigma'\!\left(\vec z^{(L)}\right) \\[4pt] &amp;= \big(\hat{\vec y} - \vec y^*\big) \otimes \sigma'\!\left(\vec z^{(L)}\right), \label{eq:chainrule5} \end{align}\] <p>with the following dimensions:</p> \[\begin{align} \vec{\delta}^{(L)} &amp;\in \mathbb{R}^{d_L}, \\ \vec a^{(L)} = \hat{\vec y}, \vec y^* &amp;\in \mathbb{R}^{d_L}, \\ \vec z^{(L)} &amp;\in \mathbb{R}^{d_L}, \\ \sigma'\!\left(\vec z^{(L)}\right) &amp;\in \mathbb{R}^{d_L}. \end{align}\] <p>Here, $\otimes$ denotes the Hadamard (element-wise) product, and the derivative $\sigma‚Äô(\cdot)$ is applied component-wise to the pre-activation vector $\vec z^{(L)}$.</p> <hr/> <h3 id="vectorized-batch-formulation">Vectorized Batch Formulation</h3> <p>When processing a batch of $\batch$ training examples, the individual output-layer error vectors $\vec{\delta}_n^{(L)}$ are collected column-wise into the batch error matrix</p> \[\begin{align} \matr\Delta^{(L)} &amp;= \begin{pmatrix} \vec{\delta}_1^{(L)} &amp; \vec{\delta}_2^{(L)} &amp; \cdots &amp; \vec{\delta}_\batch^{(L)} \end{pmatrix} \\ &amp;= \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big) \\ &amp;= \big(\matr{\hat{Y}} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big), \end{align}\] <p>with $\matr\Delta^{(L)} \in \mathbb{R}^{d_L \times \batch}$, the predictions</p> \[\matr{\hat{Y}} = \big(\vec{\yhat}_1 \ \ldots \ \vec{\yhat}_\batch \big)\in \mathbb{R}^{d_L \times \batch},\] <p>and the targets (truth labels)</p> \[\matr Y^* = \big(\vec y_1^* \ \ldots \ \vec y_\batch^*\big)\in \mathbb{R}^{d_L \times \batch}.\] <p>This matrix constitutes the initial condition for batch backpropagation. Starting from $\matr\Delta^{(L)}$, the error matrices of all preceding layers $\matr\Delta^{(\ell)}$ are obtained recursively using Eq. \eqref{eq:chainrule8}.</p> <hr/> <h2 id="summary-backpropagation">Summary: Backpropagation</h2> <p>We summarize the steps required to compute all gradients of the cost function $J(\Theta)$ for a batch of $\batch$ training examples using backpropagation. The resulting gradients can then be used in a gradient-descent update.</p> <h3 id="inputs-and-stored-quantities">Inputs and Stored Quantities</h3> <ul> <li><strong>Network architecture</strong> <ul> <li>Number of layers: $L$</li> <li>Layer sizes: $d_0=\din, d_1, \ldots, d_L$</li> </ul> </li> <li><strong>Trainable parameters (for each layer $\ell=1,\ldots,L$)</strong> <ul> <li>$\Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}$</li> <li>$\vec b^{(\ell)} \in \mathbb{R}^{d_\ell}$</li> </ul> </li> <li><strong>Batch data</strong> <ul> <li>Inputs: $\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}$</li> <li>Targets: $\matr Y^* = \big(\vec y_1^* \ \ldots \ \vec y_\batch^*\big)\in \mathbb{R}^{d_L \times \batch}$</li> </ul> </li> <li><strong>Forward-pass cache (must be available from the forward pass)</strong> <ul> <li>Activations: $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ for $\ell=0,\ldots,L$</li> <li>Pre-activations: $\matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ for $\ell=1,\ldots,L$</li> </ul> </li> <li><strong>Element-wise operations</strong> <ul> <li>$\sigma(\cdot)$ and $\sigma‚Äô(\cdot)$ are applied element-wise</li> <li>$\otimes$ denotes the Hadamard (element-wise) product</li> <li>$\vec 1 \in \mathbb{R}^{\batch}$ denotes a vector of ones</li> </ul> </li> </ul> <hr/> <h3 id="backward-pass-error-signals">Backward Pass: Error Signals</h3> <ol> <li> <p><strong>Initialize at the output layer ($\ell=L$)</strong><br/> For mean squared error (MSE), \(\matr\Delta^{(L)} = \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big), \qquad \matr\Delta^{(L)} \in \mathbb{R}^{d_L \times \batch}.\)</p> <p>(Other loss/activation pairs may yield a simpler expression; e.g. cross-entropy + sigmoid/softmax. See below.)</p> </li> <li> <p><strong>Propagate errors backward</strong><br/> For $\ell = L-1, L-2, \ldots, 1$: \(\matr\Delta^{(\ell)} = \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \ \otimes\ \sigma'\!\big(\matr Z^{(\ell)}\big), \qquad \matr\Delta^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}.\)</p> </li> </ol> <hr/> <h3 id="gradients-for-weights-and-biases-batch">Gradients for Weights and Biases (Batch)</h3> <p>For each layer $\ell = 1,\ldots,L$, the gradients of the <strong>cost</strong> \(J(\Theta)=\frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta)\) are</p> <ul> <li> <p><strong>Weight gradient</strong> \(\frac{\partial J}{\partial \Wmatr^{(\ell)}} = \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \qquad \frac{\partial J}{\partial \Wmatr^{(\ell)}} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}.\)</p> </li> <li> <p><strong>Bias gradient</strong> \(\frac{\partial J}{\partial \vec b^{(\ell)}} = \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \qquad \frac{\partial J}{\partial \vec b^{(\ell)}} \in \mathbb{R}^{d_\ell}.\)</p> </li> </ul> <hr/> <h3 id="parameter-update-gradient-descent">Parameter Update (Gradient Descent)</h3> <p>With learning rate $\eta&gt;0$, update for each layer $\ell=1,\ldots,L$:</p> \[\begin{align*} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta\,\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta\,\frac{\partial J}{\partial \vec b^{(\ell)}}. \end{align*}\] <hr/> <h3 id="remarks-1">Remarks</h3> <ul> <li>The backward pass must be evaluated from $\ell=L$ down to $\ell=1$ because each $\matr\Delta^{(\ell)}$ depends on $\matr\Delta^{(\ell+1)}$.</li> <li>The forward-pass matrices ${\matr A^{(\ell)}, \matr Z^{(\ell)}}$ are required to compute $\matr\Delta^{(\ell)}$ and the parameter gradients.</li> <li>In a standard implementation: compute all gradients first, then update all parameters.</li> </ul> <hr/> <h1 id="putting-everything-together-full-batch-vectorized">Putting Everything Together (Full-Batch, Vectorized)</h1> <p>This section summarizes one full <strong>training step</strong> (full batch) in a form that maps 1:1 to an implementation.</p> <hr/> <p><strong>1. Forward Pass (cache activations and pre-activations)</strong></p> <p><em>Initialize</em></p> \[\matr A^{(0)} := \matr X\] <p><em>For each layer $ \ \ell = 1,2,\ldots,L$</em></p> \[\matr Z^{(\ell)} := \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top\] \[\matr A^{(\ell)} := \sigma\!\left(\matr Z^{(\ell)}\right)\] <p><em>Network output</em></p> \[\matr{\hat{Y}} := \matr A^{(L)}\] <p><em>What to store for backprop</em></p> \[\{\matr A^{(\ell)}\}_{\ell=0}^{L}, \qquad \{\matr Z^{(\ell)}\}_{\ell=1}^{L}.\] <hr/> <p><strong>2. Cost (full-batch scalar)</strong></p> <p>Define the batch cost as the mean loss over the batch:</p> \[J(\Theta) := \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}\!\left(\vec y_n^*, \hat{\vec y}_n\right).\] <p>(Equivalently: aggregate a per-example loss column-wise over $\matr Y^*$ and $\matr{\hat{Y}}$.)</p> <hr/> <p><strong>3. Backward Pass</strong></p> <p><strong>Initialize at the output layer</strong></p> <p><em>MSE + general activation</em></p> \[\matr\Delta^{(L)} := \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big).\] <p><em>Binary cross-entropy + sigmoid output (simplified)</em></p> \[\matr\Delta^{(L)} := \matr A^{(L)} - \matr Y^*.\] <p><em>For other loss functions / output actitavations other initializations apply</em></p> <p><strong>Propagate backward for hidden layers</strong></p> <p>For $\ell = L-1, L-2, \ldots, 1$:</p> \[\matr\Delta^{(\ell)} := \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \ \otimes\ \sigma'\!\big(\matr Z^{(\ell)}\big).\] <hr/> <p><strong>4. Gradients (full-batch)</strong></p> <p>For each layer $\ell = 1,\ldots,L$:</p> \[\frac{\partial J}{\partial \Wmatr^{(\ell)}} := \frac{1}{\batch}\;\matr\Delta^{(\ell)}\big(\matr A^{(\ell-1)}\big)^\top\] \[\frac{\partial J}{\partial \vec b^{(\ell)}} := \frac{1}{\batch}\;\matr\Delta^{(\ell)}\vec 1\] <hr/> <p><strong>5. Gradient Descent Update</strong></p> <p>For each layer $\ell = 1,\ldots,L$:</p> \[\begin{align*} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \frac{\partial J}{\partial \Wmatr^{(\ell)}}, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \frac{\partial J}{\partial \vec b^{(\ell)}}. \end{align*}\] <hr/> <p><strong>6. One Full Training Step</strong></p> <p><em>Forward</em></p> \[\begin{align*} \matr A^{(0)} &amp;:= \matr X,\\ \matr Z^{(\ell)} &amp;:= \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)}\vec 1^\top,\\ \matr A^{(\ell)} &amp;:= \sigma(\matr Z^{(\ell)}),\ \ \ell=1..L. \end{align*}\] <p><em>Backward</em></p> \[\begin{align*} \matr\Delta^{(L)} &amp;:= \text{(choose loss/activation-specific formula)},\\ \matr\Delta^{(\ell)} &amp;:= ((\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)})\otimes \sigma'(\matr Z^{(\ell)}),\ \ \ell=L-1..1. \end{align*}\] <p><em>Gradients + update</em></p> \[\begin{align*} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;:= \frac{1}{\batch}\matr\Delta^{(\ell)}(\matr A^{(\ell-1)})^\top,\\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;:= \frac{1}{\batch}\matr\Delta^{(\ell)}\vec 1,\\ (\Wmatr^{(\ell)},\vec b^{(\ell)}) &amp;\leftarrow (\Wmatr^{(\ell)},\vec b^{(\ell)}) - \eta\left(\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \frac{\partial J}{\partial \vec b^{(\ell)}}\right). \end{align*}\] <hr/> <h2 id="practical-considerations">Practical Considerations</h2> <p>The following topics are directly relevant when implementing and training neural networks in practice. They affect numerical stability, convergence behavior, and training efficiency.</p> <h3 id="gradient-checking-finite-differences">Gradient Checking (Finite Differences)</h3> <p>When implementing a neural network from scratch, one of the very first validation tools you should add is <strong>gradient checking</strong>. Backpropagation formulas are easy to derive on paper but surprisingly easy to implement incorrectly (sign errors, missing transposes, wrong broadcasting, etc.). Gradient checking provides a simple but powerful sanity check that your analytical gradients are correct.</p> <p>Gradient checking compares two quantities for the same parameter:</p> <ol> <li> <p><strong>Analytical gradient</strong><br/> Computed via backpropagation: \(\frac{\partial J}{\partial \theta}\), where $\theta$ is a weight or bias parameter in the network.</p> </li> <li> <p><strong>Numerical gradient</strong><br/> Approximated using finite differences applied directly to the scalar cost function $J(\Theta)$.</p> </li> </ol> <p>If both gradients agree (up to a small tolerance), your backpropagation implementation is very likely correct.</p> <p>Let $\theta$ be a single scalar parameter (e.g. one entry of $\Wmatr^{(\ell)}$ or $\vec b^{(\ell)}$). The numerical gradient is approximated by the central difference formula:</p> \[\frac{\partial J}{\partial \theta} \;\approx\; \frac{J(\Theta + \varepsilon\,\mathbf e_\theta) - J(\Theta - \varepsilon\,\mathbf e_\theta)} {2\varepsilon},\] <p>where:</p> <ul> <li>$\varepsilon &gt; 0$ is a small step size (e.g. $10^{-5}$),</li> <li>$\mathbf e_\theta$ denotes a perturbation that affects <em>only</em> parameter $\theta$,</li> <li>$J(\Theta)$ is the total cost of the network, i.e. \(J(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}\!\left(\vec y_n^*, \hat{\vec y}_n\right).\)</li> </ul> <p>Importantly, <strong>no backpropagation is used</strong> to compute this quantity; only forward passes.</p> <p>For a given layer $\ell$, gradient checking compares:</p> <ul> <li> <p><strong>Backprop gradient</strong> \(g_{\text{bp}}\)</p> \[\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \qquad \frac{\partial J}{\partial \vec b^{(\ell)}}\] </li> <li> <p><strong>Numerical gradient</strong> \(g_{\text{num}}\)</p> </li> </ul> \[\left(\frac{\partial J}{\partial \Wmatr^{(\ell)}}\right)^{\text{num}}, \qquad \left(\frac{\partial J}{\partial \vec b^{(\ell)}}\right)^{\text{num}}\] <p>The comparison is can be done using a <strong>relative error</strong> score:</p> \[\mathrm{rel\_err} = \frac{\lvert g_{\text{num}} - g_{\text{bp}} \rvert} {\lvert g_{\text{num}} \rvert + \lvert g_{\text{bp}} \rvert + \varepsilon_{\text{stab}}},\] <p>evaluated element-wise, where $\varepsilon_{\text{stab}}$ is a small value like $10^{-12}$ which is added to the denominator for stability reasons. A common rule of thumb for interpretation for \(\mathrm{rel\_err}\):</p> <table> <thead> <tr> <th>Relative error</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>$&lt; 10^{-7}$</td> <td>Excellent (almost certainly correct)</td> </tr> <tr> <td>$10^{-7}$ ‚Äì $10^{-5}$</td> <td>Probably fine</td> </tr> <tr> <td>$10^{-5}$ ‚Äì $10^{-3}$</td> <td>Suspicious</td> </tr> <tr> <td>$&gt; 10^{-3}$</td> <td>Very likely a bug</td> </tr> </tbody> </table> <p>Small absolute gradients are usually exempted from strict relative-error checks.</p> <p><strong>Practical Remarks</strong></p> <ul> <li>Gradient checking is slow: each parameter requires two forward passes. ‚Üí Use it only for debugging, never during real training.</li> <li>Check one layer at a time, not the whole network at once.</li> <li>Disable regularization, dropout, and data shuffling while checking.</li> <li>Once gradient checking passes, turn it off permanently and trust backprop.</li> </ul> <hr/> <p>In our example implementation, gradient checking is performed by perturbing individual entries of $\Wmatr^{(\ell)}$ or $\vec b^{(\ell)}$, recomputing the total network cost $J(\Theta)$, and comparing the resulting numerical gradients against the gradients produced by backpropagation. This clean separation (numerical gradients from forward passes only, analytical gradients from backprop) makes gradient checking a reliable correctness test.</p> <h3 id="batching-strategies">Batching Strategies</h3> <p>In practice, the cost function</p> \[J(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta)\] <p>is rarely evaluated over the entire training set at once. Instead, training data is processed in batches, leading to different optimization regimes.</p> <ul> <li> <p><strong>Full-batch gradient descent</strong><br/> The gradient of $J(\Theta)$ is computed using all available training examples. This yields an exact descent direction but is computationally expensive and impractical for large datasets.</p> </li> <li> <p><strong>Mini-batch gradient descent</strong><br/> The dataset is split into smaller batches of size $\batch \ll N_{\text{data}}$. Each update uses \(J_{\text{batch}}(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta),\) providing a good trade-off between computational efficiency and gradient quality. This is the standard choice in modern deep learning.</p> </li> <li> <p><strong>Stochastic gradient descent (SGD)</strong><br/> A special case of mini-batch training with $\batch = 1$. Updates are noisy but inexpensive and can help escape shallow local minima and saddle points.</p> </li> </ul> <p>All three methods share the same backpropagation equations; only the batch size $\batch$ differs. In our example below, we use the full-batch approach.</p> <h3 id="weight-initialization">Weight Initialization</h3> <p>Weight initialization plays a critical role in training neural networks. In particular, initializing all weights to zero is a bad idea, because it breaks learning entirely: if all weights in a layer start with the same value, then all neurons in that layer compute identical activations and receive identical gradients during backpropagation. As a result, they remain indistinguishable throughout training, and the network effectively behaves as if each layer had only a single neuron. Random initialization is therefore essential to break this symmetry between neurons.</p> <p>In our implementation, we use Glorot (Xavier) uniform initialization for the weights. For a layer with $d_{\ell-1}$ input units and $d_\ell$ output units, the weights are drawn from a uniform distribution</p> \[w_{i,j}^{(\ell)} \sim \mathcal{U}\!\left( -\sqrt{\frac{6}{d_{\ell-1}+d_\ell}}, \ \sqrt{\frac{6}{d_{\ell-1}+d_\ell}} \right).\] <p>This choice is motivated by the desire to keep the variance of activations and gradients approximately constant across layers, which helps to avoid vanishing or exploding signals during the forward and backward passes.</p> <p>Bias terms are initialized to zero,</p> \[\vec b^{(\ell)} = \vec 0,\] <p>which is common practice and does <em>not</em> cause symmetry issues, since symmetry is already broken by the random weight initialization.</p> <blockquote> <p><strong>Further reading.</strong><br/> Other widely used initialization schemes include He initialization (for ReLU-based networks), orthogonal initialization, and data-dependent or adaptive initialization strategies. These approaches are discussed extensively in the literature.</p> </blockquote> <h3 id="learning-rate-step-size">Learning Rate (Step Size)</h3> <p>The <em>learning rate</em> $\eta&gt;0$ controls the size of the parameter update in each gradient descent step,</p> \[\Wmatr^{(\ell)} \leftarrow \Wmatr^{(\ell)} - \eta\,\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \qquad \vec b^{(\ell)} \leftarrow \vec b^{(\ell)} - \eta\,\frac{\partial J}{\partial \vec b^{(\ell)}}.\] <p>Choosing an appropriate step size is crucial:</p> <ul> <li>If $\eta$ is <em>too small</em>, training progresses very slowly and may appear to stall.</li> <li>If $\eta$ is <em>too large</em>, the updates can overshoot minima, leading to oscillations or divergence of the cost function.</li> </ul> <p>In practice, $\eta$ is often treated as a hyperparameter that must be tuned experimentally. More advanced methods adapt the effective step size automatically during training; these are discussed in the further reading section.</p> <h3 id="input-normalization">Input Normalization</h3> <p>Neural networks are sensitive to the scale and distribution of input features. If different input dimensions have very different magnitudes, the optimization problem becomes poorly conditioned, which can significantly slow down training.</p> <p>A common remedy is <em>input normalization</em>, applied to the input matrix $\matr X = \matr A^{(0)}$ before training:</p> <ul> <li> <p><strong>Feature scaling</strong><br/> Rescale each input feature to a comparable range (e.g. $[0,1]$), preventing some features from dominating others.</p> </li> <li> <p><strong>Zero-mean / unit-variance normalization</strong><br/> Standardize each feature by subtracting its mean and dividing by its standard deviation, \(x_{i,n} \leftarrow \frac{x_{i,n} - \mu_i}{\sigma_i},\) where $\mu_i$ and $\sigma_i$ are computed over the training set.</p> </li> </ul> <p>Well-normalized inputs lead to more stable gradients and typically result in faster and more reliable convergence.</p> <h3 id="vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</h3> <p>In deep networks, gradients are propagated backward through many layers via repeated matrix multiplications and element-wise derivatives. If these factors are consistently smaller than one, gradients can <em>vanish</em>; if they are larger than one, gradients can <em>explode</em>. Both effects impair learning: vanishing gradients slow or stop updates in early layers, while exploding gradients lead to numerical instability.</p> <p>Common causes include poor weight initialization, unsuitable activation functions (e.g. sigmoid in very deep networks), and deep architectures without normalization.</p> <p><strong>Practical mitigation strategies</strong>:</p> <ul> <li>Careful weight initialization (e.g. Glorot initialization)</li> <li>Appropriate activation functions (sigmoid, as used in our example code might be a bad choice for deep nets)</li> <li>Input normalization and regularization</li> <li>Gradient clipping (to control exploding gradients)</li> </ul> <h3 id="convergence-issues">Convergence Issues</h3> <p>Even with correct gradients, optimization may behave poorly:</p> <ul> <li> <p><strong>Divergence</strong><br/> Often caused by an excessively large learning rate, leading to overshooting and unstable updates.</p> </li> <li> <p><strong>Slow convergence</strong><br/> Can result from poor conditioning of the optimization problem, unnormalized inputs, or an overly small learning rate.</p> </li> <li> <p><strong>Plateaus and saddle points</strong><br/> Regions where gradients are close to zero but are not local minima, causing training to progress very slowly.</p> </li> </ul> <p>Understanding and diagnosing these behaviors is essential for successfully training neural networks in practice.</p> <h3 id="training-validation-and-test-data">Training, Validation, and Test Data</h3> <p>In practical machine learning workflows, the available data is typically split into three disjoint subsets:</p> <ul> <li> <p><strong>Training set</strong><br/> Used to compute gradients and update the parameters $\Theta$ via backpropagation and gradient descent.</p> </li> <li> <p><strong>Validation set</strong><br/> Used to monitor performance during training (e.g. for hyperparameter tuning, early stopping, or model selection) without influencing the learned parameters directly.</p> </li> <li> <p><strong>Test set</strong><br/> Used only once, after all training decisions are finalized, to obtain an unbiased estimate of the model‚Äôs generalization performance.</p> </li> </ul> <p>This separation is crucial because evaluating a model on the same data used for training leads to <em>optimistically biased performance estimates</em> and can mask overfitting.</p> <p>In the example below, we deliberately train on the full dataset to keep the mathematical derivations and implementation as transparent as possible. Since the goal here is to understand <em>how</em> backpropagation works (not to assess generalization performance) this simplification is intentional. For any real-world application, however, proper train/validation/test splits are essential.</p> <h2 id="further-reading">Further Reading</h2> <p>The following topics extend (however, the list is still fairly incomplete) beyond the basic feed-forward network and are useful for deeper understanding or more advanced applications.</p> <ul> <li><strong>Regularization</strong> <ul> <li>L2 (weight decay)</li> <li>L1 regularization</li> <li>Early stopping</li> </ul> </li> <li><strong>Adaptive optimization methods</strong> <ul> <li>Momentum</li> <li>RMSProp</li> <li>Adam, AdamW</li> </ul> </li> <li><strong>Advanced activation functions</strong> <ul> <li>ReLU, Leaky ReLU, ELU</li> <li>GELU, Swish</li> <li>Softmax (multiclass classification)</li> </ul> </li> <li><strong>Loss functions</strong> <ul> <li>Binary cross-entropy</li> <li>Categorical cross-entropy</li> <li>Mean squared error (MSE)</li> <li>Mean absolute error (MAE)</li> <li>Huber loss</li> </ul> </li> <li><strong>Numerical stability tricks</strong> <ul> <li>Log-sum-exp trick</li> <li>Stable softmax implementations</li> </ul> </li> <li><strong>Bias‚Äìvariance tradeoff</strong></li> <li><strong>Overfitting vs. underfitting</strong></li> <li><strong>Initialization and symmetry breaking</strong></li> <li><strong>Second-order methods</strong> <ul> <li>Newton‚Äôs method</li> <li>Quasi-Newton methods (L-BFGS)</li> </ul> </li> <li><strong>Backpropagation efficiency</strong> <ul> <li>Computational graphs</li> <li>Automatic differentiation</li> </ul> </li> <li><strong>Other neural network architectures</strong> <ul> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory (LSTM)</li> <li>Gated Recurrent Units (GRU)</li> <li>Transformers</li> </ul> </li> <li><strong>Other training techniques</strong> <ul> <li>Batch normalization</li> <li>Layer normalization</li> <li>Dropout</li> </ul> </li> <li><strong>Expressivity and depth</strong> <ul> <li>Shallow vs. deep networks</li> <li>Universal approximation theorem</li> </ul> </li> </ul> <p>These topics build on the foundations presented in this post and are good entry points for further study.</p> <h1 id="accompanying-jupyter-notebook-code-companion">Accompanying Jupyter Notebook (Code Companion)</h1> <style>.jupyter-child-ext{width:112%;position:relative;left:calc( - 10%)}</style> <div class="jupyter-child-ext"> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/MarkusThill.github.io-jupyter/2025_12_16_intro_neural_nets.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> </div>]]></content><author><name></name></author><category term="math"/><category term="neural nets"/><category term="neural-networks"/><category term="backpropagation"/><category term="deep-learning"/><category term="matrix-notation"/><category term="from-scratch"/><summary type="html"><![CDATA[A notation-first walkthrough of feed-forward neural networks and vectorized backpropagation, focusing on how the math translates directly into clean, correct implementations. Covers forward pass, backprop, batching, and practical training considerations.]]></summary></entry><entry><title type="html">Online and Batch-Incremental Estimation of Covariance Matrices and Means in Python</title><link href="https://markusthill.github.io/blog/2025/online-batch-estimate-cov-mu/" rel="alternate" type="text/html" title="Online and Batch-Incremental Estimation of Covariance Matrices and Means in Python"/><published>2025-09-26T09:00:51+00:00</published><updated>2025-09-26T09:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/online-batch-estimate-cov-mu</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/online-batch-estimate-cov-mu/"><![CDATA[<blockquote> <p>Estimating the mean and covariance matrix of a dataset is a cornerstone of multivariate statistics and machine learning. While batch (offline) methods are straightforward when all data is available at once, many modern applications require online or streaming estimation ‚Äî where data arrives sequentially, potentially at very high rates, and storing all past samples is infeasible.</p> </blockquote> <blockquote> <p>In this Jupyter Notebook below, we explore fully online and batch-incremental estimators for the mean and covariance matrix, including their inverses. We look at how these algorithms work, why they are useful, and how they can adapt to non-stationary data through a forgetting factor. Importantly, these approaches allow us to update estimates efficiently without recomputing matrix inverses from scratch.</p> </blockquote> <blockquote> <p>The goal is to provide both the mathematical background and practical insights for applying online covariance and mean estimation in real-world scenarios such as anomaly detection, adaptive systems, and streaming analytics.</p> </blockquote> <style>.jupyter-child-ext{width:112%;position:relative;left:calc( - 10%)}</style> <div class="jupyter-child-ext"> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/MarkusThill.github.io-jupyter/2025_09_27_online_estimate_cov_mu.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> </div>]]></content><author><name></name></author><category term="programming"/><category term="math"/><category term="stats"/><category term="math"/><summary type="html"><![CDATA[Learn how to estimate the mean, covariance, and inverse covariance matrices in an online or batch-incremental fashion. This post explains the theory behind forgetting factors and effective memory, provides Python implementations for both online and batch estimators, and investigates their accuracy and efficiency through experiments and visualizations.]]></summary></entry><entry><title type="html">The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution</title><link href="https://markusthill.github.io/blog/2025/mahalanobis-distance/" rel="alternate" type="text/html" title="The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution"/><published>2025-09-25T09:00:51+00:00</published><updated>2025-09-25T09:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/mahalanobis-distance</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/mahalanobis-distance/"><![CDATA[<p>\( \renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}} \def\matr#1{\boldsymbol{\mathbf{#1}}} \def\tp{\mathsf T} \DeclareMathOperator{\E}{\mathbb{E}} \)</p> <p>Gaussian distributions are a common choice for anomaly detection, especially when the data is roughly normally distributed. The parameters of the distribution can be estimated using maximum likelihood estimation (MLE), which gives the sample mean and covariance matrix. Once these parameters are known, the next step is to decide on a threshold that separates normal data from anomalies. A simple approach is to set this threshold based on the probability density function (PDF): if a new data point has a PDF value below the threshold, it is flagged as anomalous.</p> <p>In one dimension, this threshold separates the tails of the distribution from its center. In two dimensions, the boundary takes the shape of an ellipse, and in higher dimensions, it becomes an ellipsoid. All points on such a boundary are equally distant from the mean in terms of the Mahalanobis distance, which makes this distance a useful alternative for defining thresholds.</p> <p>Unlike the PDF, the Mahalanobis distance does not rely on assuming a Gaussian distribution. Still, if the data is approximately Gaussian, the squared Mahalanobis distance follows a Chi-square distribution‚Äîa relationship that can also be confirmed visually using a quantile‚Äìquantile plot.</p> <h1 id="prerequisites">Prerequisites</h1> <h2 id="matrix-algebra">Matrix Algebra</h2> <p>The product of an $n \times \ell$ matrix $\matr A$ and an $\ell \times p$ matrix $\matr B$ is defined entry-wise as</p> \[(\matr A \matr B)_{ij} = \sum_{k=1}^\ell \matr A_{ik}\,\matr B_{kj}.\] <p>In particular, the product of a matrix $\matr A$ with its transpose $\matr A^\top$ can be written as</p> \[\begin{align} (\matr A \matr A^\top)_{ij} &amp;= \sum_{k=1}^\ell \matr A_{ik}\,\matr A^\top_{kj} = \sum_{k=1}^\ell \matr A_{ik}\,\matr A_{jk}, \nonumber \\[6pt] \matr A \matr A^\top &amp;= \sum_{k=1}^\ell \vec a_{k}\,\vec a_{k}^\top, \label{eq:matrixProductWithTranspose} \end{align}\] <p>where $\vec a_{k}$ denotes the $k$-th column vector of $\matr A$.</p> <p>We will also use the following simple relation for vectors $\vec a, \vec b$:</p> \[\begin{align} x &amp;= \vec a^\top \vec b, \\ y &amp;= \vec b^\top \vec a = x^\top = x, \\ xy &amp;= \vec a^\top \vec b \,\vec b^\top \vec a = x^2 = (\vec a^\top \vec b)^2. \label{eq:multOfTwoScalars} \end{align}\] <p><em>(Inverse of a Matrix Product)</em></p> <p>For invertible square matrices $ \matr A \in \mathbb R^{n \times n} $ and $ \matr B \in \mathbb R^{n \times n} $, the inverse of their product is</p> \[\begin{align} (\matr A \matr B)^{-1} &amp;= \matr B^{-1} \matr A^{-1}. \label{eq:inverseProduct} \end{align}\] <p>Indeed,</p> \[\begin{align} (\matr A \matr B)(\matr B^{-1} \matr A^{-1}) &amp;= \matr A(\matr B \matr B^{-1}) \matr A^{-1} \\[4pt] &amp;= \matr A \mathbf I \matr A^{-1} \\[4pt] &amp;= \matr A \matr A^{-1} \\[4pt] &amp;= \mathbf I, \end{align}\] <p>which verifies the result.</p> <p>Note that the order of the factors is crucial: in general $ \matr B^{-1} \matr A^{-1} \ne \matr A^{-1} \matr B^{-1} $, since matrix multiplication is not commutative.</p> <p><br/></p> <h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2> <p>For a square $n \times n$ matrix $\matr A$, a non-zero vector $\vec u$ is called an eigenvector of $\matr A$ if it satisfies</p> \[\matr A \vec u = \lambda \vec u,\] <p>where the scalar $\lambda$ is the corresponding eigenvalue.</p> <p>If the eigenvectors of $\matr A$ are collected as the columns of a matrix $\matr U \in \mathbb R^{n \times n}$, with the $i$-th column given by $\vec u^{(i)}$, and if $\matr \Lambda$ is the diagonal matrix containing the associated eigenvalues $\lambda_i$, then</p> \[\begin{align} \matr A \matr U &amp;= \matr U \matr \Lambda, \nonumber \\[6pt] \matr A &amp;= \matr U \matr \Lambda \matr U^{-1}, \label{eq:eigendecomp} \end{align}\] <p>which is known as the <strong>eigenvalue decomposition</strong> of $\matr A$.</p> <p>If $\matr A$ is symmetric, its eigenvectors are orthogonal (and can be chosen to be orthonormal). In that case, $\matr U$ is an orthogonal matrix, so $\matr U^{-1} = \matr U^\top$, and the decomposition simplifies to</p> \[\matr A = \matr U \matr \Lambda \matr U^\top.\] <p>The square root of a matrix $\matr A$ (denoted $\matr A^{\tfrac{1}{2}}$), defined such that $\matr A^{\tfrac{1}{2}} \matr A^{\tfrac{1}{2}} = \matr A$, can be expressed using the eigenvalue decomposition as:</p> \[\begin{align} \matr A^{\tfrac{1}{2}} &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp}, \label{eq:sqrtSymMatrix} \end{align}\] <p>where $\matr \Lambda^{\tfrac{1}{2}}$ is the diagonal matrix containing the square roots of the eigenvalues of $\matr A$.</p> <p>Verifying this:</p> \[\begin{align*} \matr A^{\tfrac{1}{2}} \cdot \matr A^{\tfrac{1}{2}} &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \\ &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr I \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \\ &amp;= \matr U \matr \Lambda \matr U^{\tp} \\ &amp;= \matr A. \end{align*}\] <p>Similarly, the inverse of $\matr A$ can be expressed through its eigenvalue decomposition. Using the associativity of matrix multiplication, we obtain:</p> \[\begin{align} \matr A^{-1} &amp;= \big( \matr U \matr \Lambda \matr U^{-1} \big)^{-1} \\ &amp;= \big( \matr U^{-1} \big)^{-1} \matr \Lambda^{-1} \matr U^{-1} \\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1} \nonumber \\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{\tp}, \label{eq:eigenvalueInverse} \end{align}\] <p>where $\matr \Lambda^{-1}$ is a diagonal matrix containing the reciprocals of the eigenvalues of $\matr A$.</p> <p>Note that $\matr \Lambda^{-1}$ is again a diagonal matrix containing the reciprocal eigenvalues of $\matr A$.</p> <p><br/></p> <h2 id="linear-affine-transform-of-a-normally-distributed-random-variable">Linear Affine Transform of a Normally Distributed Random Variable</h2> <p>Consider a random variable $X \sim \mathcal N(\vec \mu_x, \matr \Sigma_x)$ with mean vector $\vec \mu_x$ and covariance matrix $\matr \Sigma_x$.<br/> Applying a linear affine transformation with matrix $\matr A$ and vector $\vec b$ yields a new random variable $Y$:</p> \[\begin{align} Y = \matr A X + \vec b. \end{align}\] <p>The mean $\vec \mu_y$ and covariance $\matr \Sigma_y$ of $Y$ can be derived as follows:</p> \[\begin{align} \vec \mu_y &amp;= \E \{ Y \} \\ &amp;= \E \{ \matr A X + \vec b \} \\ &amp;= \matr A \E \{ X \} + \vec b \\ &amp;= \matr A \vec \mu_x + \vec b, \label{eq:AffineLinearTransformMean} \end{align}\] <p>and</p> \[\begin{align} \matr \Sigma_y &amp;= \E \{ (Y - \vec \mu_y)(Y - \vec \mu_y)^\tp \} \\ &amp;= \E \{ \big[ \matr A (X - \vec \mu_x) \big] \big[ \matr A (X - \vec \mu_x) \big]^\tp \} \\ &amp;= \E \{ \matr A (X - \vec \mu_x)(X - \vec \mu_x)^\tp \matr A^\tp \} \\ &amp;= \matr A \E \{ (X - \vec \mu_x)(X - \vec \mu_x)^\tp \} \matr A^\tp \\ &amp;= \matr A \matr \Sigma_x \matr A^\tp. \label{eq:AffineLinearTransformCovariance} \end{align}\] <p>Thus, affine transformations preserve Gaussianity: the result is still normally distributed, but with a mean shifted and scaled by $\matr A$ and $\vec b$, and a covariance matrix transformed as $\matr A \matr \Sigma_x \matr A^\tp$.</p> <h2 id="quantile-estimation-for-multivariate-gaussian-distributions">Quantile Estimation for Multivariate Gaussian Distributions</h2> <p>Estimating quantiles for multivariate Gaussian distributions is more involved than in the one-dimensional case. In one dimension, quantiles can be obtained by directly evaluating the cumulative distribution function in the tails of the distribution. In higher dimensions, however, this approach does not generalize in a straightforward way.</p> <p>In the bivariate case, quantiles can be visualized as ellipses, and in higher dimensions as ellipsoids. A useful tool for describing such contours is the <strong>Mahalanobis distance</strong>, which measures the distance of a point from the mean while accounting for the covariance structure of the distribution. All points at the same Mahalanobis distance from the mean lie on the surface of an ellipsoid.</p> <p>More formally, the usual definition of a $p$-quantile involves a random variable: the $p$-quantile of a distribution is the value $q_p$ such that the probability of the random variable being less than or equal to $q_p$ is exactly $p$. For a multivariate Gaussian distribution, we can treat the <strong>squared Mahalanobis distance</strong> between a random point $\vec x$ and the mean $\vec \mu$ as such a random variable:</p> \[d^2 = (\vec x - \vec \mu)^\tp \matr \Sigma^{-1} (\vec x - \vec \mu).\] <p>The $p$-quantile then corresponds to the threshold value $q_p$ such that</p> \[P(d^2 \leq q_p) = p.\] <p>Geometrically, the set of all points with $d^2 \leq q_p$ forms an ellipsoid centered at the mean.</p> <p>A naive way to compute these quantiles is through a <strong>Monte Carlo approach</strong>: sample many points from the multivariate Gaussian distribution, compute their Mahalanobis distances, and then estimate the quantile empirically. While straightforward, this method becomes computationally inefficient, especially if quantiles need to be evaluated repeatedly.</p> <p>Fortunately, there is a more direct connection: the squared Mahalanobis distance of a Gaussian-distributed random vector follows a <strong>Chi-square distribution</strong> with degrees of freedom equal to the dimensionality $k$ of the Gaussian. This means that the $p$-quantile can be computed directly as</p> \[q_p = \chi^2_{k,p},\] <p>where $\chi^2_{k,p}$ is the $p$-quantile of the Chi-square distribution with $k$ degrees of freedom. In other words, for a $k$-dimensional Gaussian, the region</p> \[\{ \vec x \in \mathbb R^k : d^2 \leq \chi^2_{k,p} \}\] <p>defines the ellipsoidal contour that contains probability mass $p$.</p> <p><br/></p> <h3 id="empirical-evidence-that-the-mahalanobis-distance-is-chi-square-distributed">Empirical Evidence that the Mahalanobis Distance is Chi-Square Distributed</h3> <p>The relationship between the Mahalanobis distance and the Chi-square distribution can also be verified empirically.<br/> A common tool for this is the <strong>Quantile-Quantile (Q-Q) plot</strong>, which compares the quantiles of two distributions.<br/> If the squared Mahalanobis distance of samples drawn from a Gaussian distribution truly follows a Chi-square distribution, then the Q-Q plot of their quantiles should lie approximately on the identity line.</p> <p>The following R script demonstrates this approach by generating a sample from a multivariate Gaussian distribution, computing the squared Mahalanobis distances, and comparing them against the theoretical Chi-square quantiles:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">Matrix</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">DIM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="n">nSample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="w">

</span><span class="n">Posdef</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">ev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
  </span><span class="n">decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
  </span><span class="n">Q</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.Q</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">R</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.R</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="w">
  </span><span class="n">ph</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w">
  </span><span class="n">O</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ph</span><span class="p">)</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">O</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ev</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">O</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Posdef</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">
</span><span class="n">muhat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">


</span><span class="n">sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mvrnorm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">nSample</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">muhat</span><span class="p">,</span><span class="w"> </span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sigma</span><span class="p">)</span><span class="w">
</span><span class="n">C</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">.5</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">det</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="nb">pi</span><span class="o">*</span><span class="n">Sigma</span><span class="p">))</span><span class="w">
</span><span class="n">mahaDist2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mahalanobis</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="o">=</span><span class="n">muhat</span><span class="p">,</span><span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span><span class="w">

</span><span class="c1">#</span><span class="w">
</span><span class="c1"># Interestingly, the Mahalanobis distance of samples follows a Chi-Square distribution</span><span class="w">
</span><span class="c1"># with d degrees of freedom</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="n">pps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">100+1</span><span class="p">)</span><span class="w">
</span><span class="n">qq1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="n">quantile</span><span class="p">(</span><span class="n">mahaDist2</span><span class="p">,</span><span class="w"> </span><span class="n">probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">})</span><span class="w">
</span><span class="n">qq2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">qchisq</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">ncol</span><span class="p">(</span><span class="n">Sigma</span><span class="p">))</span><span class="w">

</span><span class="n">dat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">qEmp</span><span class="o">=</span><span class="w"> </span><span class="n">qq1</span><span class="p">,</span><span class="w"> </span><span class="n">qChiSq</span><span class="o">=</span><span class="n">qq2</span><span class="p">)</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dat</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">qEmp</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">qChiSq</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Sample quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Chi-Squared Quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></figure> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-480.webp 480w,/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-800.webp 800w,/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-09-25-mahalanobis-distance/q-q-plot.png" class="img-fluid rounded z-depth-1 imgcenter" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 1:</b> Quantile‚ÄìQuantile (Q‚ÄìQ) plot comparing the squared Mahalanobis distances of a Gaussian sample with the theoretical Chi-square distribution. The close alignment of the points along the diagonal line indicates that the squared Mahalanobis distance is well-approximated by a Chi-square distribution. </figcaption> </figure> <p><br/></p> <h1 id="the-squared-mahalanobis-distance-follows-a-chi-square-distribution">The Squared Mahalanobis Distance follows a Chi-Square Distribution</h1> <p>In this section we prove the conjecture: ‚ÄúThe squared Mahalanobis distance of a Gaussian-distributed random vector $\matr X$ and the center $\vec\mu$ of this Gaussian distribution follows a Chi-square distribution.‚Äù</p> <p><br/></p> <h2 id="derivation-based-on-the-eigenvalue-decomposition">Derivation Based on the Eigenvalue Decomposition</h2> <p>The Mahalanobis distance between two points $\vec x$ and $\vec y$ is defined as</p> \[\begin{align} d(\vec x,\vec y) = \sqrt{(\vec x -\vec y )^\tp \matr \Sigma^{-1} (\vec x - \vec y)}. \end{align}\] <p>Thus, the squared Mahalanobis distance of a random vector $\matr X$ and the center $\vec \mu$ of a multivariate Gaussian distribution is defined as:</p> \[\begin{align} D = d(\matr X,\vec \mu)^2 = (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ), \label{eq:sqMahalanobis} \end{align}\] <p>where $\matr \Sigma$ is a $\ell \times \ell$ covariance matrix and $\vec \mu \in \mathbb{R}^\ell$ is the mean vector. In order to achieve a different representation of $D$ one can first perform an eigenvalue decomposition on $\matr \Sigma^{-1}$ which is (with Eq. $\eqref{eq:eigenvalueInverse}$ and assuming orthonormal eigenvectors):</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1} = \matr U \matr \Lambda^{-1} \matr U^{T} \end{align}\] <p>With Eq. \eqref{eq:matrixProductWithTranspose} we obtain:</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \label{eq:SigmaInverseAsSum} \end{align}\] <p>where $\vec u_{k}$ is the $k$-th eigenvector of the corresponding eigenvalue $\lambda_k$. Plugging \eqref{eq:SigmaInverseAsSum} back into \eqref{eq:sqMahalanobis} results in:</p> \[\begin{align*} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) = (\matr X -\vec \mu )^\tp \Bigg( \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \Bigg) (\matr X - \vec \mu ) \\ &amp;= \sum_{k=1}^\ell \lambda_k^{-1} (\matr X -\vec \mu )^\tp \vec u_{k} \vec u_{k}^\tp (\matr X - \vec \mu )\\ &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \Big[ \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2 = \sum_{k=1}^\ell \Big[ \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2\\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align*}\] <p>where $Y_k$ is a new random variable based on an affine linear transform of the random vector $\matr X$. According to Eq. \eqref{eq:AffineLinearTransformMean} , we have $\matr Z = (\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)$. If we set $ \vec a_{k}^\tp = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp$ then we get $Y_k = \vec a_{k}^\tp \matr Z = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \matr Z$. Note that $Y_k$ is now a random variable drawn from a univariate normal distribution $Y_k \thicksim N(0,\sigma_k^2)$, where, according to \eqref{eq:AffineLinearTransformCovariance}:</p> \[\begin{align} \sigma_k^2 &amp;= \vec a_{k}^\tp \Sigma \vec a_{k}= \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \Sigma \lambda_k^{-\frac{1}{2}} \vec u_{k} \\ &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} \label{eq:smallSigma} \end{align}\] <p>If we insert $\matr \Sigma = \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp$ into Eq. \eqref{eq:smallSigma}, we get:</p> \[\begin{align*} \sigma_k^2 &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} = \lambda_k^{-1} \vec u_{k}^\tp \Bigg( \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp \Bigg) \vec u_{k} = \sum_{j=1}^\ell \lambda_k^{-1} \vec u_{k}^\tp \lambda_j \vec u_{j} \vec u_{j}^\tp \vec u_{k} \\ &amp;= \sum_{j=1}^\ell \lambda_k^{-1} \lambda_j \vec u_{k}^\tp \vec u_{j} \vec u_{j}^\tp \vec u_{k} \end{align*}\] <p>Since all eigenvectors $\vec u_{i}$ are pairwise orthonormal the dotted products $\vec u_{k}^\tp \vec u_{j}$ and $\vec u_{j}^\tp \vec u_{k}$ will be zero for $j \neq k$. Only for the case $j = k$ we get:</p> \[\begin{align*} \sigma_k^2 &amp;= \lambda_k^{-1} \lambda_k \vec u_{k}^\tp \vec u_{k} \vec u_{k}^\tp \vec u_{k} = \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 = \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 \\ &amp;= 1, \end{align*}\] <p>since the the norm $||\vec u_{k}||$ of a orthonormal eigenvector is equal to 1. Thus, the squared Mahalanobis distance can be expressed as:</p> \[\begin{align*} D = \sum_{k=1}^\ell Y_k^2, \end{align*}\] <p>where</p> \[\begin{align} Y_k \thicksim N(0,1). \end{align}\] <p>Now the Chi-square distribution with $\ell$ degrees of freedom is exactly defined as being the distribution of a variable which is the sum of the squares of $\ell$ random variables being standard normally distributed. Hence, $D$ is Chi-square distributed with $\ell$ degrees of freedom.</p> <p><br/></p> <h2 id="alternative-derivation-based-on-the-whitening-property-of-the-mahalanobis-distance">Alternative Derivation Based on the Whitening Property of the Mahalanobis Distance</h2> <p>Since the inverse $\matr \Sigma^{-1}$ of the covariance matrix $\matr \Sigma$ is also a symmetric matrix, its square root can be found ‚Äì based on Eq. \eqref{eq:sqrtSymMatrix} ‚Äì to be a symmetric matrix. In this case we can write the squared Mahalanobis distance as</p> \[\begin{align*} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) = (\matr X -\vec \mu )^\tp \matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu )\\ &amp;= \Big( \matr \Sigma^{-\frac{1}{2}} (\matr X -\vec \mu ) \Big)^\tp \Big(\matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu ) \Big) = \matr Y^\tp \matr Y = ||\matr Y||^2 \\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align*}\] <p>The multiplication $\matr Y = \matr W \matr Z$, with $\matr W=\matr \Sigma^{-\frac{1}{2}}$ and $\matr Z= \matr X -\vec \mu $ is typically referred to as a whitening transform, where in this case $\matr W=\matr \Sigma^{-\frac{1}{2}}$ is the so called Mahalanobis (or ZCA) whitening matrix. $\matr Y$ has zero mean, since $(\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)$. Due to the (linear) whitening transform the new covariance matrix $\matr \Sigma_y$ is the identity matrix $\matr I$, as shown in the following (using the property in Eq. \eqref{eq:AffineLinearTransformCovariance}):</p> \[\begin{align*} \matr \Sigma_y &amp;= \matr W \matr \Sigma \matr W^\tp = \matr \Sigma^{-\frac{1}{2}} \matr \Sigma \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp = \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp \\ &amp;= \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \matr \Sigma^{-\frac{1}{2}} = \Big(\matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{\frac{1}{2}} \Big) \Big(\matr \Sigma^{\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}}\Big)\\ &amp;= \matr I. \end{align*}\] <p>Hence, all elements $Y_k$ in the random vector $\matr Y$ are random variables drawn from independent normal distributions $Y_k \thicksim N(0,1)$, which leads us to the same conclusion as before, that $D$ is Chi-square distributed with $\ell$ degrees of freedom.</p> <p><br/></p> <h1 id="conclusion">Conclusion</h1> <p>We have shown that the squared Mahalanobis distance of a Gaussian random vector follows a Chi-square distribution with degrees of freedom equal to the dimension of the data.<br/> This result has a very practical consequence: it allows us to replace heuristic or Monte Carlo‚Äìbased thresholding with an exact statistical criterion.</p> <p>In anomaly detection, instead of deciding ‚Äúby hand‚Äù where to cut off the probability density, we can set thresholds directly using the quantiles of the Chi-square distribution.<br/> For example, in $\ell$ dimensions, the 95% confidence region of a Gaussian distribution is exactly the ellipsoid</p> \[\{ \vec x \in \mathbb R^\ell : d^2 \leq \chi^2_{\ell,0.95} \}.\] <p>This means we can flag data points outside this ellipsoid as anomalies with a well-defined false alarm rate.</p> <p>More generally, the Mahalanobis distance provides a natural way to measure how unusual a point is relative to a multivariate distribution. Combined with the Chi-square connection, it gives both a geometric intuition (ellipsoids of equal distance) and a rigorous statistical tool for multivariate analysis.</p> <p>üëâ A more detailed exploration of the Mahalanobis distance ‚Äî including multiple Python implementations, benchmarks, and visualizations ‚Äî is covered in a separate blog post <strong><a href="/blog/2025/mahalanobis-distance-implementations/">here</a></strong>.</p>]]></content><author><name></name></author><category term="programming"/><category term="math"/><category term="stats"/><category term="math"/><summary type="html"><![CDATA[This post explores why the squared Mahalanobis distance of Gaussian data follows a Chi-square distribution. We cover the theory step by step, show empirical evidence, and explain how this relationship provides a principled way to set anomaly detection thresholds using quantiles.A companion Jupyter Notebook with code, benchmarks, and visualizations is provided to put the theory into practice.]]></summary></entry><entry><title type="html">Notes on the Runtime Complexity of Latin Hypercube Sampling</title><link href="https://markusthill.github.io/blog/2025/a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/" rel="alternate" type="text/html" title="Notes on the Runtime Complexity of Latin Hypercube Sampling"/><published>2025-09-24T19:00:51+00:00</published><updated>2025-09-24T19:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/"><![CDATA[<p>\( <br/> \def\myT{\mathsf{T}} \def\myPhi{\mathbf{\Phi}} \)</p> <p>Recently, I needed to tune several parameters of a simple algorithm (five in total). To efficiently explore the parameter space, I chose to use a <strong>Latin Hypercube Sampling (LHS)</strong> design. Because each design point could be evaluated very quickly, selecting <strong>1,000 design points</strong> seemed reasonable.</p> <p>In <strong>R</strong>, the <code class="language-plaintext highlighter-rouge">lhs</code> package provides a convenient function, <code class="language-plaintext highlighter-rouge">optimumLHS</code>, to generate such designs. However, when attempting to generate (n = 1000) points, the function failed to return even after several minutes. This behavior highlights an important issue: <strong>the runtime complexity of the LHS algorithm</strong>.</p> <p>To better understand this, I measured the runtime of LHS for different numbers of design points. The following R code illustrates how these measurements were obtained:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">lhs</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">pbapply</span><span class="p">)</span><span class="w">

</span><span class="n">runLHS</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">whichN</span><span class="p">,</span><span class="w"> </span><span class="n">repeats</span><span class="p">){</span><span class="w">
  </span><span class="n">lhsTimes</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">()</span><span class="w">
  </span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">whichN</span><span class="p">){</span><span class="w">
    </span><span class="k">for</span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">repeats</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="n">tStart</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">proc.time</span><span class="p">()</span><span class="w">
      </span><span class="nf">invisible</span><span class="p">(</span><span class="n">optimumLHS</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">))</span><span class="w">
      </span><span class="n">tt</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="nf">proc.time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">tStart</span><span class="p">)</span><span class="w">
      </span><span class="n">lhsTimes</span><span class="o">&lt;-</span><span class="n">rbind</span><span class="p">(</span><span class="n">lhsTimes</span><span class="p">,</span><span class="n">data.frame</span><span class="p">(</span><span class="n">user</span><span class="o">=</span><span class="n">tt</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">system</span><span class="o">=</span><span class="n">tt</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">elapsed</span><span class="o">=</span><span class="n">tt</span><span class="p">[</span><span class="m">3</span><span class="p">],</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="n">i</span><span class="p">))</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">lhsTimes</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">


</span><span class="n">res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pblapply</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">rev</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">30</span><span class="p">)</span><span class="o">*</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="o">=</span><span class="n">runLHS</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">cl</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">do.call</span><span class="p">(</span><span class="n">rbind</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">)</span><span class="w">
</span><span class="c1"># saveRDS(object=res, file="lhs.rds") # In case you want to save the results</span><span class="w">
</span><span class="c1"># res &lt;- readRDS(file="lhs.rds") # In case you want to load the results</span><span class="w">
</span><span class="n">stop</span><span class="p">(</span><span class="s2">"Stop here for the moment"</span><span class="p">)</span><span class="w">

</span><span class="n">plotData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res</span><span class="p">[</span><span class="n">which</span><span class="p">(</span><span class="n">res</span><span class="o">$</span><span class="n">elapsed</span><span class="o">&gt;</span><span class="m">.01</span><span class="p">),]</span><span class="w"> </span><span class="c1"># smaller values are not really interesting</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">plotData</span><span class="p">,</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">user</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"computation time / s"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">12</span><span class="p">),</span><span class="w">
        </span><span class="n">axis.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">14</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></figure> <hr/> <p></p> <p>The corresponding plot is shown below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/initialplot-480.webp 480w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/initialplot-800.webp 800w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/initialplot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/initialplot.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 1:</b> Computation time of the LHS algorithm for different numbers of design points \(n\). For each \(n\) we ran LHS 10 times and measured the times. Each design point is a 5-dimensional vector. </figcaption> </figure> <p>At first glance, the runtime appears to grow at least polynomially, and it could even suggest exponential growth. To investigate this further, we add a logarithmic scale to the y-axis:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/log_y_scalePlot-480.webp 480w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/log_y_scalePlot-800.webp 800w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/log_y_scalePlot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/log_y_scalePlot.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 2:</b> Same as before, but with a log10 scale on the y-axis. From this view, the runtime growth does not appear exponential. </figcaption> </figure> <p>If the runtime were exponential, the log-scaled y-axis would produce a straight line. Since this is not the case, a polynomial growth seems more likely. To confirm, we also apply a log10 scale to the x-axis:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logxyPlot-480.webp 480w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logxyPlot-800.webp 800w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logxyPlot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logxyPlot.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 3:</b> Same as Fig. 1, but with log10 scales on both axes. The resulting straight line suggests that the runtime complexity of LHS is polynomial. </figcaption> </figure> <p>Since we now observe a straight line on the log-log plot, it is reasonable to conclude that the runtime of the LHS algorithm grows polynomially. Next, it is interesting to determine the order of this polynomial. To do so, we estimate the slope and intercept of the line using linear regression.</p> <p>Because we have 10 measurements for each value of \(n\), we can employ a weighted linear regression approach to account for variations in the observed runtimes.</p> \[\begin{align} \vec{\theta} = \big(\myPhi^\myT \mathbf{W} \myPhi + \lambda \mathbf{I} \big)^{-1} \myPhi^\myT \mathbf{W} \vec{y}_* \end{align}\] <p>with</p> \[\mathbf{W} = \mbox{diag}(\vec{w})\] <p>and</p> \[\begin{align} w_i = \frac{1}{\sigma_{x_i}^2}. \end{align}\] <p>The R code for estimating the Intercept \(b\) and the slope \(m\) of the line is as follows:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># Apply log10 to the data</span><span class="w">
</span><span class="n">log_n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log10</span><span class="p">(</span><span class="n">plotData</span><span class="o">$</span><span class="n">n</span><span class="p">)</span><span class="w">
</span><span class="n">log_y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log10</span><span class="p">(</span><span class="n">plotData</span><span class="o">$</span><span class="n">user</span><span class="p">)</span><span class="w">
</span><span class="n">log_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">log_n</span><span class="p">,</span><span class="w"> </span><span class="n">log_y</span><span class="p">)</span><span class="w">

</span><span class="c1">#</span><span class="w">
</span><span class="c1"># Since we repeated each run several times: Measure the variance for each point n</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="n">grouped</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">group_by</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">log_n</span><span class="o">=</span><span class="n">log_n</span><span class="p">),</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">log_y</span><span class="o">=</span><span class="n">log_y</span><span class="p">)),</span><span class="w"> </span><span class="n">log_n</span><span class="p">)</span><span class="w">
</span><span class="n">stats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">summarise</span><span class="p">(</span><span class="n">grouped</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">log_y</span><span class="p">),</span><span class="w"> </span><span class="n">var</span><span class="o">=</span><span class="n">var</span><span class="p">(</span><span class="n">log_y</span><span class="p">))</span><span class="w">

</span><span class="c1"># Now assign the corresponding variance to each data-point again</span><span class="w">
</span><span class="n">mergedData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">merge</span><span class="p">(</span><span class="n">log_data</span><span class="p">,</span><span class="w"> </span><span class="n">stats</span><span class="p">)</span><span class="w">

</span><span class="c1"># Prepare the vectors and matrices for the weighted linear least squares estimator</span><span class="w">
</span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="m">1.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mergedData</span><span class="o">$</span><span class="n">var</span><span class="p">)</span><span class="w">
</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">mergedData</span><span class="o">$</span><span class="n">log_n</span><span class="p">)</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mergedData</span><span class="o">$</span><span class="n">log_y</span><span class="w">

</span><span class="c1"># Estimate the Intercept and Slope using weighted linear least squares</span><span class="w">
</span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ginv</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">y</span><span class="w">
</span><span class="n">cat</span><span class="p">(</span><span class="s2">"slope m="</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="s2">". Intercept b="</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span><span class="w">

</span><span class="c1"># Plot the estimated line</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">plotData</span><span class="p">,</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">user</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="o">=</span><span class="n">theta</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">intercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">theta</span><span class="p">[</span><span class="m">1</span><span class="p">])</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"computation time / s"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">axis.text</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">12</span><span class="p">),</span><span class="w">
        </span><span class="n">axis.title</span><span class="o">=</span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">14</span><span class="p">,</span><span class="n">face</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></figure> <hr/> <p></p> <p>The estimator computes the following values for the slope and the intercept:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">Slope</span><span class="w"> </span><span class="n">m</span><span class="o">=</span><span class="m">3.993954</span><span class="n">.</span><span class="w"> </span><span class="n">Intercept</span><span class="w"> </span><span class="n">b</span><span class="o">=</span><span class="m">-7.875513</span></code></pre></figure> <p>The fitted line aligns closely with the data, as illustrated in the plot below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logXYfitPlot-480.webp 480w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logXYfitPlot-800.webp 800w,/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logXYfitPlot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-09-23-a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/logXYfitPlot.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <b>Figure 4:</b> Computation time of the LHS algorithm for different numbers of design points \(n\), shown on a log10 scale for both axes. A fitted line through all points helps estimate the runtime complexity of LHS. </figcaption> </figure> <p>Using the estimated slope and intercept, we can now express the expected runtime of the LHS algorithm as a function of the number of design points \(n\):</p> \[\begin{align} \log_{10} y &amp;= m \log_{10} n + b \\ f(n) = y &amp;= 10 ^{m \log_{10} n + b} \\ &amp;= \big(10^{\log_{10} n} \big)^m \cdot 10^b \\ &amp;= 10^b \cdot n^m \\ &amp;= 10^{-7.8755} \cdot n^{3.9939} \\ &amp;\approx 10^{-7.8} \cdot n^{4} \end{align}\] <p>The estimated runtime from the fitted line is slightly pessimistic, so in practice the actual computation time (at least on my machine) is likely a bit lower. Extrapolating this estimate to \(n=1000\), we would expect a runtime of approximately 16,000 seconds, or about 4.5 hours.</p> <p>In summary, we can conclude that the LHS algorithm exhibits a runtime complexity of order \(\mathcal{O}(n^4)\).</p>]]></content><author><name></name></author><category term="programming"/><category term="math"/><category term="math"/><summary type="html"><![CDATA[Exploring the runtime complexity of the Latin Hypercube Sampling (LHS) algorithm, this post investigates how computation time scales with the number of design points. By measuring runtimes, applying log-log transformations, and using weighted linear regression, we estimate the polynomial order of growth and provide practical insights into the expected performance of LHS for larger datasets.]]></summary></entry><entry><title type="html">Implementing the Mahalanobis Distance in Python</title><link href="https://markusthill.github.io/blog/2025/mahalanobis-distance-implementations/" rel="alternate" type="text/html" title="Implementing the Mahalanobis Distance in Python"/><published>2025-09-24T09:00:51+00:00</published><updated>2025-09-24T09:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/mahalanobis-distance-implementations</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/mahalanobis-distance-implementations/"><![CDATA[<blockquote> <p>Below you will find a Jupyter Notebook that explores the Mahalanobis distance in depth.<br/> It begins with the theoretical foundations and practical applications, followed by multiple Python implementations (NumPy, JAX, TensorFlow, SciPy) to ensure correctness and compare performance.<br/> The notebook validates these implementations, benchmarks them across low- and high-dimensional datasets, and illustrates the geometric intuition behind the Mahalanobis distance through visualizations and whitening transformations.<br/> Finally, it demonstrates the close connection to the Chi-square distribution and applies the method to a simple anomaly detection task.</p> <p>The aim is to provide both a solid theoretical understanding and practical tools for applying the Mahalanobis distance in real-world scenarios.</p> </blockquote> <p>üëâ A more theoretical discussion of the Mahalanobis distance and its connection to the Chi-square distribution can be found <strong><a href="/blog/2025/mahalanobis-distance/">here</a></strong>.</p> <style>.jupyter-child-ext{width:112%;position:relative;left:calc( - 10%)}</style> <div class="jupyter-child-ext"> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/MarkusThill.github.io-jupyter/2025_09_26_mahalanobis.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> </div>]]></content><author><name></name></author><category term="programming"/><category term="math"/><category term="stats"/><category term="math"/><summary type="html"><![CDATA[A hands-on Jupyter Notebook implementation of the Mahalanobis distance in Python. Covers theory, multiple implementations (NumPy, JAX, TensorFlow, SciPy), benchmarking on low- and high-dimensional data, visualizations, and its connection to the Chi-square distribution for anomaly detection.]]></summary></entry><entry><title type="html">Building Intelligent Agents for Connect-4: Tree Search Algorithms</title><link href="https://markusthill.github.io/blog/2025/connect-4-tree-search-algorithms/" rel="alternate" type="text/html" title="Building Intelligent Agents for Connect-4: Tree Search Algorithms"/><published>2025-09-24T08:00:51+00:00</published><updated>2025-09-24T08:00:51+00:00</updated><id>https://markusthill.github.io/blog/2025/connect-4-tree-search-algorithms</id><content type="html" xml:base="https://markusthill.github.io/blog/2025/connect-4-tree-search-algorithms/"><![CDATA[<p>This post is the 2nd part of a series of 7 articles:</p> <ol> <li><a href="/blog/2025/connect-4-introduction-and-tree-search-algorithms/">Building Intelligent Agents for Connect-4: First Steps</a></li> <li><strong><a href="/blog/2025/connect-4-tree-search-algorithms/">Building Intelligent Agents for Connect-4: Tree Search Algorithms</a></strong></li> <li><a href="#">Building Intelligent Agents for Connect-4: Board Representations</a></li> <li><a href="#">Building Intelligent Agents for Connect-4: Move Ordering</a></li> <li><a href="#">Building Intelligent Agents for Connect-4: Transposition Tables</a></li> <li><a href="#">Building Intelligent Agents for Connect-4: Opening Databases</a></li> <li><a href="#">Building Intelligent Agents for Connect-4: Final Considerations</a></li> </ol> <p>To play Connect-4 perfectly, an agent must be capable of fully exploring the entire game tree‚Äîwhich, in the most extreme case, can span all 42 moves of the game. However, the search effort grows exponentially with depth, making this a significant computational challenge and requiring considerable development effort to achieve optimal play.</p> <p>This post (and the ones that follow) will introduce some of the techniques and strategies that make such an agent possible. In this article, we‚Äôll focus on one of the fundamental methods for efficient game tree exploration: <strong>alpha-beta search</strong>.</p> <h2 id="the-alpha-beta-search">The Alpha-Beta Search</h2> <p>The <strong>Alpha-Beta search</strong> is an optimized version of the classic Minimax algorithm. While a simple Minimax search evaluates <em>every</em> node in the game tree, Alpha-Beta introduces two bounds‚Äî<strong>alpha</strong> and <strong>beta</strong>‚Äîto prune (cut off) large parts of the tree that cannot influence the final decision. The effectiveness of this pruning depends on factors such as the quality of the move ordering.</p> <p>The core idea is straightforward: because each player alternates between maximizing and minimizing outcomes, certain branches can be skipped if a better alternative is already known. Two key variables guide this process:</p> <ul> <li><strong>Alpha</strong> ‚Äì the best score the <em>maximizing</em> player can guarantee so far (a lower bound).</li> <li><strong>Beta</strong> ‚Äì the best score the <em>minimizing</em> player can guarantee so far (an upper bound).</li> </ul> <p>As the search progresses recursively through the tree, alpha and beta are updated and compared at each node. When the algorithm detects that further exploration cannot improve the current result, it prunes that branch. This pruning can occur at any level, not just near the root.</p> <p>From the maximizing player‚Äôs perspective, the procedure for processing a node is as follows:</p> <ol> <li>Generate all legal moves for the current state.</li> <li>Evaluate these moves in sequence.</li> <li>If any move returns a value <strong>greater than or equal to beta</strong>, the search stops at that node and returns beta (a <strong>fail-high</strong>). This indicates that the minimizing opponent will never allow this position to occur.</li> <li>If none of the moves improves alpha, the node returns alpha as a bound (<strong>fail-low</strong>), signaling that this position is not good enough to pursue.</li> <li>Only values between alpha and beta are considered <strong>exact</strong> evaluations; values outside this range are treated as bounds.</li> </ol> <p>As chess programmer <a href="http://web.archive.org/web/20040512194831/brucemo.com/compchess/programming/glossary.htm">Bruce Moreland</a> explains succinctly:</p> <blockquote> <p><em>‚ÄúA fail-high indicates that the search found something that was ‚Äòtoo good.‚Äô The opponent can avoid this position, so there‚Äôs no need to explore further. A fail-low indicates that the position was not good enough‚Äîwe have a better alternative, and will not choose the move that leads here.‚Äù</em></p> </blockquote> <p>You can find a simplified pseudo-code implementation below, and the full implementation is available as <code class="language-plaintext highlighter-rouge">AlphaBetaAgent</code> in the appendix. Some of these elements will be discussed in greater depth in upcoming posts.</p> <figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="kt">double</span> <span class="nf">max</span><span class="p">(</span><span class="kt">double</span> <span class="n">alpha</span><span class="p">,</span> <span class="kt">double</span> <span class="n">beta</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">hasWin</span><span class="p">(</span><span class="n">playerMax</span><span class="p">))</span>
    <span class="k">return</span> <span class="o">+</span><span class="mi">1</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">isDraw</span><span class="p">())</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">bookEntryAvailable</span><span class="p">())</span> <span class="k">return</span> <span class="n">bookValue</span><span class="p">();</span> <span class="c1">// Opening Book</span>
  <span class="k">if</span><span class="p">(</span><span class="n">hasTransTableEntry</span><span class="p">(</span><span class="n">board</span><span class="p">,</span> <span class="n">mirroredBoard</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">transTableEntryValue</span><span class="p">(</span><span class="n">board</span><span class="p">,</span> <span class="n">mirroredBoard</span><span class="p">)</span>
  <span class="n">generateMoves</span><span class="p">(</span><span class="n">max</span><span class="p">);</span>
  <span class="n">sortMoves</span><span class="p">(</span><span class="n">max</span><span class="p">);</span>
  <span class="k">if</span><span class="p">(</span><span class="n">isSymmetricPosition</span><span class="p">())</span>
    <span class="n">removeSymMoves</span><span class="p">();</span>
  <span class="k">while</span><span class="p">(</span><span class="n">hasMovesLeft</span><span class="p">(</span><span class="n">max</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">makeMove</span><span class="p">(</span><span class="n">max</span><span class="p">);</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">);</span>
    <span class="n">takeMoveBack</span><span class="p">(</span><span class="n">max</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">value</span> <span class="o">&gt;=</span> <span class="n">beta</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">putEntryInTranspositionTable</span><span class="p">();</span>
      <span class="k">return</span> <span class="n">beta</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span><span class="p">(</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">alpha</span><span class="p">)</span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">putEntryInTranspositionTable</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">alpha</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">double</span> <span class="nf">min</span><span class="p">(</span><span class="kt">double</span> <span class="n">alpha</span><span class="p">,</span> <span class="kt">double</span> <span class="n">beta</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span><span class="p">(</span><span class="n">hasWin</span><span class="p">(</span><span class="n">min</span><span class="p">))</span> <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">enhancedTranspositionCutoff</span><span class="p">(</span><span class="n">board</span><span class="p">,</span> <span class="n">mirroredBoard</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ETCValue</span><span class="p">;</span>
  <span class="n">generateMoves</span><span class="p">(</span><span class="n">min</span><span class="p">);</span>
  <span class="n">sortMoves</span><span class="p">(</span><span class="n">min</span><span class="p">);</span>
  <span class="k">if</span><span class="p">(</span><span class="n">isSymmetricPosition</span><span class="p">())</span>
    <span class="n">removeSymMoves</span><span class="p">();</span>
  <span class="k">while</span><span class="p">(</span><span class="n">hasMovesLeft</span><span class="p">(</span><span class="n">min</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">makeMove</span><span class="p">(</span><span class="n">min</span><span class="p">);</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">);</span>
    <span class="n">takeMoveBack</span><span class="p">(</span><span class="n">min</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">value</span> <span class="o">&lt;=</span> <span class="n">alpha</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">alpha</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">value</span> <span class="o">&lt;</span> <span class="n">beta</span><span class="p">)</span>
      <span class="n">beta</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
  <span class="p">}</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h3 id="the-negamax-variant">The Negamax Variant</h3> <p>In our implementation, we use the <strong>Negamax</strong> variant of the Minimax algorithm. Negamax is mathematically equivalent to Minimax but simplifies the code by taking advantage of the symmetry between maximizing and minimizing players. Instead of maintaining two separate evaluation branches‚Äîone for the maximizing player and one for the minimizing player‚ÄîNegamax represents both using a single recursive function. The idea is based on the relation:</p> \[\min(a, b) = -\max(-a, -b)\] <p>At each step, the algorithm evaluates a move by calling itself recursively with inverted scores and a flipped sign, effectively switching the roles of the players. This reduces code complexity and makes implementing features like alpha-beta pruning more straightforward.</p> <p>Negamax is widely used in game AI development for games like chess, checkers, and Connect-4 because it keeps the logic compact while maintaining the same performance and search results as the standard Minimax algorithm.</p> <p>The corresponding code for our negamax implementation can be found on <a href="https://github.com/MarkusThill/BitBully/blob/9c8993b185662d800568494dd4983a7aa2bb7fb8/src/BitBully.h#L100">GitHub</a>.</p> <p><br/></p> <h2 id="additional-resources-and-source-code">Additional Resources and Source Code</h2> <p>More recently, I developed a high-performance C++ and Python version of a Connect-4 solver called <strong>BitBully</strong>, available on <a href="https://github.com/MarkusThill/BitBully">GitHub</a> and <a href="https://pypi.org/project/bitbully/">PyPI</a>. Also, check out the related <a href="https://markusthill.github.io/projects/0_bitbully/">project page</a>.</p> <p>For those interested in a more educational or research-oriented setup, an earlier Java-based framework for <em>Connect-4</em> is also available on GitHub: <a href="http://github.com/MarkusThill/Connect-Four">http://github.com/MarkusThill/Connect-Four</a>.</p>]]></content><author><name></name></author><category term="Programming"/><category term="Connect-4"/><category term="AI"/><category term="tree-search"/><category term="alpha-beta"/><category term="minimax"/><category term="transposition tables"/><category term="opening databases"/><category term="move ordering"/><category term="bitboards"/><summary type="html"><![CDATA[Learn how the Alpha-Beta search algorithm optimizes Minimax for Connect-4 by pruning unnecessary branches, improving efficiency, and enabling stronger gameplay strategies.]]></summary></entry></feed>