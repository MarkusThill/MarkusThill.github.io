<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> BitBully | Markus Thill </title> <meta name="author" content="Markus Thill"> <meta name="description" content="One of the fastest and perfect-playing Connect-4 solvers around"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://markusthill.github.io/projects/0_bitbully/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Markus</span> Thill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">BitBully</h1> <p class="post-description">One of the fastest and perfect-playing Connect-4 solvers around</p> </header> <article> <p><strong>Links</strong><br> 🔗 <a href="https://github.com/MarkusThill/BitBully" rel="external nofollow noopener" target="_blank">GitHub</a> · <a href="https://pypi.org/project/bitbully" rel="external nofollow noopener" target="_blank">PyPI</a> · <a href="https://markusthill.github.io/BitBully" rel="external nofollow noopener" target="_blank">Docs</a></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_bitbully/c4-1-480.webp 480w,/assets/img/project_bitbully/c4-1-800.webp 800w,/assets/img/project_bitbully/c4-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_bitbully/c4-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Connect4 (1)" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_bitbully/c4-2-480.webp 480w,/assets/img/project_bitbully/c4-2-800.webp 800w,/assets/img/project_bitbully/c4-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_bitbully/c4-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Connect4 (2)" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_bitbully/c4-3-480.webp 480w,/assets/img/project_bitbully/c4-3-800.webp 800w,/assets/img/project_bitbully/c4-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_bitbully/c4-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Connect4 (3)" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>From Opening to Victory:</b> The image shows three key stages of a Connect 4 match — an early board with initial placements, a mid-game filled with tension and strategy, and the final state where yellow wins by connecting four discs. Connect 4 is a two-player game where discs are dropped into columns, aiming to form a straight line of four. It blends tactical planning with defensive play, and despite its simplicity, it’s a classic example of solvable strategy games in AI. </div> <p><strong>BitBully</strong> is a high-performance, perfect-playing Connect-4 solver and analysis engine written in C++ with Python bindings. It’s designed for both developers and researchers who want to explore game-theoretic strategies or integrate a strong Connect-4 AI into their own projects.</p> <hr> <h2 id="-key-features">🚀 Key Features</h2> <ul> <li> <strong>Blazing Fast Solving</strong>: Uses MTD(f) and null-window search algorithms.</li> <li> <strong>Bitboard Engine</strong>: Board states are handled efficiently via low-level bitwise operations.</li> <li> <strong>Advanced Heuristics</strong>: Threat detection, move ordering, and transposition tables.</li> <li> <strong>Opening Databases</strong>: Covers all positions with up to 12 tokens, annotated with exact win/loss distances.</li> <li> <strong>Cross-Platform</strong>: Compatible with Linux, Windows, and macOS.</li> <li> <strong>Python API</strong>: Seamlessly integrates into Python projects via <code class="language-plaintext highlighter-rouge">bitbully_core</code> (powered by <code class="language-plaintext highlighter-rouge">pybind11</code>).</li> <li> <strong>Open Source</strong>: Available under the AGPL-3.0 license.</li> </ul> <hr> <h2 id="-installation">📦 Installation</h2> <p>Install the latest stable release from PyPI:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>bitbully
</code></pre></div></div> <p>No compilation needed—pre-built wheels included!</p> <hr> <h2 id="-example-usage-python">🧠 Example Usage (Python)</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bitbully</span> <span class="kn">import</span> <span class="n">bitbully_core</span> <span class="k">as</span> <span class="n">bbc</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">board</span> <span class="o">=</span> <span class="n">bbc</span><span class="p">.</span><span class="nc">Board</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">board</span><span class="p">.</span><span class="nf">playMove</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">board</span><span class="p">)</span>

<span class="n">solver</span> <span class="o">=</span> <span class="n">bbc</span><span class="p">.</span><span class="nc">BitBully</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">solver</span><span class="p">.</span><span class="nf">mtdf</span><span class="p">(</span><span class="n">board</span><span class="p">,</span> <span class="n">first_guess</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Solved in </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">s → Score: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>You can also solve boards defined as NumPy arrays, use opening books, and generate random game states. For more examples check out the <a href="https://markusthill.github.io/BitBully" rel="external nofollow noopener" target="_blank">docs</a> or the <a href="https://github.com/MarkusThill/BitBully" rel="external nofollow noopener" target="_blank">GitHub repository</a>·</p> <hr> <h2 id="-license">📜 License</h2> <p>AGPL-3.0. <a href="https://github.com/MarkusThill/BitBully/tree/master?tab=AGPL-3.0-1-ov-file#readme" rel="external nofollow noopener" target="_blank">View License</a></p> <hr> <h2 id="-acknowledgments">🙏 Acknowledgments</h2> <p>Inspired by the solvers of <a href="https://github.com/PascalPons/connect4" rel="external nofollow noopener" target="_blank">Pascal Pons</a> and <a href="https://tromp.github.io/c4/Connect4.java" rel="external nofollow noopener" target="_blank">John Tromp</a>.</p> <hr> <h2 id="related-work">Related Work</h2> <h3 id="literature">Literature</h3> <p>The application of machine learning to board games remains an active and challenging research area, particularly due to the complexity and strategic depth of games like Chess, Go, and Connect Four. Unlike humans who can intuitively recognize patterns, artificial agents require structured learning approaches, often supported by carefully engineered features or representations.</p> <p>A milestone in this field was <strong>Tesauro’s TD-Gammon</strong>, which demonstrated that self-play combined with temporal difference learning (TDL) could lead to expert-level performance in backgammon. Inspired by this success, many studies attempted to apply TDL to other board games, but the outcomes were often mixed due to higher complexity and lack of domain knowledge <a class="citation" href="#Thill12">(Thill, 2012; Thill et al., 2012)</a>.</p> <p>Prior work on <strong>Connect Four</strong> showed that learning strong strategies through self-play alone is feasible, but only with a <strong>very rich feature representation</strong> and a large number of training games. One such approach used <strong>N-tuple systems</strong> in combination with TDL to approximate value functions. These systems produced high-quality agents capable of defeating even perfect-play opponents, all without incorporating handcrafted game-theoretic knowledge. The success was largely attributed to the expressiveness of the N-tuple representation and extensive training with millions of games <a class="citation" href="#Thill12">(Thill, 2012; Thill et al., 2012)</a>.</p> <p>Subsequent research introduced <strong>eligibility traces</strong>—including standard, resetting, and replacing variants—into these systems. Eligibility traces enhanced temporal credit assignment and significantly <strong>accelerated learning (by a factor of two)</strong> while improving asymptotic playing strength <a class="citation" href="#Thill2015a">(Thill, 2015)</a>.</p> <p>To further improve training efficiency, recent studies investigated <strong>online-adaptable learning rate algorithms</strong>, such as <strong>Incremental Delta-Bar-Delta (IDBD)</strong> and <strong>Temporal Coherence Learning (TCL)</strong>. A novel variant using <strong>geometric step-size adaptation</strong> outperformed conventional methods, reducing the number of required training games by <strong>up to 75%</strong> in some cases. The most effective algorithms proved to be those that combined geometric learning rates with nonlinear value functions and eligibility traces. These methods brought the total training requirement for learning Connect Four <strong>down to just over 100,000 games</strong>, a <strong>13× improvement</strong> over earlier baselines <a class="citation" href="#bagh16b">(Bagheri et al., 2016)</a>.</p> <p>Finally, preliminary experiments also applied this enhanced learning framework to other strategic games, such as <strong>Dots-and-Boxes</strong>, showing the framework’s potential for broader generalization, though some unique domain-specific challenges were identified <a class="citation" href="#Thil14">(Thill et al., 2014)</a>.</p> <h3 id="connect-4-game-playing-framework-c4gpf"><a href="https://github.com/MarkusThill/Connect-Four" rel="external nofollow noopener" target="_blank">Connect-4 Game Playing Framework (C4GPF)</a></h3> <p>The <strong>C4GPF</strong> is a Java-based framework for training, evaluating, and interacting with Connect Four agents. It features a GUI and supports various agent types, including:</p> <ul> <li> <strong>Perfect-play Minimax agent</strong> with database and transposition table support.</li> <li> <strong>Reinforcement Learning agent</strong> using n-tuple systems and TD-learning with eligibility traces.</li> <li> <strong>Monte Carlo Tree Search (MCTS)</strong> agent.</li> <li> <strong>RL-Minimax hybrid</strong>, combining tree search with learned state evaluations.</li> </ul> <p>Key capabilities include:</p> <ul> <li>Animated or step-by-step agent matches.</li> <li>Benchmarking and head-to-head competitions.</li> <li>Visualization and editing of n-tuple lookup tables.</li> <li>Support for adaptive step-size algorithms (e.g., IDBD, TCL, AutoStep).</li> </ul> <p>The framework is extensible and designed for research and teaching.</p> <p><strong>🔗 Repository:</strong> <a href="https://github.com/MarkusThill/Connect-Four" rel="external nofollow noopener" target="_blank">Connect-4 Game Playing Framework (C4GPF)</a></p> <h3 id="general-board-game-framework-gbg"><a href="https://github.com/WolfgangKonen/GBG" rel="external nofollow noopener" target="_blank">General Board Game Framework (GBG)</a></h3> <p><strong>GBG</strong> is a flexible Java-based framework for <strong>general board game (GBG)</strong> learning and playing. Designed for research and education, it allows users to implement new board games or AI agents once and run them across all supported components.</p> <p>Key features:</p> <ul> <li>Supports <strong>1-player, 2-player, and n-player</strong> board games.</li> <li>Comes with a variety of <strong>built-in AI agents</strong>, including reinforcement learning and tree-based strategies.</li> <li>Standardized <strong>interfaces and abstract classes</strong> make it easy to plug in new games or agents.</li> <li>Enables fair <strong>competitions and benchmarking</strong> between agents across multiple games.</li> <li>Suitable for both classroom use and research projects.</li> </ul> <p>The framework includes documentation, a GUI, and a <a href="http://www.gm.fh-koeln.de/ciopwebpub/Konen22a.d/TR-GBG.pdf" rel="external nofollow noopener" target="_blank">technical report</a> explaining its architecture.</p> <p><strong>🔗 Repository:</strong> <a href="https://github.com/WolfgangKonen/GBG" rel="external nofollow noopener" target="_blank">General Board Game Framework (GBG)</a></p> <p><br></p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE Trans. Games</abbr> </div> <div id="bagh16b" class="col-sm-8"> <div class="title">Online Adaptable Learning Rates for the Game Connect-4</div> <div class="author"> Samineh Bagheri, Markus Thill, Patrick Koch, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Wolfgang Konen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Computational Intelligence and AI in Games</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=Bu5eFxQAAAAJ&amp;citation_for_view=Bu5eFxQAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Learning board games by self-play has a long tradition in computational intelligence for games. Based on Tesauro’s seminal success with TD-Gammon in 1994, many successful agents use temporal difference learning today. But in order to be successful with temporal difference learning on game tasks, often a careful selection of features and a large number of training games is necessary. Even for board games of moderate complexity like Connect-4, we found in previous work that a very rich initial feature set and several millions of game plays are required. In this work we investigate different approaches of online-adaptable learning rates like Incremental Delta Bar Delta (IDBD) or temporal coherence learning (TCL) whether they have the potential to speed up learning for such a complex task. We propose a new variant of TCL with geometric step size changes. We compare those algorithms with several other state-of-the-art learning rate adaptation algorithms and perform a case study on the sensitivity with respect to their meta parameters. We show that in this set of learning algorithms those with geometric step size changes outperform those other algorithms with constant step size changes. Algorithms with nonlinear output functions are slightly better than linear ones. Algorithms with geometric step size changes learn faster by a factor of 4 as compared to previously published results on the task Connect-4.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Masters thesis</abbr> </div> <div id="Thill2015a" class="col-sm-8"> <div class="title">Temporal Difference Learning Methods with Automatic Step-Size Adaption for Strategic Board Games: Connect-4 and Dots-and-Boxes</div> <div class="author"> Markus Thill </div> <div class="periodical"> <em>TH Köln – University of Applied Sciences</em>, 2015 </div> <div class="periodical"> Master thesis, Festo award 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://www.gm.fh-koeln.de/%7Ekonen/research/PaperPDF/MT-Thill2015-final.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/MT-Thill2015-final.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Machine learning tasks for board games which rely solely on self-play methods remain rather challenging up till today. The perhaps most impressive breakthrough in this field was achieved by Tesauro’s TD-Gammon, which was able to learn the game backgammon at expert level with a self-play variant of the temporal difference learning (TDL) algorithm. Since then, many studies attempted to replicate some of TD-Gammon’s success by applying TDL to other board games, however, mostly with mixed results. We found in our earlier work on the board game Connect-4 that a rich feature set is required to successfully learn a near-perfect strategy. Nonetheless, several millions of self-play training games were necessary in order to generate strong Connect-4 agents. In this thesis we will mainly focus on two topics, namely online-adaptable learning rate methods and eligibility traces, and investigate whether these approaches have the potential to speed up learning. For the Connect-4 learning task we show that algorithms with geometric step-size changes have the best performance, in some cases reducing the required number of training games to learn the game by more than 40%. In a case study, we compare several state-of-the-art step-size adaptation algorithms with respect to their sensitivity towards certain meta parameters. In the further course of this thesis, we investigate the benefits of different eligibility trace variants. Additionally, we extend several learning rate algorithms to eligibility traces and examine their performance. We could observe that eligibility traces improve the speed of learning by a factor of two for our Connect-4 task. Overall, with several additional enhancements, we could reduce the number of training games to learn Connect-4 to slightly more than 100 000, which is an improvement by a factor of 13, compared to previously published results. In the last sections of this work, we apply the learning framework that we developed for Connect-4 – with several adjustments – to the strategic board game Dots-and-Boxes and discuss the main problems that we observed for our initial experiments. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIG</abbr> </div> <div id="Thil14" class="col-sm-8"> <div class="title">Temporal Difference Learning with Eligibility Traces for the Game Connect-4</div> <div class="author"> Markus Thill, Samineh Bagheri, Patrick Koch, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Wolfgang Konen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In CIG’2014, International Conference on Computational Intelligence in Games, Dortmund</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=Bu5eFxQAAAAJ&amp;citation_for_view=Bu5eFxQAAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Systems that learn to play board games are often trained by self-play on the basis of temporal difference (TD) learning. Successful examples include Tesauro’s well known TD-Gammon and Lucas’ Othello agent. For other board games of moderate complexity like Connect Four, we found in previous work that a successful system requires a very rich initial feature set with more than half a million of weights and several millions of training games. In this work we study the benefits of eligibility traces added to this system. To the best of our knowledge, eligibility traces have not been used before for such a large system. Different versions of eligibility traces (standard, resetting, and replacing traces) are compared. We show that eligibility traces speed up the learning by a factor of two and that they increase the asymptotic playing strength.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Bachelor thesis</abbr> </div> <div id="Thill12" class="col-sm-8"> <div class="title">Reinforcement Learning mit N-Tupel-Systemen für Vier Gewinnt</div> <div class="author"> Markus Thill </div> <div class="periodical"> <em>TH Köln – University of Applied Sciences</em>, 2012 </div> <div class="periodical"> Bachelor thesis, 1st prize in Opitz award 2013, Festo award 2012, Ferchau award 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://www.gm.fh-koeln.de/ciopwebpub/Theses.d/Thill12.d/BA-Thill-2012.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/BA-Thill-2012.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Die Untersuchung maschineller Lernverfahren für Brettspiele stellt auch heute noch ein sehr interessantes Forschungsgebiet dar. Dies liegt vor allem daran, dass das Erlernen komplexer Spiele wie dem Schach- oder Go-Spiel nach wie vor als sehr anspruchsvoll gilt. Während Menschen in der Lage sind, gewisse Zusammenhänge bzw. Gesetzmäßigkeiten in Spielen zu erkennen und daraus die richtigen Rückschlüsse zu ziehen, ist dies für ein Computerprogramm deutlich schwieriger. Aus diesem Grund müssen die Entwickler häufig viel spieltheoretisches Wissen in das Programm einbringen, damit der Lernprozess überhaupt fähig ist, auf die besonderen spielspezifischen Merkmale zu achten. In dieser Arbeit wird die Anwendung von sogenannten N-Tupel-Systemen – in Kombination mit einer Reinforcement-Learning-Trainingsumgebung – auf das Spiel Vier Gewinnt untersucht. N-Tupel-Systeme dienen dazu, lineare Nutzenfunktionen von Agenten zu approximieren, sodass Stellungsbewertungen vorgenommen werden können. Um diese Funktionen zu erlernen, werden die N-Tupel-Systeme mithilfe des Temporal Difference Learnings (TDL), einem Algorithmus zur Lösung von RL- Problemen, trainiert. Das Training der Agenten erfolgte ausschließlich durch Self-Play, während des Trainings kam daher kein Lehrer oder spieltheoretisches Wissen irgendeiner Form zum Einsatz. Dennoch gelang es, Agenten mit hoher Spielstärke zu trainieren, die in vielen Fällen einen perfekten Spieler schlagen konnten. Insbesondere die N-Tupel-Systeme, die eine sehr große Zahl an Features generieren und die passenden selektieren, tragen zu den außerordentlich guten Ergebnissen bei.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PPSN</abbr> </div> <div id="Thil12" class="col-sm-8"> <div class="title">Reinforcement learning with n-tuples on the game Connect-4</div> <div class="author"> Markus Thill, Patrick Koch, and Wolfgang Konen </div> <div class="periodical"> <em>In PPSN’2012: 12th International Conference on Parallel Problem Solving From Nature, Taormina</em>, 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=Bu5eFxQAAAAJ&amp;citation_for_view=Bu5eFxQAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Learning complex game functions is still a difficult task. We apply temporal difference learning (TDL), a well-known variant of the reinforcement learning approach, in combination with n-tuple networks to the game Connect-4. Our agent is trained just by self-play. It is able, for the first time, to consistently beat the optimal-playing Minimax agent (in game situations where a win is possible). The n-tuple network induces a mighty feature space: It is not necessary to design certain features, but the agent learns to select the right ones. We believe that the n-tuple network is an important ingredient for the overall success and identify several aspects that are relevant for achieving high-quality results. The architecture is sufficiently general to be applied to similar reinforcement learning tasks as well.</p> </div> </div> </div> </li> </ol> </div> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'MarkusThill/MarkusThill.github.io',
        'data-repo-id': 'R_kgDOLnMaxQ',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOLnMaxc4CeUxG',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Markus Thill. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a> and from <a href="https://www.freepik.com/" rel="external nofollow noopener" target="_blank">Freepik</a>. Last updated: July 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?6bab9ec621eb188fdd3221e1f3861398"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>