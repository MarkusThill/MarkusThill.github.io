<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution | Markus Thill </title> <meta name="author" content="Markus Thill"> <meta name="description" content="This post explores why the squared Mahalanobis distance of Gaussian data follows a Chi-square distribution. We cover the theory step by step, show empirical evidence, and explain how this relationship provides a principled way to set anomaly detection thresholds using quantiles.A companion Jupyter Notebook with code, benchmarks, and visualizations is provided to put the theory into practice."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://markusthill.github.io/blog/2025/mahalanobis-distance/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Markus</span> Thill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution</h1> <p class="post-meta"> Created on September 25, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a> ¬† ¬∑ ¬† <a href="/blog/category/programming"> <i class="fa-solid fa-tag fa-sm"></i> programming</a> ¬† <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a> ¬† <a href="/blog/category/stats"> <i class="fa-solid fa-tag fa-sm"></i> stats</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#prerequisites">Prerequisites</a> <ul> <li class="toc-entry toc-h2"><a href="#matrix-algebra">Matrix Algebra</a></li> <li class="toc-entry toc-h2"><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li> <li class="toc-entry toc-h2"><a href="#linear-affine-transform-of-a-normally-distributed-random-variable">Linear Affine Transform of a Normally Distributed Random Variable</a></li> <li class="toc-entry toc-h2"> <a href="#quantile-estimation-for-multivariate-gaussian-distributions">Quantile Estimation for Multivariate Gaussian Distributions</a> <ul> <li class="toc-entry toc-h3"><a href="#empirical-evidence-that-the-mahalanobis-distance-is-chi-square-distributed">Empirical Evidence that the Mahalanobis Distance is Chi-Square Distributed</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#the-squared-mahalanobis-distance-follows-a-chi-square-distribution">The Squared Mahalanobis Distance follows a Chi-Square Distribution</a> <ul> <li class="toc-entry toc-h2"><a href="#derivation-based-on-the-eigenvalue-decomposition">Derivation Based on the Eigenvalue Decomposition</a></li> <li class="toc-entry toc-h2"><a href="#alternative-derivation-based-on-the-whitening-property-of-the-mahalanobis-distance">Alternative Derivation Based on the Whitening Property of the Mahalanobis Distance</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>\( \renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}} \def\matr#1{\boldsymbol{\mathbf{#1}}} \def\tp{\mathsf T} \DeclareMathOperator{\E}{\mathbb{E}} \)</p> <p>Gaussian distributions are a common choice for anomaly detection, especially when the data is roughly normally distributed. The parameters of the distribution can be estimated using maximum likelihood estimation (MLE), which gives the sample mean and covariance matrix. Once these parameters are known, the next step is to decide on a threshold that separates normal data from anomalies. A simple approach is to set this threshold based on the probability density function (PDF): if a new data point has a PDF value below the threshold, it is flagged as anomalous.</p> <p>In one dimension, this threshold separates the tails of the distribution from its center. In two dimensions, the boundary takes the shape of an ellipse, and in higher dimensions, it becomes an ellipsoid. All points on such a boundary are equally distant from the mean in terms of the Mahalanobis distance, which makes this distance a useful alternative for defining thresholds.</p> <p>Unlike the PDF, the Mahalanobis distance does not rely on assuming a Gaussian distribution. Still, if the data is approximately Gaussian, the squared Mahalanobis distance follows a Chi-square distribution‚Äîa relationship that can also be confirmed visually using a quantile‚Äìquantile plot.</p> <h1 id="prerequisites">Prerequisites</h1> <h2 id="matrix-algebra">Matrix Algebra</h2> <p>The product of an $n \times \ell$ matrix $\matr A$ and an $\ell \times p$ matrix $\matr B$ is defined entry-wise as</p> \[(\matr A \matr B)_{ij} = \sum_{k=1}^\ell \matr A_{ik}\,\matr B_{kj}.\] <p>In particular, the product of a matrix $\matr A$ with its transpose $\matr A^\top$ can be written as</p> \[\begin{align} (\matr A \matr A^\top)_{ij} &amp;= \sum_{k=1}^\ell \matr A_{ik}\,\matr A^\top_{kj} = \sum_{k=1}^\ell \matr A_{ik}\,\matr A_{jk}, \nonumber \\[6pt] \matr A \matr A^\top &amp;= \sum_{k=1}^\ell \vec a_{k}\,\vec a_{k}^\top, \label{eq:matrixProductWithTranspose} \end{align}\] <p>where $\vec a_{k}$ denotes the $k$-th column vector of $\matr A$.</p> <p>We will also use the following simple relation for vectors $\vec a, \vec b$:</p> \[\begin{align} x &amp;= \vec a^\top \vec b, \\ y &amp;= \vec b^\top \vec a = x^\top = x, \\ xy &amp;= \vec a^\top \vec b \,\vec b^\top \vec a = x^2 = (\vec a^\top \vec b)^2. \label{eq:multOfTwoScalars} \end{align}\] <p><em>(Inverse of a Matrix Product)</em></p> <p>For invertible square matrices $ \matr A \in \mathbb R^{n \times n} $ and $ \matr B \in \mathbb R^{n \times n} $, the inverse of their product is</p> \[\begin{align} (\matr A \matr B)^{-1} &amp;= \matr B^{-1} \matr A^{-1}. \label{eq:inverseProduct} \end{align}\] <p>Indeed,</p> \[\begin{align} (\matr A \matr B)(\matr B^{-1} \matr A^{-1}) &amp;= \matr A(\matr B \matr B^{-1}) \matr A^{-1} \\[4pt] &amp;= \matr A \mathbf I \matr A^{-1} \\[4pt] &amp;= \matr A \matr A^{-1} \\[4pt] &amp;= \mathbf I, \end{align}\] <p>which verifies the result.</p> <p>Note that the order of the factors is crucial: in general $ \matr B^{-1} \matr A^{-1} \ne \matr A^{-1} \matr B^{-1} $, since matrix multiplication is not commutative.</p> <p><br></p> <h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2> <p>For a square $n \times n$ matrix $\matr A$, a non-zero vector $\vec u$ is called an eigenvector of $\matr A$ if it satisfies</p> \[\matr A \vec u = \lambda \vec u,\] <p>where the scalar $\lambda$ is the corresponding eigenvalue.</p> <p>If the eigenvectors of $\matr A$ are collected as the columns of a matrix $\matr U \in \mathbb R^{n \times n}$, with the $i$-th column given by $\vec u^{(i)}$, and if $\matr \Lambda$ is the diagonal matrix containing the associated eigenvalues $\lambda_i$, then</p> \[\begin{align} \matr A \matr U &amp;= \matr U \matr \Lambda, \nonumber \\[6pt] \matr A &amp;= \matr U \matr \Lambda \matr U^{-1}, \label{eq:eigendecomp} \end{align}\] <p>which is known as the <strong>eigenvalue decomposition</strong> of $\matr A$.</p> <p>If $\matr A$ is symmetric, its eigenvectors are orthogonal (and can be chosen to be orthonormal). In that case, $\matr U$ is an orthogonal matrix, so $\matr U^{-1} = \matr U^\top$, and the decomposition simplifies to</p> \[\matr A = \matr U \matr \Lambda \matr U^\top.\] <p>The square root of a matrix $\matr A$ (denoted $\matr A^{\tfrac{1}{2}}$), defined such that $\matr A^{\tfrac{1}{2}} \matr A^{\tfrac{1}{2}} = \matr A$, can be expressed using the eigenvalue decomposition as:</p> \[\begin{align} \matr A^{\tfrac{1}{2}} &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp}, \label{eq:sqrtSymMatrix} \end{align}\] <p>where $\matr \Lambda^{\tfrac{1}{2}}$ is the diagonal matrix containing the square roots of the eigenvalues of $\matr A$.</p> <p>Verifying this:</p> \[\begin{align*} \matr A^{\tfrac{1}{2}} \cdot \matr A^{\tfrac{1}{2}} &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \matr U \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \\ &amp;= \matr U \matr \Lambda^{\tfrac{1}{2}} \matr I \matr \Lambda^{\tfrac{1}{2}} \matr U^{\tp} \\ &amp;= \matr U \matr \Lambda \matr U^{\tp} \\ &amp;= \matr A. \end{align*}\] <p>Similarly, the inverse of $\matr A$ can be expressed through its eigenvalue decomposition. Using the associativity of matrix multiplication, we obtain:</p> \[\begin{align} \matr A^{-1} &amp;= \big( \matr U \matr \Lambda \matr U^{-1} \big)^{-1} \\ &amp;= \big( \matr U^{-1} \big)^{-1} \matr \Lambda^{-1} \matr U^{-1} \\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1} \nonumber \\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{\tp}, \label{eq:eigenvalueInverse} \end{align}\] <p>where $\matr \Lambda^{-1}$ is a diagonal matrix containing the reciprocals of the eigenvalues of $\matr A$.</p> <p>Note that $\matr \Lambda^{-1}$ is again a diagonal matrix containing the reciprocal eigenvalues of $\matr A$.</p> <p><br></p> <h2 id="linear-affine-transform-of-a-normally-distributed-random-variable">Linear Affine Transform of a Normally Distributed Random Variable</h2> <p>Consider a random variable $X \sim \mathcal N(\vec \mu_x, \matr \Sigma_x)$ with mean vector $\vec \mu_x$ and covariance matrix $\matr \Sigma_x$.<br> Applying a linear affine transformation with matrix $\matr A$ and vector $\vec b$ yields a new random variable $Y$:</p> \[\begin{align} Y = \matr A X + \vec b. \end{align}\] <p>The mean $\vec \mu_y$ and covariance $\matr \Sigma_y$ of $Y$ can be derived as follows:</p> \[\begin{align} \vec \mu_y &amp;= \E \{ Y \} \\ &amp;= \E \{ \matr A X + \vec b \} \\ &amp;= \matr A \E \{ X \} + \vec b \\ &amp;= \matr A \vec \mu_x + \vec b, \label{eq:AffineLinearTransformMean} \end{align}\] <p>and</p> \[\begin{align} \matr \Sigma_y &amp;= \E \{ (Y - \vec \mu_y)(Y - \vec \mu_y)^\tp \} \\ &amp;= \E \{ \big[ \matr A (X - \vec \mu_x) \big] \big[ \matr A (X - \vec \mu_x) \big]^\tp \} \\ &amp;= \E \{ \matr A (X - \vec \mu_x)(X - \vec \mu_x)^\tp \matr A^\tp \} \\ &amp;= \matr A \E \{ (X - \vec \mu_x)(X - \vec \mu_x)^\tp \} \matr A^\tp \\ &amp;= \matr A \matr \Sigma_x \matr A^\tp. \label{eq:AffineLinearTransformCovariance} \end{align}\] <p>Thus, affine transformations preserve Gaussianity: the result is still normally distributed, but with a mean shifted and scaled by $\matr A$ and $\vec b$, and a covariance matrix transformed as $\matr A \matr \Sigma_x \matr A^\tp$.</p> <h2 id="quantile-estimation-for-multivariate-gaussian-distributions">Quantile Estimation for Multivariate Gaussian Distributions</h2> <p>Estimating quantiles for multivariate Gaussian distributions is more involved than in the one-dimensional case. In one dimension, quantiles can be obtained by directly evaluating the cumulative distribution function in the tails of the distribution. In higher dimensions, however, this approach does not generalize in a straightforward way.</p> <p>In the bivariate case, quantiles can be visualized as ellipses, and in higher dimensions as ellipsoids. A useful tool for describing such contours is the <strong>Mahalanobis distance</strong>, which measures the distance of a point from the mean while accounting for the covariance structure of the distribution. All points at the same Mahalanobis distance from the mean lie on the surface of an ellipsoid.</p> <p>More formally, the usual definition of a $p$-quantile involves a random variable: the $p$-quantile of a distribution is the value $q_p$ such that the probability of the random variable being less than or equal to $q_p$ is exactly $p$. For a multivariate Gaussian distribution, we can treat the <strong>squared Mahalanobis distance</strong> between a random point $\vec x$ and the mean $\vec \mu$ as such a random variable:</p> \[d^2 = (\vec x - \vec \mu)^\tp \matr \Sigma^{-1} (\vec x - \vec \mu).\] <p>The $p$-quantile then corresponds to the threshold value $q_p$ such that</p> \[P(d^2 \leq q_p) = p.\] <p>Geometrically, the set of all points with $d^2 \leq q_p$ forms an ellipsoid centered at the mean.</p> <p>A naive way to compute these quantiles is through a <strong>Monte Carlo approach</strong>: sample many points from the multivariate Gaussian distribution, compute their Mahalanobis distances, and then estimate the quantile empirically. While straightforward, this method becomes computationally inefficient, especially if quantiles need to be evaluated repeatedly.</p> <p>Fortunately, there is a more direct connection: the squared Mahalanobis distance of a Gaussian-distributed random vector follows a <strong>Chi-square distribution</strong> with degrees of freedom equal to the dimensionality $k$ of the Gaussian. This means that the $p$-quantile can be computed directly as</p> \[q_p = \chi^2_{k,p},\] <p>where $\chi^2_{k,p}$ is the $p$-quantile of the Chi-square distribution with $k$ degrees of freedom. In other words, for a $k$-dimensional Gaussian, the region</p> \[\{ \vec x \in \mathbb R^k : d^2 \leq \chi^2_{k,p} \}\] <p>defines the ellipsoidal contour that contains probability mass $p$.</p> <p><br></p> <h3 id="empirical-evidence-that-the-mahalanobis-distance-is-chi-square-distributed">Empirical Evidence that the Mahalanobis Distance is Chi-Square Distributed</h3> <p>The relationship between the Mahalanobis distance and the Chi-square distribution can also be verified empirically.<br> A common tool for this is the <strong>Quantile-Quantile (Q-Q) plot</strong>, which compares the quantiles of two distributions.<br> If the squared Mahalanobis distance of samples drawn from a Gaussian distribution truly follows a Chi-square distribution, then the Q-Q plot of their quantiles should lie approximately on the identity line.</p> <p>The following R script demonstrates this approach by generating a sample from a multivariate Gaussian distribution, computing the squared Mahalanobis distances, and comparing them against the theoretical Chi-square quantiles:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">Matrix</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">DIM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="n">nSample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="w">

</span><span class="n">Posdef</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">ev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
  </span><span class="n">decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
  </span><span class="n">Q</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.Q</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">R</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.R</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="w">
  </span><span class="n">ph</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w">
  </span><span class="n">O</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ph</span><span class="p">)</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">O</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ev</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">O</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Posdef</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">
</span><span class="n">muhat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">


</span><span class="n">sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mvrnorm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">nSample</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">muhat</span><span class="p">,</span><span class="w"> </span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sigma</span><span class="p">)</span><span class="w">
</span><span class="n">C</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">.5</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">det</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="nb">pi</span><span class="o">*</span><span class="n">Sigma</span><span class="p">))</span><span class="w">
</span><span class="n">mahaDist2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mahalanobis</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="o">=</span><span class="n">muhat</span><span class="p">,</span><span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span><span class="w">

</span><span class="c1">#</span><span class="w">
</span><span class="c1"># Interestingly, the Mahalanobis distance of samples follows a Chi-Square distribution</span><span class="w">
</span><span class="c1"># with d degrees of freedom</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="n">pps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">100+1</span><span class="p">)</span><span class="w">
</span><span class="n">qq1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="n">quantile</span><span class="p">(</span><span class="n">mahaDist2</span><span class="p">,</span><span class="w"> </span><span class="n">probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">})</span><span class="w">
</span><span class="n">qq2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">qchisq</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">ncol</span><span class="p">(</span><span class="n">Sigma</span><span class="p">))</span><span class="w">

</span><span class="n">dat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">qEmp</span><span class="o">=</span><span class="w"> </span><span class="n">qq1</span><span class="p">,</span><span class="w"> </span><span class="n">qChiSq</span><span class="o">=</span><span class="n">qq2</span><span class="p">)</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dat</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">qEmp</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">qChiSq</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Sample quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Chi-Squared Quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></figure> <p><br></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-480.webp 480w,/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-800.webp 800w,/assets/img/2025-09-25-mahalanobis-distance/q-q-plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-09-25-mahalanobis-distance/q-q-plot.png" class="img-fluid rounded z-depth-1 imgcenter" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <b>Figure 1:</b> Quantile‚ÄìQuantile (Q‚ÄìQ) plot comparing the squared Mahalanobis distances of a Gaussian sample with the theoretical Chi-square distribution. The close alignment of the points along the diagonal line indicates that the squared Mahalanobis distance is well-approximated by a Chi-square distribution. </figcaption> </figure> <p><br></p> <h1 id="the-squared-mahalanobis-distance-follows-a-chi-square-distribution">The Squared Mahalanobis Distance follows a Chi-Square Distribution</h1> <p>In this section we prove the conjecture: ‚ÄúThe squared Mahalanobis distance of a Gaussian-distributed random vector $\matr X$ and the center $\vec\mu$ of this Gaussian distribution follows a Chi-square distribution.‚Äù</p> <p><br></p> <h2 id="derivation-based-on-the-eigenvalue-decomposition">Derivation Based on the Eigenvalue Decomposition</h2> <p>The Mahalanobis distance between two points $\vec x$ and $\vec y$ is defined as</p> \[\begin{align} d(\vec x,\vec y) = \sqrt{(\vec x -\vec y )^\tp \matr \Sigma^{-1} (\vec x - \vec y)}. \end{align}\] <p>Thus, the squared Mahalanobis distance of a random vector $\matr X$ and the center $\vec \mu$ of a multivariate Gaussian distribution is defined as:</p> \[\begin{align} D = d(\matr X,\vec \mu)^2 = (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ), \label{eq:sqMahalanobis} \end{align}\] <p>where $\matr \Sigma$ is a $\ell \times \ell$ covariance matrix and $\vec \mu \in \mathbb{R}^\ell$ is the mean vector. In order to achieve a different representation of $D$ one can first perform an eigenvalue decomposition on $\matr \Sigma^{-1}$ which is (with Eq. $\eqref{eq:eigenvalueInverse}$ and assuming orthonormal eigenvectors):</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1} = \matr U \matr \Lambda^{-1} \matr U^{T} \end{align}\] <p>With Eq. \eqref{eq:matrixProductWithTranspose} we obtain:</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \label{eq:SigmaInverseAsSum} \end{align}\] <p>where $\vec u_{k}$ is the $k$-th eigenvector of the corresponding eigenvalue $\lambda_k$. Plugging \eqref{eq:SigmaInverseAsSum} back into \eqref{eq:sqMahalanobis} results in:</p> \[\begin{align*} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) = (\matr X -\vec \mu )^\tp \Bigg( \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \Bigg) (\matr X - \vec \mu ) \\ &amp;= \sum_{k=1}^\ell \lambda_k^{-1} (\matr X -\vec \mu )^\tp \vec u_{k} \vec u_{k}^\tp (\matr X - \vec \mu )\\ &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \Big[ \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2 = \sum_{k=1}^\ell \Big[ \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2\\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align*}\] <p>where $Y_k$ is a new random variable based on an affine linear transform of the random vector $\matr X$. According to Eq. \eqref{eq:AffineLinearTransformMean} , we have $\matr Z = (\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)$. If we set $ \vec a_{k}^\tp = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp$ then we get $Y_k = \vec a_{k}^\tp \matr Z = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \matr Z$. Note that $Y_k$ is now a random variable drawn from a univariate normal distribution $Y_k \thicksim N(0,\sigma_k^2)$, where, according to \eqref{eq:AffineLinearTransformCovariance}:</p> \[\begin{align} \sigma_k^2 &amp;= \vec a_{k}^\tp \Sigma \vec a_{k}= \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \Sigma \lambda_k^{-\frac{1}{2}} \vec u_{k} \\ &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} \label{eq:smallSigma} \end{align}\] <p>If we insert $\matr \Sigma = \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp$ into Eq. \eqref{eq:smallSigma}, we get:</p> \[\begin{align*} \sigma_k^2 &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} = \lambda_k^{-1} \vec u_{k}^\tp \Bigg( \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp \Bigg) \vec u_{k} = \sum_{j=1}^\ell \lambda_k^{-1} \vec u_{k}^\tp \lambda_j \vec u_{j} \vec u_{j}^\tp \vec u_{k} \\ &amp;= \sum_{j=1}^\ell \lambda_k^{-1} \lambda_j \vec u_{k}^\tp \vec u_{j} \vec u_{j}^\tp \vec u_{k} \end{align*}\] <p>Since all eigenvectors $\vec u_{i}$ are pairwise orthonormal the dotted products $\vec u_{k}^\tp \vec u_{j}$ and $\vec u_{j}^\tp \vec u_{k}$ will be zero for $j \neq k$. Only for the case $j = k$ we get:</p> \[\begin{align*} \sigma_k^2 &amp;= \lambda_k^{-1} \lambda_k \vec u_{k}^\tp \vec u_{k} \vec u_{k}^\tp \vec u_{k} = \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 = \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 \\ &amp;= 1, \end{align*}\] <p>since the the norm $||\vec u_{k}||$ of a orthonormal eigenvector is equal to 1. Thus, the squared Mahalanobis distance can be expressed as:</p> \[\begin{align*} D = \sum_{k=1}^\ell Y_k^2, \end{align*}\] <p>where</p> \[\begin{align} Y_k \thicksim N(0,1). \end{align}\] <p>Now the Chi-square distribution with $\ell$ degrees of freedom is exactly defined as being the distribution of a variable which is the sum of the squares of $\ell$ random variables being standard normally distributed. Hence, $D$ is Chi-square distributed with $\ell$ degrees of freedom.</p> <p><br></p> <h2 id="alternative-derivation-based-on-the-whitening-property-of-the-mahalanobis-distance">Alternative Derivation Based on the Whitening Property of the Mahalanobis Distance</h2> <p>Since the inverse $\matr \Sigma^{-1}$ of the covariance matrix $\matr \Sigma$ is also a symmetric matrix, its square root can be found ‚Äì based on Eq. \eqref{eq:sqrtSymMatrix} ‚Äì to be a symmetric matrix. In this case we can write the squared Mahalanobis distance as</p> \[\begin{align*} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) = (\matr X -\vec \mu )^\tp \matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu )\\ &amp;= \Big( \matr \Sigma^{-\frac{1}{2}} (\matr X -\vec \mu ) \Big)^\tp \Big(\matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu ) \Big) = \matr Y^\tp \matr Y = ||\matr Y||^2 \\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align*}\] <p>The multiplication $\matr Y = \matr W \matr Z$, with $\matr W=\matr \Sigma^{-\frac{1}{2}}$ and $\matr Z= \matr X -\vec \mu $ is typically referred to as a whitening transform, where in this case $\matr W=\matr \Sigma^{-\frac{1}{2}}$ is the so called Mahalanobis (or ZCA) whitening matrix. $\matr Y$ has zero mean, since $(\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)$. Due to the (linear) whitening transform the new covariance matrix $\matr \Sigma_y$ is the identity matrix $\matr I$, as shown in the following (using the property in Eq. \eqref{eq:AffineLinearTransformCovariance}):</p> \[\begin{align*} \matr \Sigma_y &amp;= \matr W \matr \Sigma \matr W^\tp = \matr \Sigma^{-\frac{1}{2}} \matr \Sigma \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp = \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp \\ &amp;= \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \matr \Sigma^{-\frac{1}{2}} = \Big(\matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{\frac{1}{2}} \Big) \Big(\matr \Sigma^{\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}}\Big)\\ &amp;= \matr I. \end{align*}\] <p>Hence, all elements $Y_k$ in the random vector $\matr Y$ are random variables drawn from independent normal distributions $Y_k \thicksim N(0,1)$, which leads us to the same conclusion as before, that $D$ is Chi-square distributed with $\ell$ degrees of freedom.</p> <p><br></p> <h1 id="conclusion">Conclusion</h1> <p>We have shown that the squared Mahalanobis distance of a Gaussian random vector follows a Chi-square distribution with degrees of freedom equal to the dimension of the data.<br> This result has a very practical consequence: it allows us to replace heuristic or Monte Carlo‚Äìbased thresholding with an exact statistical criterion.</p> <p>In anomaly detection, instead of deciding ‚Äúby hand‚Äù where to cut off the probability density, we can set thresholds directly using the quantiles of the Chi-square distribution.<br> For example, in $\ell$ dimensions, the 95% confidence region of a Gaussian distribution is exactly the ellipsoid</p> \[\{ \vec x \in \mathbb R^\ell : d^2 \leq \chi^2_{\ell,0.95} \}.\] <p>This means we can flag data points outside this ellipsoid as anomalies with a well-defined false alarm rate.</p> <p>More generally, the Mahalanobis distance provides a natural way to measure how unusual a point is relative to a multivariate distribution. Combined with the Chi-square connection, it gives both a geometric intuition (ellipsoids of equal distance) and a rigorous statistical tool for multivariate analysis.</p> <p>üëâ A more detailed exploration of the Mahalanobis distance ‚Äî including multiple Python implementations, benchmarks, and visualizations ‚Äî is covered in a separate blog post <strong><a href="/blog/2025/mahalanobis-distance-implementations/">here</a></strong>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/neural-nets-and-backprop/">Backpropagation from Scratch: Feed-Forward Neural Networks in Matrix Notation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/online-batch-estimate-cov-mu/">Online and Batch-Incremental Estimation of Covariance Matrices and Means in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/">Notes on the Runtime Complexity of Latin Hypercube Sampling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mahalanobis-distance-implementations/">Implementing the Mahalanobis Distance in Python</a> </li> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Markus Thill. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a> and from <a href="https://www.freepik.com/" rel="external nofollow noopener" target="_blank">Freepik</a>. Last updated: December 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b608866baffe761c7f8f7670a3310d0f"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>