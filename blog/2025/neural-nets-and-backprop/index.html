<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation from Scratch: Feed-Forward Neural Networks in Matrix Notation | Markus Thill </title> <meta name="author" content="Markus Thill"> <meta name="description" content="A notation-first walkthrough of feed-forward neural networks and vectorized backpropagation, focusing on how the math translates directly into clean, correct implementations. Covers forward pass, backprop, batching, and practical training considerations."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://markusthill.github.io/blog/2025/neural-nets-and-backprop/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Markus</span> Thill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Backpropagation from Scratch: Feed-Forward Neural Networks in Matrix Notation</h1> <p class="post-meta"> Created on December 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>   <a href="/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> backpropagation</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/matrix-notation"> <i class="fa-solid fa-hashtag fa-sm"></i> matrix-notation</a>   <a href="/blog/tag/from-scratch"> <i class="fa-solid fa-hashtag fa-sm"></i> from-scratch</a>   ·   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/neural-nets"> <i class="fa-solid fa-tag fa-sm"></i> neural nets</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h1"><a href="#notation-guide">Notation Guide</a></li> <li class="toc-entry toc-h1"><a href="#classical-feed-forward-neural-network-architecture">Classical Feed-Forward Neural Network Architecture</a></li> <li class="toc-entry toc-h1"> <a href="#the-feed-forward-pass">The Feed-Forward Pass</a> <ul> <li class="toc-entry toc-h2"><a href="#the-activation-of-a-neuron">The Activation of a Neuron</a></li> <li class="toc-entry toc-h2"> <a href="#the-activation-of-a-layer-of-neurons">The Activation of a Layer of Neurons</a> <ul> <li class="toc-entry toc-h3"><a href="#unpacking-the-batch-forward-pass-column-wise-view">Unpacking the Batch Forward Pass (Column-wise View)</a></li> <li class="toc-entry toc-h3"><a href="#computing-the-activation-of-the-first-hidden-layer">Computing the Activation of the first Hidden Layer</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#summary-forward-pass-through-the-network">Summary: Forward Pass Through the Network</a> <ul> <li class="toc-entry toc-h3"><a href="#inputs-and-parameters">Inputs and Parameters</a></li> <li class="toc-entry toc-h3"><a href="#forward-pass">Forward Pass</a></li> <li class="toc-entry toc-h3"><a href="#remarks">Remarks</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#the-backpropagation-algorithm">The Backpropagation Algorithm</a> <ul> <li class="toc-entry toc-h2"><a href="#cost-function-and-gradient-descent">Cost Function and Gradient Descent</a></li> <li class="toc-entry toc-h2"><a href="#interlude-the-chain-rule-and-nested-dependencies">Interlude: The Chain Rule and Nested Dependencies</a></li> <li class="toc-entry toc-h2"> <a href="#backpropagation-via-the-chain-rule">Backpropagation via the Chain Rule</a> <ul> <li class="toc-entry toc-h3"><a href="#gradient-with-respect-to-the-weights-vectorized-form">Gradient with respect to the weights (vectorized form)</a></li> <li class="toc-entry toc-h3"><a href="#gradient-with-respect-to-the-biases-vectorized-form">Gradient with respect to the biases (vectorized form)</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#batch-backpropagation-recursion">Batch Backpropagation Recursion</a> <ul> <li class="toc-entry toc-h3"><a href="#batch-gradients-for-weights-and-biases">Batch gradients for weights and biases</a></li> <li class="toc-entry toc-h3"><a href="#weight-gradient-as-a-sum-of-outer-products">Weight gradient as a sum of outer products</a></li> <li class="toc-entry toc-h3"><a href="#bias-gradient-as-a-sum-of-error-signals">Bias Gradient as a Sum of Error Signals</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#gradient-descent-parameter-updates">Gradient Descent Parameter Updates</a></li> <li class="toc-entry toc-h2"> <a href="#initialization-of-backpropagation-at-the-output-layer">Initialization of Backpropagation at the Output Layer</a> <ul> <li class="toc-entry toc-h3"><a href="#output-layer-error-signal-single-training-example">Output-layer Error Signal (single training example)</a></li> <li class="toc-entry toc-h3"><a href="#vectorized-form-single-training-example">Vectorized Form (single training example)</a></li> <li class="toc-entry toc-h3"><a href="#vectorized-batch-formulation">Vectorized Batch Formulation</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#summary-backpropagation">Summary: Backpropagation</a> <ul> <li class="toc-entry toc-h3"><a href="#inputs-and-stored-quantities">Inputs and Stored Quantities</a></li> <li class="toc-entry toc-h3"><a href="#backward-pass-error-signals">Backward Pass: Error Signals</a></li> <li class="toc-entry toc-h3"><a href="#gradients-for-weights-and-biases-batch">Gradients for Weights and Biases (Batch)</a></li> <li class="toc-entry toc-h3"><a href="#parameter-update-gradient-descent">Parameter Update (Gradient Descent)</a></li> <li class="toc-entry toc-h3"><a href="#remarks-1">Remarks</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#putting-everything-together-full-batch-vectorized">Putting Everything Together (Full-Batch, Vectorized)</a> <ul> <li class="toc-entry toc-h2"> <a href="#practical-considerations">Practical Considerations</a> <ul> <li class="toc-entry toc-h3"><a href="#gradient-checking-finite-differences">Gradient Checking (Finite Differences)</a></li> <li class="toc-entry toc-h3"><a href="#batching-strategies">Batching Strategies</a></li> <li class="toc-entry toc-h3"><a href="#weight-initialization">Weight Initialization</a></li> <li class="toc-entry toc-h3"><a href="#learning-rate-step-size">Learning Rate (Step Size)</a></li> <li class="toc-entry toc-h3"><a href="#input-normalization">Input Normalization</a></li> <li class="toc-entry toc-h3"><a href="#vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</a></li> <li class="toc-entry toc-h3"><a href="#convergence-issues">Convergence Issues</a></li> <li class="toc-entry toc-h3"><a href="#training-validation-and-test-data">Training, Validation, and Test Data</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#further-reading">Further Reading</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#accompanying-jupyter-notebook-code-companion">Accompanying Jupyter Notebook (Code Companion)</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>\( \renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}} \def\matr#1{\boldsymbol{\mathbf{#1}}} \def\Wmatr{\matr{W}} \newcommand{\din}{\mathord{d_0}} \newcommand{\batch}{\mathord{N}} \newcommand{\yhat}{\vec{\hat{y}}} \)</p> <h1 id="introduction">Introduction</h1> <p>Neural networks have been explained a thousand times — so why another intro?</p> <p>Because for many people (my past self included) the hard part isn’t “deep math”, it’s getting the <strong>nuts and bolts</strong> of training straight: what exactly happens in the forward pass, where the gradients come from, and how <em>backpropagation</em> is really just an efficient, structured application of the chain rule that makes gradient descent practical.</p> <p>The basics are also the best entry point: once you understand a plain fully connected feed-forward network end to end, the same ideas carry over to CNNs, recurrent nets, and even transformers. In my experience, neural networks are not conceptually difficult - the main obstacle is notation. At the end of the day it’s often <strong>“notation, notation, notation”</strong> (and much less multivariate calculus than it first appears).</p> <p>That’s why this post takes a deliberately “from scratch” approach: we build a mathematical formulation step by step (single weight → vectors/matrices → batches), derive backpropagation in a way that matches an implementation, and then implement a small network accordingly. Along the way we also cover practical details that matter in real code, such as weight initialization and gradient checking, to make sure training works and the derivations actually hold up in practice.</p> <hr> <h1 id="notation-guide">Notation Guide</h1> <p>The following table summarizes all symbols and notation used throughout this post. Vectors are written in <strong>bold lowercase</strong>, matrices in <strong>bold uppercase</strong>, and scalars in regular font. Unless stated otherwise, vectors are column vectors.</p> <table> <thead> <tr> <th>Symbol</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$L$</td> <td>scalar</td> <td>Number of parameterized (trainable) layers in the network</td> </tr> <tr> <td>$\ell$</td> <td>index</td> <td>Layer index, $\ell = 0,1,\ldots,L$</td> </tr> <tr> <td>$\din$</td> <td>scalar</td> <td>Input dimension of the network</td> </tr> <tr> <td>$d_L$</td> <td>scalar</td> <td>Output dimension of the network</td> </tr> <tr> <td>$d_\ell$</td> <td>scalar</td> <td>Number of neurons in layer $\ell$</td> </tr> <tr> <td>$\batch$</td> <td>scalar</td> <td>Batch size (number of training examples processed in parallel)</td> </tr> <tr> <td>$\vec{x}$</td> <td>vector</td> <td>Single input vector $\vec{x} \in \mathbb{R}^{\din}$</td> </tr> <tr> <td>$\vec{x}_n$</td> <td>vector</td> <td>$n$-th input vector in a batch</td> </tr> <tr> <td>$\matr X$</td> <td>matrix</td> <td>Input batch matrix, $\matr X \in \mathbb{R}^{\din \times \batch}$</td> </tr> <tr> <td>$a_i^{(\ell)}$</td> <td>scalar</td> <td>Activation of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec a^{(\ell)}$</td> <td>vector</td> <td>Activation vector of layer $\ell$ for a single example</td> </tr> <tr> <td>$\vec a_n^{(\ell)}$</td> <td>vector</td> <td>Activation vector at layer $\ell$ for the $n$-th example</td> </tr> <tr> <td>$\matr A^{(\ell)}$</td> <td>matrix</td> <td>Activation matrix at layer $\ell$, $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$</td> </tr> <tr> <td>$z_i^{(\ell)}$</td> <td>scalar</td> <td>Pre-activation of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec z^{(\ell)}$</td> <td>vector</td> <td>Pre-activation vector of layer $\ell$ (single example)</td> </tr> <tr> <td>$\matr Z^{(\ell)}$</td> <td>matrix</td> <td>Pre-activation matrix of layer $\ell$ (batch)</td> </tr> <tr> <td>$w_{i,j}^{(\ell)}$</td> <td>scalar</td> <td>Weight connecting neuron $j$ in layer $\ell-1$ to neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec w^{(\ell)}_{i}$</td> <td>vector</td> <td>Incoming weight vector of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\Wmatr^{(\ell)}$</td> <td>matrix</td> <td>Weight matrix of layer $\ell$, $\Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}$</td> </tr> <tr> <td>$b_i^{(\ell)}$</td> <td>scalar</td> <td>Bias of neuron $i$ in layer $\ell$</td> </tr> <tr> <td>$\vec b^{(\ell)}$</td> <td>vector</td> <td>Bias vector of layer $\ell$, $\vec b^{(\ell)} \in \mathbb{R}^{d_\ell}$</td> </tr> <tr> <td>$\sigma(\cdot)$</td> <td>function</td> <td>Activation function (applied element-wise)</td> </tr> <tr> <td>$\sigma’(\cdot)$</td> <td>function</td> <td>Derivative of the activation function</td> </tr> <tr> <td>$\hat{\vec y}$</td> <td>vector</td> <td>Network output for a single input</td> </tr> <tr> <td>$\hat{\vec y}_n$</td> <td>vector</td> <td>Predicted output for the $n$-th training example</td> </tr> <tr> <td>$\matr{\hat{Y}}$</td> <td>matrix</td> <td>Output matrix of the network for a batch</td> </tr> <tr> <td>$\vec y_n^*$</td> <td>vector</td> <td>True target/output for the $n$-th training example</td> </tr> <tr> <td>$\matr Y^*$</td> <td>matrix</td> <td>Target output matrix for a batch</td> </tr> <tr> <td>$\mathcal{L}_n$</td> <td>scalar</td> <td>Loss for the $n$-th training example</td> </tr> <tr> <td>$\mathcal{L}(\vec y^*, \hat{\vec y})$</td> <td>scalar</td> <td>Loss function for a single example</td> </tr> <tr> <td>$J(\Theta)$</td> <td>scalar</td> <td>Cost function (average loss over a batch)</td> </tr> <tr> <td>$\Theta$</td> <td>set</td> <td>Set of all trainable parameters of the network</td> </tr> <tr> <td>$\delta_i^{(\ell)}$</td> <td>scalar</td> <td>Error signal (loss derivative w.r.t. $z_i^{(\ell)}$)</td> </tr> <tr> <td>$\vec{\delta}^{(\ell)}$</td> <td>vector</td> <td>Error vector at layer $\ell$ (single example)</td> </tr> <tr> <td>$\matr\Delta^{(\ell)}$</td> <td>matrix</td> <td>Error matrix at layer $\ell$ for a batch</td> </tr> <tr> <td>$\otimes$</td> <td>operator</td> <td>Hadamard (element-wise) product</td> </tr> <tr> <td>$\vec 1$</td> <td>vector</td> <td>Vector of ones, $\vec 1 \in \mathbb{R}^{\batch}$</td> </tr> <tr> <td>$\eta$</td> <td>scalar</td> <td>Learning rate</td> </tr> <tr> <td>$\frac{\partial J}{\partial \Wmatr^{(\ell)}}$</td> <td>matrix</td> <td>Gradient of the cost w.r.t. weights of layer $\ell$</td> </tr> <tr> <td>$\frac{\partial J}{\partial \vec b^{(\ell)}}$</td> <td>vector</td> <td>Gradient of the cost w.r.t. biases of layer $\ell$</td> </tr> <tr> <td>$(\cdot)^\top$</td> <td>operator</td> <td>Matrix or vector transpose</td> </tr> </tbody> </table> <p><br> <strong>Conventions</strong></p> <ul> <li>All vectors are column vectors unless explicitly stated otherwise.</li> <li>Batch data is stacked <strong>column-wise</strong>.</li> <li>Activation functions and their derivatives are applied element-wise.</li> <li>Gradients are derived using the denominator-layout convention for matrix calculus.</li> </ul> <hr> <h1 id="classical-feed-forward-neural-network-architecture">Classical Feed-Forward Neural Network Architecture</h1> <p>Figure 1 illustrates the general structure of a typical fully connected feed-forward neural network. The network consists of $L$ parameterized feed-forward layers with trainable weights, starting with the first hidden layer at $\ell = 1$ and ending with the output layer at $\ell = L$.</p> <p>In total, the network comprises $L + 1$ layers, since the input is treated as an additional layer at $\ell = 0$. This input layer does not contain trainable parameters but serves to provide the input values to the network. The layer at $\ell = L$ is the output layer, while the layers with indices $1 \le \ell &lt; L$ are referred to as hidden layers. The input to the network is given by the vector $\vec{x} = (x_1, x_2, \ldots, x_{\din})^\top \in \mathbb{R}^{\din}$, where $\din$ denotes the input dimension. In the input layer, the components of $\vec{x}$ are identified with the activations $a_i^{(0)} = x_i$. Throughout this work, a superscript $(\ell)$ is used to indicate that a quantity is associated with layer $\ell$.</p> <p>Each layer $\ell$ consists of $d_\ell$ neurons, where $d_0 = \din$ corresponds to the input dimension and $d_L$ denotes the output dimension of the network. In Figure 1, neurons are depicted as circles with incoming arrows. A neuron (also referred to as a node or unit) is the fundamental computational element of a neural network. It receives inputs from the preceding layer, forms a weighted sum of these inputs, adds a bias term, and subsequently applies a nonlinear activation function to produce its output. The output of the $i$-th neuron in layer $\ell$ is typically referred to as the activation $a_i^{(\ell)}$.</p> <p>The neurons of adjacent layers are fully connected, meaning that each neuron in layer $\ell - 1$ is connected to every neuron in layer $\ell$. The weight $w^{(\ell)}_{i,j}$ denotes the trainable parameter connecting the $j$-th neuron of layer $\ell - 1$ to the $i$-th neuron of layer $\ell$. Bias terms are associated with each neuron in layers $\ell \ge 1$. For a given layer $\ell$, the bias term $b_i^{(\ell)}$ corresponds to the $i$-th neuron and is added to the weighted sum of its inputs before the activation function is applied.</p> <p>The activations of the output layer, $a_i^{(L)}$, constitute the network output and are commonly denoted by $\hat{y}_i$, as indicated in Figure 1.</p> <p><br></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig1-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig1-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig1.png" class="img-fluid rounded z-depth-1 imgcenter" width="95%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <b>Figure 1:</b> Typical structure of a feed-forward neural network. </figcaption> </figure> <h1 id="the-feed-forward-pass">The Feed-Forward Pass</h1> <h2 id="the-activation-of-a-neuron">The Activation of a Neuron</h2> <p>Figure 2 illustrates the internal structure of a neuron, including the weighted summation of its inputs, the addition of a bias term, and the subsequent application of a (nonlinear) activation function.</p> <p>The neuron receives as inputs the activations \(a^{(\ell-1)}_1, a^{(\ell-1)}_2, \ldots, a^{(\ell-1)}_{d_{\ell-1}}\) from all neurons in the preceding layer $\ell - 1$. Each input activation \(a^{(\ell-1)}_j\) is multiplied by a corresponding trainable weight $w^{(\ell)}_{i,j}$, where the index $i$ denotes the neuron in the current layer and $j$ denotes the neuron in the previous layer. These multiplications are indicated by the multiplication symbols ($\times$) in the diagram.</p> <p>The weighted inputs are then summed, as represented by the summation node ($\sum$). In addition, a bias term $b_i^{(\ell)}$, which is specific to the $i$-th neuron in layer $\ell$, is added to this sum. The resulting quantity,</p> \[z_i^{(\ell)} = \sum_{j=1}^{d_{\ell-1}} w^{(\ell)}_{i,j} \, a^{(\ell-1)}_j + b_i^{(\ell)},\] <p>is referred to as the pre-activation of the neuron.</p> <p>Finally, the pre-activation $z_i^{(\ell)}$ is passed through a nonlinear activation function $\sigma(\cdot)$, producing the output (or activation)</p> \[\begin{equation} a_i^{(\ell)} = \sigma\!\left(z_i^{(\ell)}\right) = \sigma \Bigg( \sum_{j=1}^{d_{\ell-1}} {w_{i,j}^{(\ell)} \cdot a_j^{(\ell-1)} } + b_i^{(\ell)} \Bigg) \label{eq:activation} \end{equation}\] <p>This activation constitutes the output of the neuron and serves as an input to the neurons in the subsequent layer $\ell + 1$.</p> <p><br></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig2-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig2-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig2.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <b>Figure 2:</b> Representation of a typical neuron in a neural net. </figcaption> </figure> <h2 id="the-activation-of-a-layer-of-neurons">The Activation of a Layer of Neurons</h2> <p>The figure thus highlights the three fundamental steps performed by a neuron during the forward pass: weighted summation of inputs, addition of a bias term, and application of a nonlinear activation function.</p> <p>In matrix form, Eq. \eqref{eq:activation} can be written as</p> \[\begin{equation} \vec a^{(\ell)} = \sigma\left(\vec z^{(\ell)}\right) = \sigma\left( \Wmatr^{(\ell)} \vec a^{(\ell-1)} + \vec b^{(\ell)} \right), \label{eq:activation3} \end{equation}\] <p>where all vectors are interpreted as column vectors:</p> \[\begin{align} \vec a^{(\ell)}, \vec z^{(\ell)}, \vec b^{(\ell)} &amp;\in \mathbb{R}^{d_\ell}, \\ \Wmatr^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times d_{\ell-1}}, \\ \vec a^{(\ell-1)} &amp;\in \mathbb{R}^{d_{\ell-1}}. \label{eq:activation4} \end{align}\] <p>The activation function $\sigma(\cdot)$ is applied element-wise to its vector argument.</p> <p>It is also possible to pass multiple input vectors through the network simultaneously by stacking them into a matrix. Let</p> \[\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_\batch\] <p>denote $\batch$ input examples, where $\batch$ is referred to as the batch size. These inputs are arranged column-wise into the input matrix</p> \[\begin{align} \matr X &amp;= \big( \vec{x}_1 \ \vec{x}_2 \ \ldots \ \vec{x}_\batch \big), \\ \matr X &amp;\in \mathbb{R}^{\din \times \batch}. \label{eq:input-matrix} \end{align}\] <p>This procedure is commonly referred to as batching. A matrix containing multiple input vectors is called a batch.</p> <blockquote> <p>In some implementations, input vectors are stacked row-wise, with each row representing one example. Here, we adopt the column-wise convention, which is consistent with the preceding vector notation and the matrix formulations used throughout.</p> </blockquote> <p>For each input example $\vec{x}_n$, a corresponding activation vector $\vec{a}_n^{(\ell)}$ is produced at layer $\ell$. These activations can be collected into the activation matrix</p> \[\matr A^{(\ell)} = \big( \vec a_1^{(\ell)} \ \vec a_2^{(\ell)} \ \ldots \ \vec a_\batch^{(\ell)} \big) \in \mathbb{R}^{d_\ell \times \batch}.\] <p>Using matrix notation, the forward pass for a batch of inputs can be written as</p> \[\begin{align} \matr Z^{(\ell)} &amp;= \begin{pmatrix} \vec b^{(\ell)} &amp; \Wmatr^{(\ell)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr A^{(\ell-1)} \end{pmatrix}, \\ &amp;= \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top \\ \matr A^{(\ell)} &amp;= \sigma\!\left( \matr Z^{(\ell)} \right), \label{eq:input-matrix1} \end{align}\] <p>where $\vec{1} \in \mathbb{R}^{\batch}$ denotes a vector of ones.</p> <p>Note that the inclusion of the bias vector into the weight matrix is a purely notational convenience. From a mathematical perspective, the bias term can equivalently be added as a separate vector after the weighted sum has been computed. By augmenting the input with a constant component equal to one and concatenating the bias vector to the weight matrix, the affine transformation can be written as a single matrix multiplication. Throughout this post, this augmented representation is used where it simplifies the notation, but it does not constitute an additional modeling assumption.</p> <hr> <h3 id="unpacking-the-batch-forward-pass-column-wise-view">Unpacking the Batch Forward Pass (Column-wise View)</h3> <p>To see explicitly how the augmented matrix multiplication works, we expand Eq. \eqref{eq:input-matrix1} using block-matrix multiplication:</p> \[\matr A^{(\ell-1)} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec a^{(\ell-1)}_{1} &amp; \vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} \in \mathbb{R}^{d_{\ell-1}\times \batch}.\] <p>Then the matrix multiplication expands as</p> \[\Wmatr^{(\ell)}\matr A^{(\ell-1)} = \Wmatr^{(\ell)} \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec a^{(\ell-1)}_{1} &amp; \vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} &amp; \cdots &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>To add the bias to every column, we replicate it across the batch:</p> \[\vec b^{(\ell)}\vec 1^\top = \vec b^{(\ell)} \begin{pmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \end{pmatrix} = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \vec b^{(\ell)} &amp; \vec b^{(\ell)} &amp; \cdots &amp; \vec b^{(\ell)} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>Hence the pre-activation matrix becomes</p> \[\matr Z^{(\ell)} = \Wmatr^{(\ell)}\matr A^{(\ell-1)} + \vec b^{(\ell)}\vec 1^\top = \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} + \vec b^{(\ell)} &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} + \vec b^{(\ell)} &amp; \cdots &amp; \Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} + \vec b^{(\ell)} \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix}.\] <p>Applying $\sigma(\cdot)$ element-wise yields the activation matrix</p> \[\begin{align*} \matr A^{(\ell)} = \sigma\!\left(\matr Z^{(\ell)}\right) &amp;= \begin{pmatrix} \vert &amp; \vert &amp; &amp; \vert \\ \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{1} + \vec b^{(\ell)}\big) &amp; \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{2} + \vec b^{(\ell)}\big) &amp; \cdots &amp; \sigma\!\big(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{\batch} + \vec b^{(\ell)}\big) \\ \vert &amp; \vert &amp; &amp; \vert \end{pmatrix} \\[12pt] &amp;= \big( \vec a_1^{(\ell)} \ \vec a_2^{(\ell)} \ \ldots \ \vec a_\batch^{(\ell)} \big) \end{align*}\] <p>So each column is exactly the single-example relation we already wrote:</p> \[\vec a^{(\ell)}_{n} = \sigma\!\left(\vec z^{(\ell)}_{n}\right) = \sigma\!\left(\Wmatr^{(\ell)}\vec a^{(\ell-1)}_{n} + \vec b^{(\ell)}\right), \qquad n=1,\ldots,\batch.\] <hr> <h3 id="computing-the-activation-of-the-first-hidden-layer">Computing the Activation of the first Hidden Layer</h3> <p>Using the augmented representation and $\matr A^{(0)}= \matr X$, the affine transformation of the first hidden layer can be written as</p> \[\begin{equation} \matr A^{(1)} = \sigma\!\left( \begin{pmatrix} \vec b^{(1)} &amp; \Wmatr^{(1)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr X \end{pmatrix} \right). \label{eq:input-matrix2} \end{equation}\] <p>Here, the augmented weight matrix</p> \[\begin{pmatrix} \vec b^{(1)} &amp; \Wmatr^{(1)} \end{pmatrix} \in \mathbb{R}^{d_1 \times (\din + 1)}\] <p>is formed by concatenating the bias vector $\vec b^{(1)} \in \mathbb{R}^{d_1}$ and the weight matrix $\Wmatr^{(1)} \in \mathbb{R}^{d_1 \times \din}$, while the augmented input matrix</p> <p>\(\begin{pmatrix} \vec{1}^\top \\ \matr X \end{pmatrix} \in \mathbb{R}^{(\din + 1) \times \batch}\) contains a row of ones and the input batch $\matr X \in \mathbb{R}^{\din \times \batch}$. As a result, the matrix product is well defined and yields</p> \[\matr A^{(1)} \in \mathbb{R}^{d_1 \times \batch}.\] <hr> <h2 id="summary-forward-pass-through-the-network">Summary: Forward Pass Through the Network</h2> <p>We summarize the steps required to compute a forward pass for a batch of inputs through a fully connected feed-forward neural network. This provides a compact, algorithmic view of the computations introduced so far and serves as a reference for later gradient derivations.</p> <h3 id="inputs-and-parameters">Inputs and Parameters</h3> <ul> <li> <strong>Network architecture</strong> <ul> <li>Number of layers: $L$</li> <li>Layer sizes: $d_0=\din, d_1, \ldots, d_L$</li> </ul> </li> <li> <strong>Trainable parameters (for each layer $\ell = 1,\ldots,L$)</strong> <ul> <li>Weight matrix: $ \Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}} $</li> <li>Bias vector: $ \vec b^{(\ell)} \in \mathbb{R}^{d_\ell} $</li> </ul> </li> <li> <strong>Activation function</strong> <ul> <li>Nonlinear function $\sigma(\cdot)$, applied element-wise</li> </ul> </li> <li> <strong>Batch input</strong> <ul> <li>$\batch$ input vectors stacked column-wise: \(\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}\)</li> </ul> </li> </ul> <hr> <h3 id="forward-pass">Forward Pass</h3> <ol> <li> <p><strong>Initialization</strong> \(\matr A^{(0)} := \matr X\)</p> </li> <li> <strong>Layer-wise propagation</strong><br> For each layer $\ell = 1,2,\ldots,L$: <ul> <li> <p>Pre-activations: \(\matr Z^{(\ell)} = \begin{pmatrix} \vec b^{(\ell)} &amp; \Wmatr^{(\ell)} \end{pmatrix} \begin{pmatrix} \vec{1}^\top \\ \matr A^{(\ell-1)} \end{pmatrix}, \qquad \matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch},\)</p> <p>where $\vec 1 \in \mathbb{R}^{\batch}$ denotes a vector of ones.</p> <p>Equivalently: $\matr Z^{(\ell)} = \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top$.</p> </li> <li> <p>Activations: \(\matr A^{(\ell)} = \sigma\!\left(\matr Z^{(\ell)}\right), \qquad \matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}\)</p> </li> </ul> </li> <li> <p><strong>Network output</strong> \(\matr A^{(L)} = \big( \hat{\vec y}_1 \ \hat{\vec y}_2 \ \ldots \ \hat{\vec y}_\batch \big) ,\qquad \matr A^{(L)} \in \mathbb{R}^{d_L \times \batch},\)</p> <p>where $\hat{\vec y}_n \in \mathbb{R}^{d_L}$ denotes the output of the network for the $n$-th input example.</p> </li> </ol> <p>Each column of $\matr A^{(\ell)}$ corresponds to the activations produced by one training example, and the same weight matrices and bias vectors are shared across all columns.</p> <hr> <h3 id="remarks">Remarks</h3> <ul> <li>The batch forward pass is mathematically equivalent to performing $\batch$ independent forward passes in parallel.</li> <li>Writing the computation in matrix form enables efficient implementations using linear algebra routines.</li> <li>All intermediate matrices \(\{\matr A^{(\ell)}, \matr Z^{(\ell)}\}_{\ell=1}^L\) must be retained, as they are required later during backpropagation.</li> </ul> <p>This completes the forward-pass specification for batched inputs.</p> <hr> <h1 id="the-backpropagation-algorithm">The Backpropagation Algorithm</h1> <h2 id="cost-function-and-gradient-descent">Cost Function and Gradient Descent</h2> <p>To train the neural network, a suitable differentiable cost (sometimes objective) function $J(\Theta)$ is required, where $\Theta$ denotes the set of all trainable parameters of the network, including both the weight matrices and the bias vectors of all layers. Explicitly, we write</p> \[\Theta = \left\{ \Wmatr^{(1)}, \vec b^{(1)}, \Wmatr^{(2)}, \vec b^{(2)}, \ldots, \Wmatr^{(L)}, \vec b^{(L)} \right\}.\] <p>We denote the loss for a single ($n$-th) training example by $\mathcal{L}(\vec y_n^*, \hat{\vec y_n})$. A commonly used choice is the mean squared error (MSE) loss, which measures the prediction error for a single training example. For an individual example $n$, the loss is defined as</p> \[\mathcal{L}_n = \frac{1}{2}\left\| \vec{y}_n^* - \hat{\vec y}_n \right\|^2,\] <p>where $\vec y_n^* \in \mathbb{R}^{d_L}$ denotes the true output vector of the $n$-th example and $\hat{\vec y}_n = \vec a_n^{(L)} \in \mathbb{R}^{d_L}$ is the corresponding predicted output of the network.</p> <blockquote> <p><strong>Note.</strong><br> The factor $\tfrac{1}{2}$ is included purely for convenience. When differentiating the squared error, it cancels the factor $2$ arising from the derivative of the square and thus simplifies the resulting gradient expressions. Importantly, this factor does not affect the location of the minimum of the cost function.</p> </blockquote> <p>The cost function $J(\Theta)$ is obtained by aggregating the loss over a batch of $\batch$ training examples. Using the mean squared error, the cost function is given by</p> \[J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n = \frac{1}{\batch} \sum_{n=1}^{\batch} \frac{1}{2}\left\| \vec{y}_n^* - \hat{\vec y}_n \right\|^2. \label{eq:mse}\] <p>In Figure 3, the local structure of two consecutive layers in a feed-forward neural network is illustrated in a form that is particularly useful for deriving the backpropagation algorithm. The figure highlights a single neuron $i$ in layer $\ell$, its pre-activation $z_i^{(\ell)}$, and its activation</p> \[a_i^{(\ell)} = \sigma\left(z_i^{(\ell)}\right),\] <p>as well as how this activation is propagated forward to all neurons in the subsequent layer $\ell + 1$.</p> <p>Specifically, the activation $a_i^{(\ell)}$ serves as an input to each neuron in layer $\ell + 1$, where it is multiplied by the corresponding weights $w_{k,i}^{(\ell+1)}$, summed together with the other incoming weighted activations, and combined with the bias terms $b_k^{(\ell+1)}$ to form the pre-activations $z_k^{(\ell+1)}$. This explicit depiction makes clear how a single activation influences multiple downstream neurons and, conversely (as we will see below), how gradients computed at layer $\ell + 1$ will later be propagated backward through the same weighted connections to layer $\ell$.</p> <p><br></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-08-02-neural-nets-and-backprop/fig3-480.webp 480w,/assets/img/2025-08-02-neural-nets-and-backprop/fig3-800.webp 800w,/assets/img/2025-08-02-neural-nets-and-backprop/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-08-02-neural-nets-and-backprop/fig3.png" class="img-fluid rounded z-depth-1 imgcenter" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <b>Figure 3:</b> Zoom into the neural network. </figcaption> </figure> <p>Gradient descent can be used to minimize the cost function $J(\Theta)$. This requires computing the partial derivatives of $J$ with respect to all trainable parameters contained in $\Theta$. Due to the layered structure of a neural network, these derivatives cannot be computed directly in a single step. Figure 3 illustrates this dependency structure for a specific weight $w_{i,j}^{(\ell)}$.</p> <p>At this point, it is convenient to distinguish between the <strong>loss</strong> $\mathcal{L}(\vec y^*, \hat{\vec y})$ of a single training example and the <strong>cost function</strong> $J(\Theta)$, which aggregates the loss over a batch of $\batch$ examples. The derivation of backpropagation is carried out at the single-sample level, since differentiation is a linear operation. In particular, if</p> \[J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n(\Theta),\] <p>then the gradient of the cost function is given by</p> \[\frac{\partial J}{\partial \Theta} = \frac{1}{\batch} \sum_{n=1}^{\batch} \frac{\partial \mathcal{L}_n}{\partial \Theta}.\] <p>Thus, the gradient of $J$ is simply the average of the gradients of the individual losses. For this reason, we first derive the gradients of the loss for a single training example and later extend the result to a batch by averaging.</p> <p>Assume that we want to compute the partial derivative of the loss function with respect to the weight $w_{i,j}^{(\ell)}$. Since this weight influences the loss only indirectly through the pre-activation $z_i^{(\ell)}$ of neuron $i$ in layer $\ell$, the chain rule yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \frac{\partial z_i^{(\ell)}}{\partial w_{i,j}^{(\ell)}}. \label{eq:partial1} \end{align}\] <p>From the definition of the pre-activation</p> \[z_i^{(\ell)} = \sum_{j} w_{i,j}^{(\ell)} a_j^{(\ell-1)} + b_i^{(\ell)},\] <p>it follows immediately that</p> \[\begin{align} \frac{\partial z_i^{(\ell)}}{\partial w_{i,j}^{(\ell)}} = a_j^{(\ell-1)}. \label{eq:partial2} \end{align}\] <p>Substituting this result into Eq. \eqref{eq:partial1} yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} = \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}. \end{align}\] <p>So far, we have considered the partial derivative of the loss with respect to the weights $w_{i,j}^{(\ell)}$. An analogous and even simpler expression is obtained for the bias parameters $b_i^{(\ell)}$.</p> <p>Since the bias enters the pre-activation additively, we have</p> \[\frac{\partial z_i^{(\ell)}}{\partial b_i^{(\ell)}} = 1.\] <p>Therefore, by the chain rule,</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \frac{\partial z_i^{(\ell)}}{\partial b_i^{(\ell)}} \\ &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \end{align}\] <p>In summary, we have:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}, \label{eq:partial3} \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \label{eq:partial3_b} \end{align}\] <p>The remaining partial derivative $\partial \mathcal{L} / \partial z_i^{(\ell)}$ cannot be evaluated directly. However, it can be expressed recursively by applying the chain rule, which forms the basis of the backpropagation algorithm.</p> <hr> <h2 id="interlude-the-chain-rule-and-nested-dependencies">Interlude: The Chain Rule and Nested Dependencies</h2> <p>Before deriving the backpropagation algorithm, it is helpful to briefly recall the chain rule for derivatives, which plays a central role in computing gradients in neural networks. In particular, neural networks are composed of nested functions, so changes in a parameter typically affect the output only indirectly through a sequence of intermediate variables.</p> <p>Consider a function $z = f(x, y)$ that depends on two intermediate variables $x$ and $y$, which in turn both depend on a common variable $t$, i.e.</p> \[x = g(t), \qquad y = h(t).\] <p>In this case, the total derivative of $z$ with respect to $t$ is given by the chain rule as</p> \[\begin{align} z &amp;= f(x, y), \quad x = g(t), \quad y = h(t), \\ \frac{d z}{d t} &amp;= \frac{\partial z}{\partial x} \frac{d x}{d t} + \frac{\partial z}{\partial y} \frac{d y}{d t}. \label{eq:chainrule1} \end{align}\] <p>This expression shows that the influence of $t$ on $z$ is obtained by summing all paths through which $t$ affects the intermediate variables $x$ and $y$.</p> <p>Consider the composite function</p> \[z = \sin(x^2 + y^2), \qquad x = e^t, \qquad y = t^2.\] <p>Since $z$ depends on $t$ only through the intermediate variables $x$ and $y$, the chain rule gives</p> \[\begin{align} \frac{\partial z}{\partial x} &amp;= 2x \cos(x^2 + y^2), \\ \frac{\partial z}{\partial y} &amp;= 2y \cos(x^2 + y^2), \\ \frac{d x}{d t} &amp;= e^t, \\ \frac{d y}{d t} &amp;= 2t. \end{align}\] <p>Therefore, the total derivative of $z$ with respect to $t$ is</p> \[\begin{align} \frac{d z}{d t} &amp;= 2x \cos(x^2 + y^2)\,\frac{d x}{d t} + 2y \cos(x^2 + y^2)\,\frac{d y}{d t} \\ &amp;= 2x \cos(x^2 + y^2)\,e^t + 2y \cos(x^2 + y^2)\,2t \\ &amp;= 2\cos(x^2 + y^2)\left(x\,e^t + 2yt\right) \\ &amp;= 2\cos\left(e^{2t} + t^4\right)\left(e^{2t} + 2t^3\right). \end{align}\] <p>Using direct differentiation, we obtain</p> \[\begin{align} z &amp;= \sin(e^{2t} + t^4), \\ \frac{d z}{d t} &amp;= \cos(e^{2t} + t^4)\left(2 e^{2t} + 4 t^3\right) \\ &amp;= 2\cos(e^{2t} + t^4)\left(e^{2t} + 2 t^3\right), \end{align}\] <p>which is identical to the result obtained via the chain rule.</p> <p>Similarly, an analogous form of the chain rule applies when the intermediate variables are vector-valued. Let $\vec t$ be a vector and let $z = f(\vec x)$ with $\vec x = g(\vec t)$. Then the partial derivative of $z$ with respect to the $i$-th component of $\vec t$ is given by</p> \[\begin{align} z &amp;= f(\vec{x}), \qquad \vec{x} = g(\vec{t}), \\ \frac{\partial z}{\partial t_i} &amp;= \frac{\partial z}{\partial x_1} \frac{\partial x_1}{\partial t_i} + \frac{\partial z}{\partial x_2} \frac{\partial x_2}{\partial t_i} + \cdots \\ &amp;= \sum_{j} \frac{\partial z}{\partial x_j} \frac{\partial x_j}{\partial t_i}. \label{eq:chainrule2} \end{align}\] <p>This expression shows that the influence of a single component $t_i$ on $z$ is obtained by summing over all paths through which $t_i$ affects the intermediate variables $\vec x$.</p> <hr> <h2 id="backpropagation-via-the-chain-rule">Backpropagation via the Chain Rule</h2> <p>We can now use the chain rule to compute the partial derivative $\partial \mathcal{L} / \partial z_i^{(\ell)}$ in Eq. \eqref{eq:partial3} &amp; Eq. \eqref{eq:partial3_b} in a recursive manner. As illustrated in Figure 3, the pre-activations $z_k^{(\ell+1)}$ of the next layer can be regarded as functions of the pre-activations of the current layer, in particular of $z_i^{(\ell)}$, i.e., $z_k^{(\ell+1)} = z_k^{(\ell+1)}(\ldots, z_i^{(\ell)}, \ldots)$. Consequently, the loss function depends on $z_i^{(\ell)}$ only indirectly through its influence on all downstream pre-activations $z_k^{(\ell+1)}$. Formally, one may therefore view the loss function as a composite function of the form $\mathcal{L} = \mathcal{L}\big(\ldots, z_k^{(\ell+1)}(\ldots, z_i^{(\ell)}, \ldots), \ldots\big)$, which naturally leads to a recursive application of the chain rule.</p> <p>Applying the chain rule, we obtain</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_1^{(\ell+1)}} \frac{\partial z_1^{(\ell+1)}}{\partial z_i^{(\ell)}} + \frac{\partial \mathcal{L}}{\partial z_2^{(\ell+1)}} \frac{\partial z_2^{(\ell+1)}}{\partial z_i^{(\ell)}} + \cdots \\ &amp;= \sum_{k=1}^{d_{\ell+1}} \frac{\partial \mathcal{L}}{\partial z_k^{(\ell+1)}} \frac{\partial z_k^{(\ell+1)}}{\partial z_i^{(\ell)}}. \label{eq:chainrule3} \end{align}\] <p>From the definition</p> \[z_k^{(\ell+1)} = \sum_j w^{(\ell+1)}_{k,j} a_j^{(\ell)} + b_k^{(\ell+1)}, \qquad a_j^{(\ell)} = \sigma\!\left(z_j^{(\ell)}\right),\] <p>it follows that</p> \[\frac{\partial z_k^{(\ell+1)}}{\partial z_i^{(\ell)}} = w^{(\ell+1)}_{k,i} \, \sigma'\!\left(z_i^{(\ell)}\right),\] <p>where $\sigma’(z)$ denotes the derivative of the activation function $\sigma(\cdot)$ with respect to its argument.</p> <p>Substituting this result into Eq. \eqref{eq:chainrule3} yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} = \sigma'\!\left(z_i^{(\ell)}\right) \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i} \frac{\partial \mathcal{L}}{\partial z_k^{(\ell+1)}}. \label{eq:backprop-recursion} \end{align}\] <p>Note the recursive structure of Eq. \eqref{eq:backprop-recursion}. For notational convenience, we define the error signal (also called the delta term), i.e. the gradient of the loss with respect to the pre-activation, at neuron $i$ in layer $\ell$ as</p> \[\delta_i^{(\ell)} := \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}.\] <p>With this definition, Eq. \eqref{eq:backprop-recursion} can be written in the recursive form</p> \[\begin{align} \delta_i^{(\ell)} &amp;= \sigma'\!\left(z_i^{(\ell)}\right) \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i}\, \delta_k^{(\ell+1)}. \label{eq:chainrule4} \end{align}\] <p>Equation \eqref{eq:chainrule4} can be written in a more compact vector form. To this end, we introduce the error vectors</p> \[\begin{align} \vec{\delta}^{(\ell)} &amp;\in \mathbb{R}^{d_\ell }, \\ \vec{\delta}^{(\ell+1)} &amp;\in \mathbb{R}^{d_{\ell+1} }, \end{align}\] <p>and the weight matrix</p> \[\begin{align} \Wmatr^{(\ell+1)} \in \mathbb{R}^{d_{\ell+1} \times d_\ell}. \label{eq:chainrule6} \end{align}\] <p>In Eq.\eqref{eq:chainrule4}, the summation is carried out over the index $k$, which corresponds to the first index of the weight $w_{k,i}^{(\ell+1)}$. This implies that, in matrix notation, the transpose of $\Wmatr^{(\ell+1)}$ must be used. Indeed, we have</p> \[\begin{align} (\Wmatr^{(\ell+1)})^\top &amp;\in \mathbb{R}^{d_\ell \times d_{\ell+1}}, \\ (\Wmatr^{(\ell+1)})^\top \vec{\delta}^{(\ell+1)} &amp;\in \mathbb{R}^{d_\ell }. \label{eq:chainrule7} \end{align}\] <p>To see how the transpose arises explicitly, write the weight matrix row-wise as</p> \[\Wmatr^{(\ell+1)} = \begin{pmatrix} (\vec w^{(\ell+1)}_{1})^\top \\ (\vec w^{(\ell+1)}_{2})^\top \\ \vdots \\ (\vec w^{(\ell+1)}_{d_{\ell+1}})^\top \end{pmatrix}, \qquad \vec w^{(\ell+1)}_{k} \in \mathbb{R}^{d_\ell},\] <p>i.e., the $k$-th row contains the incoming weights of neuron $k$ in layer $\ell+1$. Taking the transpose yields</p> \[(\Wmatr^{(\ell+1)})^\top = \begin{pmatrix} \vec w^{(\ell+1)}_{1} &amp; \vec w^{(\ell+1)}_{2} &amp; \cdots &amp; \vec w^{(\ell+1)}_{d_{\ell+1}} \end{pmatrix}.\] <p>Now let</p> \[\vec\delta^{(\ell+1)} = \begin{pmatrix} \delta^{(\ell+1)}_{1}\\ \delta^{(\ell+1)}_{2}\\ \vdots\\ \delta^{(\ell+1)}_{d_{\ell+1}} \end{pmatrix}.\] <p>Then the matrix-vector product can be interpreted as a weighted sum of the (transposed) row vectors:</p> \[(\Wmatr^{(\ell+1)})^\top \vec\delta^{(\ell+1)} = \sum_{k=1}^{d_{\ell+1}} \vec w^{(\ell+1)}_{k}\,\delta^{(\ell+1)}_{k}.\] <p>Looking at the $i$-th component of this vector gives</p> \[\big[(\Wmatr^{(\ell+1)})^\top \vec\delta^{(\ell+1)}\big]_i = \sum_{k=1}^{d_{\ell+1}} w^{(\ell+1)}_{k,i}\,\delta^{(\ell+1)}_{k},\] <p>which is exactly the summation term appearing in Eq. \eqref{eq:chainrule4}.</p> <p>Using this notation (following the <a href="https://en.wikipedia.org/wiki/Matrix_calculus" rel="external nofollow noopener" target="_blank">denominator-layout convention</a>) and writing the derivative of the activation function element-wise, the backpropagation recursion can be expressed as</p> \[\begin{align} \vec{\delta}^{(\ell)} := \frac{\partial \mathcal{L}}{\partial \vec z^{(\ell)}} = \Big( (\Wmatr^{(\ell+1)})^\top \vec{\delta}^{(\ell+1)} \Big) \otimes \sigma'\!\left(\vec z^{(\ell)}\right), \label{eq:chainrule8} \end{align}\] <p>where $\otimes$ denotes the Hadamard (element-wise) product and the derivative $\sigma’(\cdot)$ is applied component-wise to the vector $\vec z^{(\ell)}$.</p> <p>Recall Eqs. \eqref{eq:partial3} and \eqref{eq:partial3_b}, which give the gradients of the loss with respect to the weights and biases of layer $\ell$:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}} \, a_j^{(\ell-1)}, \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i^{(\ell)}}. \end{align}\] <p>By comparison with Eq. \eqref{eq:chainrule8}, we immediately obtain</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} &amp;= \delta_i^{(\ell)} \, a_j^{(\ell-1)}, \label{eq:finalbackprop} \\ \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} &amp;= \delta_i^{(\ell)} \label{eq:finalbackprop_b} \end{align}\] <p>Thus, once the error signal $\vec{\delta}^{(\ell)}$ has been computed for a given layer, the gradients with respect to both the weights and the biases follow directly. These observations naturally lead to compact matrix expressions for the gradients with respect to the weight matrices and bias vectors, which we derive next.</p> <p>From the component-wise relations</p> \[\frac{\partial \mathcal{L}}{\partial w_{i,j}^{(\ell)}} = \delta_i^{(\ell)}\, a_j^{(\ell-1)}, \qquad \frac{\partial \mathcal{L}}{\partial b_i^{(\ell)}} = \delta_i^{(\ell)},\] <p>we can derive compact matrix-vector expressions for the gradients with respect to the weight matrix $\Wmatr^{(\ell)}$ and the bias vector $\vec b^{(\ell)}$.</p> <hr> <h3 id="gradient-with-respect-to-the-weights-vectorized-form">Gradient with respect to the weights (vectorized form)</h3> <p>Collecting all partial derivatives $\partial \mathcal{L}/\partial w_{i,j}^{(\ell)}$ into a matrix yields</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial \Wmatr^{(\ell)}} &amp;= \vec{\delta}^{(\ell)} \, \big(\vec a^{(\ell-1)}\big)^\top. \label{eq:finalbackprop2} \end{align}\] <p>The transpose arises because $\vec{\delta}^{(\ell)}$ and $\vec a^{(\ell-1)}$ are column vectors. With the (column-vector) convention used throughout,</p> \[\vec{\delta}^{(\ell)} \in \mathbb{R}^{d_\ell \times 1}, \qquad \vec a^{(\ell-1)} \in \mathbb{R}^{d_{\ell-1} \times 1}.\] <p>To obtain a matrix in $\mathbb{R}^{d_\ell \times d_{\ell-1}}$, we must form an outer product: the first factor stays a column vector, while the second is transposed into a row vector:</p> \[\big(\vec a^{(\ell-1)}\big)^\top \in \mathbb{R}^{1 \times d_{\ell-1}}.\] <p>Thus the dimensions match as</p> \[\begin{align} \vec{\delta}^{(\ell)} \, \big(\vec a^{(\ell-1)}\big)^\top &amp;\in \mathbb{R}^{d_\ell \times 1}\;\cdot\;\mathbb{R}^{1 \times d_{\ell-1}} = \mathbb{R}^{d_\ell \times d_{\ell-1}}. \end{align}\] <p>Equivalently, the $(i,j)$-th entry of this outer product is</p> \[\left[\vec{\delta}^{(\ell)} \, (\vec a^{(\ell-1)})^\top\right]_{i,j} = \delta_i^{(\ell)}\, a_j^{(\ell-1)},\] <p>which matches the component-wise gradient $\partial \mathcal{L}/\partial w_{i,j}^{(\ell)}$.</p> <hr> <h3 id="gradient-with-respect-to-the-biases-vectorized-form">Gradient with respect to the biases (vectorized form)</h3> <p>The bias gradient collects the partial derivatives $\partial \mathcal{L}/\partial b_i^{(\ell)}$ into a vector:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial \vec b^{(\ell)}} &amp;= \vec{\delta}^{(\ell)} \in \mathbb{R}^{d_\ell \times 1}. \label{eq:finalbackprop2_b} \end{align}\] <p>Hence, once the error vector $\vec{\delta}^{(\ell)}$ has been computed via the backpropagation recursion \eqref{eq:chainrule8}, both gradients $\partial \mathcal{L}/\partial \Wmatr^{(\ell)}$ and $\partial \mathcal{L}/\partial \vec b^{(\ell)}$ follow immediately.</p> <p>As before, we can propagate a batch of training examples through the network by collecting them column-wise into the input matrix</p> \[\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}.\] <p>During the forward pass, each layer produces an activation matrix $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$, where the $n$-th column corresponds to the activations generated by the $n$-th training example. Proceeding layer by layer, we eventually obtain the output matrix $\matr A^{(L)}$.</p> <p>In the derivation above, the gradients were computed for a single training example. To perform a batch update of the parameters, these gradients must be aggregated over all $\batch$ examples in the batch. Since differentiation is linear, this aggregation is achieved by summing (or averaging) the per-example gradients.</p> <p>To this end, we collect the error vectors $\vec{\delta}^{(\ell)}$ of all training examples at layer $\ell$ into the error matrix</p> \[\matr\Delta^{(\ell)} = \big( \vec{\delta}_1^{(\ell)} \; \vec{\delta}_2^{(\ell)} \; \ldots \; \vec{\delta}_\batch^{(\ell)} \big) \in \mathbb{R}^{d_\ell \times \batch}.\] <p>Analogously, we denote by \(\matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}\) the matrix of pre-activations at layer $\ell$.</p> <hr> <h2 id="batch-backpropagation-recursion">Batch Backpropagation Recursion</h2> <p>Applying the single-sample recursion \eqref{eq:chainrule8} column-wise to all examples yields the batch version of backpropagation:</p> \[\begin{align} \matr\Delta^{(\ell)} &amp;= \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \;\otimes\; \sigma'\!\left(\matr Z^{(\ell)}\right), \label{eq:matrixBackprop} \end{align}\] <p>where the derivative $\sigma’(\cdot)$ is applied element-wise and $\otimes$ denotes the Hadamard product.</p> <p>The dimensions involved are</p> \[\begin{align} (\Wmatr^{(\ell+1)})^\top &amp;\in \mathbb{R}^{d_\ell \times d_{\ell+1}}, \\ \matr\Delta^{(\ell+1)} &amp;\in \mathbb{R}^{d_{\ell+1} \times \batch}, \\ \matr Z^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times \batch}, \end{align}\] <p>so that</p> \[(\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \in \mathbb{R}^{d_\ell \times \batch},\] <p>and the element-wise multiplication with $\sigma’(\matr Z^{(\ell)})$ is well defined.</p> <hr> <h3 id="batch-gradients-for-weights-and-biases">Batch gradients for weights and biases</h3> <p>Using the batch error matrix $\matr\Delta^{(\ell)}$, the gradients of the loss with respect to the parameters of layer $\ell$ can be written compactly as</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \label{eq:J_W_ell} \\[6pt] \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \label{eq:J_b_ell} \end{align}\] <p>with the following dimensions:</p> \[\begin{align} \matr\Delta^{(\ell)} &amp;\in \mathbb{R}^{d_\ell \times \batch}, \\ \matr A^{(\ell-1)} &amp;\in \mathbb{R}^{d_{\ell-1} \times \batch}, \\ \big(\matr A^{(\ell-1)}\big)^\top &amp;\in \mathbb{R}^{\batch \times d_{\ell-1}}, \\ \vec 1 &amp;\in \mathbb{R}^{\batch}, \\ \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;\in \mathbb{R}^{d_\ell \times d_{\ell-1}}, \\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;\in \mathbb{R}^{d_\ell}. \end{align}\] <p>Here, the weight gradient has the same shape as the weight matrix $\Wmatr^{(\ell)}$, and the bias gradient has the same shape as the bias vector $\vec b^{(\ell)}$, as required for gradient-based optimization.</p> <p>The first expression Eq. \eqref{eq:J_W_ell} corresponds to summing the outer products $\vec{\delta}_n^{(\ell)} (\vec a_n^{(\ell-1)})^\top$ over all training examples, while the second expression in Eq. \eqref{eq:J_b_ell} reflects the fact that each bias gradient is obtained by summing the corresponding error signals across the batch.</p> <p>These batch-wise expressions allow all gradients to be computed efficiently using matrix operations, which is crucial for practical implementations of backpropagation.</p> <p>To make the batch expressions more explicit, we write out the matrices $\matr\Delta^{(\ell)}$ and $\matr A^{(\ell-1)}$ in terms of their column vectors.</p> <p>The activation matrix of layer $\ell-1$ is given by</p> \[\matr A^{(\ell-1)} = \begin{pmatrix} \vec a^{(\ell-1)}_1 &amp; \vec a^{(\ell-1)}_2 &amp; \cdots &amp; \vec a^{(\ell-1)}_\batch \end{pmatrix} \in \mathbb{R}^{d_{\ell-1} \times \batch},\] <p>where</p> \[\vec a^{(\ell-1)}_n \in \mathbb{R}^{d_{\ell-1}}\] <p>denotes the activation vector produced by the $n$-th training example at layer $\ell-1$.</p> <p>Similarly, the error matrix at layer $\ell$ is</p> \[\matr\Delta^{(\ell)} = \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \in \mathbb{R}^{d_\ell \times \batch},\] <p>where each column</p> \[\vec\delta^{(\ell)}_n \in \mathbb{R}^{d_\ell}\] <p>is the error signal corresponding to the $n$-th training example.</p> <p>Taking the transpose of the activation matrix yields</p> \[\big(\matr A^{(\ell-1)}\big)^\top = \begin{pmatrix} (\vec a^{(\ell-1)}_1)^\top \\ (\vec a^{(\ell-1)}_2)^\top \\ \vdots \\ (\vec a^{(\ell-1)}_\batch)^\top \end{pmatrix} \in \mathbb{R}^{\batch \times d_{\ell-1}}.\] <hr> <h3 id="weight-gradient-as-a-sum-of-outer-products">Weight gradient as a sum of outer products</h3> <p>Using these explicit forms, the batch gradient with respect to the weight matrix can be expanded as</p> \[\begin{align} \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top &amp;= \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \begin{pmatrix} (\vec a^{(\ell-1)}_1)^\top \\ (\vec a^{(\ell-1)}_2)^\top \\ \vdots \\ (\vec a^{(\ell-1)}_\batch)^\top \end{pmatrix} \\[6pt] &amp;= \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n \, (\vec a^{(\ell-1)}_n)^\top. \end{align}\] <p>Thus, the gradient of the cost function with respect to the weights is</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top \\ &amp;= \frac{1}{\batch} \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n \, (\vec a^{(\ell-1)}_n)^\top, \end{align}\] <p>which is exactly the average of the single-sample gradients derived earlier.</p> <hr> <h3 id="bias-gradient-as-a-sum-of-error-signals">Bias Gradient as a Sum of Error Signals</h3> <p>For the bias parameters, we multiply the error matrix by a vector of ones,</p> \[\vec 1 = \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} \in \mathbb{R}^{\batch}.\] <p>This yields</p> \[\begin{align} \matr\Delta^{(\ell)} \vec 1 &amp;= \begin{pmatrix} \vec\delta^{(\ell)}_1 &amp; \vec\delta^{(\ell)}_2 &amp; \cdots &amp; \vec\delta^{(\ell)}_\batch \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} = \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n. \end{align}\] <p>Therefore, the batch gradient with respect to the bias vector is</p> \[\begin{align} \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1 \\ &amp;= \frac{1}{\batch} \sum_{n=1}^{\batch} \vec\delta^{(\ell)}_n. \end{align}\] <hr> <p>These expansions make explicit that the matrix-based batch formulas implement nothing more than an efficient summation (or averaging) of the per-example gradients, fully consistent with the definition of the cost function \(J(\Theta) = \frac{1}{\batch} \sum_{n=1}^{\batch} \mathcal{L}_n(\Theta).\)</p> <hr> <h2 id="gradient-descent-parameter-updates">Gradient Descent Parameter Updates</h2> <p>Once the gradients of the cost function $J(\Theta)$ with respect to all parameters have been computed (typically for a full batch or mini-batch; see the section on practial considerations for details), the weights and biases can be updated using a gradient-based optimizer. For standard (batch) gradient descent with learning rate $\eta&gt;0$, the update rule for layer $\ell$ is</p> \[\begin{align} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \, \frac{\partial J}{\partial \Wmatr^{(\ell)}}, \label{eq:gd-update_1} \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \, \frac{\partial J}{\partial \vec b^{(\ell)}}. \label{eq:gd-update} \end{align}\] <p>Using the batch-gradient expressions derived above, we have</p> \[\begin{align} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;= \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \end{align}\] <p>where $\matr\Delta^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ collects the error vectors of layer $\ell$ for all $\batch$ examples and $\vec 1 \in \mathbb{R}^{\batch}$ is a vector of ones. Substituting these gradients into \eqref{eq:gd-update_1} &amp; \eqref{eq:gd-update} yields the explicit batch update rules</p> \[\begin{align} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \,\frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \,\frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1. \label{eq:gd-update-batch} \end{align}\] <p>These updates are applied for $\ell = 1,2,\ldots,L$. In practice, $J$ is often computed over a mini-batch, so $\batch$ denotes the mini-batch size.</p> <blockquote> <p><strong>Implementation note (when to update).</strong><br> In a standard backpropagation implementation, one first performs the full backward pass to compute all error signals $\matr\Delta^{(\ell)}$ (and thus all gradients) before updating any parameters. Updating weights prematurely would mix parameters from different stages of the backward pass and can lead to incorrect gradients.</p> <p>That said, with careful bookkeeping (e.g., storing the required activations and error signals, or recomputing them deterministically), updates can be scheduled in different ways. The safest approach is: compute all gradients first, then update. Implement this approach first and make sure everything is working as intended, before considering optimizations of the backpropagation algorithm.</p> </blockquote> <h2 id="initialization-of-backpropagation-at-the-output-layer">Initialization of Backpropagation at the Output Layer</h2> <p>The recursive relations in Eqs. \eqref{eq:chainrule4} and \eqref{eq:chainrule8} express the error signal in layer $\ell$ in terms of the error signals in the subsequent layer $\ell+1$. Consequently, this recursion cannot be evaluated forward and must instead be initialized at the output layer $\ell = L$ and then propagated backward through the network.</p> <p>This backward flow of error signals (from the output layer toward the input layer) is the origin of the term <em>backpropagation</em>.</p> <hr> <h3 id="output-layer-error-signal-single-training-example">Output-layer Error Signal (single training example)</h3> <p>We begin by computing the error signal for the output layer $\ell = L$. For the mean squared error (MSE) loss and a single training example, the error signal of the $i$-th output neuron is defined as</p> \[\begin{align} \delta_i^{(L)} &amp;:= \frac{\partial \mathcal{L}}{\partial z_i^{(L)}} \\[4pt] &amp;= \frac{\partial}{\partial z_i^{(L)}} \left[ \frac{1}{2} \big(y_i^* - a_i^{(L)}\big)^2 \right] \\[4pt] &amp;= \frac{\partial}{\partial z_i^{(L)}} \left[ \frac{1}{2} \big(y_i^* - \sigma(z_i^{(L)})\big)^2 \right] \\[4pt] &amp;= -\big(y_i^* - \sigma(z_i^{(L)})\big)\, \sigma'\!\left(z_i^{(L)}\right) \\[4pt] &amp;= \big(a_i^{(L)} - y_i^*\big)\, \sigma'\!\left(z_i^{(L)}\right)\\[4pt] &amp;= \big(\yhat_i - y_i^*\big)\, \sigma'\!\left(z_i^{(L)}\right). \end{align}\] <p>This expression provides the starting point for the recursive computation of error signals in all preceding layers.</p> <hr> <blockquote> <p><strong>Remark (Cross-entropy loss).</strong><br> For the binary cross-entropy loss in combination with a sigmoid activation function in the output layer (or for the generalized cross-entropy loss with a softmax activation), the derivative of the activation function cancels with a corresponding term from the loss derivative. In this case, the output-layer error signal simplifies to $\delta_i^{(L)} = a_i^{(L)} - y_i^*.$ This simplification is one of the main reasons why cross-entropy loss is commonly preferred over mean squared error for classification tasks.</p> </blockquote> <hr> <h3 id="vectorized-form-single-training-example">Vectorized Form (single training example)</h3> <p>Collecting the error signals of all output neurons into a vector yields</p> \[\begin{align} \vec{\delta}^{(L)} &amp;= \big(\vec a^{(L)} - \vec y^*\big) \otimes \sigma'\!\left(\vec z^{(L)}\right) \\[4pt] &amp;= \big(\hat{\vec y} - \vec y^*\big) \otimes \sigma'\!\left(\vec z^{(L)}\right), \label{eq:chainrule5} \end{align}\] <p>with the following dimensions:</p> \[\begin{align} \vec{\delta}^{(L)} &amp;\in \mathbb{R}^{d_L}, \\ \vec a^{(L)} = \hat{\vec y}, \vec y^* &amp;\in \mathbb{R}^{d_L}, \\ \vec z^{(L)} &amp;\in \mathbb{R}^{d_L}, \\ \sigma'\!\left(\vec z^{(L)}\right) &amp;\in \mathbb{R}^{d_L}. \end{align}\] <p>Here, $\otimes$ denotes the Hadamard (element-wise) product, and the derivative $\sigma’(\cdot)$ is applied component-wise to the pre-activation vector $\vec z^{(L)}$.</p> <hr> <h3 id="vectorized-batch-formulation">Vectorized Batch Formulation</h3> <p>When processing a batch of $\batch$ training examples, the individual output-layer error vectors $\vec{\delta}_n^{(L)}$ are collected column-wise into the batch error matrix</p> \[\begin{align} \matr\Delta^{(L)} &amp;= \begin{pmatrix} \vec{\delta}_1^{(L)} &amp; \vec{\delta}_2^{(L)} &amp; \cdots &amp; \vec{\delta}_\batch^{(L)} \end{pmatrix} \\ &amp;= \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big) \\ &amp;= \big(\matr{\hat{Y}} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big), \end{align}\] <p>with $\matr\Delta^{(L)} \in \mathbb{R}^{d_L \times \batch}$, the predictions</p> \[\matr{\hat{Y}} = \big(\vec{\yhat}_1 \ \ldots \ \vec{\yhat}_\batch \big)\in \mathbb{R}^{d_L \times \batch},\] <p>and the targets (truth labels)</p> \[\matr Y^* = \big(\vec y_1^* \ \ldots \ \vec y_\batch^*\big)\in \mathbb{R}^{d_L \times \batch}.\] <p>This matrix constitutes the initial condition for batch backpropagation. Starting from $\matr\Delta^{(L)}$, the error matrices of all preceding layers $\matr\Delta^{(\ell)}$ are obtained recursively using Eq. \eqref{eq:chainrule8}.</p> <hr> <h2 id="summary-backpropagation">Summary: Backpropagation</h2> <p>We summarize the steps required to compute all gradients of the cost function $J(\Theta)$ for a batch of $\batch$ training examples using backpropagation. The resulting gradients can then be used in a gradient-descent update.</p> <h3 id="inputs-and-stored-quantities">Inputs and Stored Quantities</h3> <ul> <li> <strong>Network architecture</strong> <ul> <li>Number of layers: $L$</li> <li>Layer sizes: $d_0=\din, d_1, \ldots, d_L$</li> </ul> </li> <li> <strong>Trainable parameters (for each layer $\ell=1,\ldots,L$)</strong> <ul> <li>$\Wmatr^{(\ell)} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}$</li> <li>$\vec b^{(\ell)} \in \mathbb{R}^{d_\ell}$</li> </ul> </li> <li> <strong>Batch data</strong> <ul> <li>Inputs: $\matr X = \matr A^{(0)} \in \mathbb{R}^{\din \times \batch}$</li> <li>Targets: $\matr Y^* = \big(\vec y_1^* \ \ldots \ \vec y_\batch^*\big)\in \mathbb{R}^{d_L \times \batch}$</li> </ul> </li> <li> <strong>Forward-pass cache (must be available from the forward pass)</strong> <ul> <li>Activations: $\matr A^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ for $\ell=0,\ldots,L$</li> <li>Pre-activations: $\matr Z^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}$ for $\ell=1,\ldots,L$</li> </ul> </li> <li> <strong>Element-wise operations</strong> <ul> <li>$\sigma(\cdot)$ and $\sigma’(\cdot)$ are applied element-wise</li> <li>$\otimes$ denotes the Hadamard (element-wise) product</li> <li>$\vec 1 \in \mathbb{R}^{\batch}$ denotes a vector of ones</li> </ul> </li> </ul> <hr> <h3 id="backward-pass-error-signals">Backward Pass: Error Signals</h3> <ol> <li> <p><strong>Initialize at the output layer ($\ell=L$)</strong><br> For mean squared error (MSE), \(\matr\Delta^{(L)} = \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big), \qquad \matr\Delta^{(L)} \in \mathbb{R}^{d_L \times \batch}.\)</p> <p>(Other loss/activation pairs may yield a simpler expression; e.g. cross-entropy + sigmoid/softmax. See below.)</p> </li> <li> <p><strong>Propagate errors backward</strong><br> For $\ell = L-1, L-2, \ldots, 1$: \(\matr\Delta^{(\ell)} = \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \ \otimes\ \sigma'\!\big(\matr Z^{(\ell)}\big), \qquad \matr\Delta^{(\ell)} \in \mathbb{R}^{d_\ell \times \batch}.\)</p> </li> </ol> <hr> <h3 id="gradients-for-weights-and-biases-batch">Gradients for Weights and Biases (Batch)</h3> <p>For each layer $\ell = 1,\ldots,L$, the gradients of the <strong>cost</strong> \(J(\Theta)=\frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta)\) are</p> <ul> <li> <p><strong>Weight gradient</strong> \(\frac{\partial J}{\partial \Wmatr^{(\ell)}} = \frac{1}{\batch}\; \matr\Delta^{(\ell)} \big(\matr A^{(\ell-1)}\big)^\top, \qquad \frac{\partial J}{\partial \Wmatr^{(\ell)}} \in \mathbb{R}^{d_\ell \times d_{\ell-1}}.\)</p> </li> <li> <p><strong>Bias gradient</strong> \(\frac{\partial J}{\partial \vec b^{(\ell)}} = \frac{1}{\batch}\; \matr\Delta^{(\ell)} \vec 1, \qquad \frac{\partial J}{\partial \vec b^{(\ell)}} \in \mathbb{R}^{d_\ell}.\)</p> </li> </ul> <hr> <h3 id="parameter-update-gradient-descent">Parameter Update (Gradient Descent)</h3> <p>With learning rate $\eta&gt;0$, update for each layer $\ell=1,\ldots,L$:</p> \[\begin{align*} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta\,\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta\,\frac{\partial J}{\partial \vec b^{(\ell)}}. \end{align*}\] <hr> <h3 id="remarks-1">Remarks</h3> <ul> <li>The backward pass must be evaluated from $\ell=L$ down to $\ell=1$ because each $\matr\Delta^{(\ell)}$ depends on $\matr\Delta^{(\ell+1)}$.</li> <li>The forward-pass matrices ${\matr A^{(\ell)}, \matr Z^{(\ell)}}$ are required to compute $\matr\Delta^{(\ell)}$ and the parameter gradients.</li> <li>In a standard implementation: compute all gradients first, then update all parameters.</li> </ul> <hr> <h1 id="putting-everything-together-full-batch-vectorized">Putting Everything Together (Full-Batch, Vectorized)</h1> <p>This section summarizes one full <strong>training step</strong> (full batch) in a form that maps 1:1 to an implementation.</p> <hr> <p><strong>1. Forward Pass (cache activations and pre-activations)</strong></p> <p><em>Initialize</em></p> \[\matr A^{(0)} := \matr X\] <p><em>For each layer $ \ \ell = 1,2,\ldots,L$</em></p> \[\matr Z^{(\ell)} := \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)} \vec 1^\top\] \[\matr A^{(\ell)} := \sigma\!\left(\matr Z^{(\ell)}\right)\] <p><em>Network output</em></p> \[\matr{\hat{Y}} := \matr A^{(L)}\] <p><em>What to store for backprop</em></p> \[\{\matr A^{(\ell)}\}_{\ell=0}^{L}, \qquad \{\matr Z^{(\ell)}\}_{\ell=1}^{L}.\] <hr> <p><strong>2. Cost (full-batch scalar)</strong></p> <p>Define the batch cost as the mean loss over the batch:</p> \[J(\Theta) := \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}\!\left(\vec y_n^*, \hat{\vec y}_n\right).\] <p>(Equivalently: aggregate a per-example loss column-wise over $\matr Y^*$ and $\matr{\hat{Y}}$.)</p> <hr> <p><strong>3. Backward Pass</strong></p> <p><strong>Initialize at the output layer</strong></p> <p><em>MSE + general activation</em></p> \[\matr\Delta^{(L)} := \big(\matr A^{(L)} - \matr Y^*\big)\ \otimes\ \sigma'\!\big(\matr Z^{(L)}\big).\] <p><em>Binary cross-entropy + sigmoid output (simplified)</em></p> \[\matr\Delta^{(L)} := \matr A^{(L)} - \matr Y^*.\] <p><em>For other loss functions / output actitavations other initializations apply</em></p> <p><strong>Propagate backward for hidden layers</strong></p> <p>For $\ell = L-1, L-2, \ldots, 1$:</p> \[\matr\Delta^{(\ell)} := \Big( (\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)} \Big) \ \otimes\ \sigma'\!\big(\matr Z^{(\ell)}\big).\] <hr> <p><strong>4. Gradients (full-batch)</strong></p> <p>For each layer $\ell = 1,\ldots,L$:</p> \[\frac{\partial J}{\partial \Wmatr^{(\ell)}} := \frac{1}{\batch}\;\matr\Delta^{(\ell)}\big(\matr A^{(\ell-1)}\big)^\top\] \[\frac{\partial J}{\partial \vec b^{(\ell)}} := \frac{1}{\batch}\;\matr\Delta^{(\ell)}\vec 1\] <hr> <p><strong>5. Gradient Descent Update</strong></p> <p>For each layer $\ell = 1,\ldots,L$:</p> \[\begin{align*} \Wmatr^{(\ell)} &amp;\leftarrow \Wmatr^{(\ell)} - \eta \frac{\partial J}{\partial \Wmatr^{(\ell)}}, \\ \vec b^{(\ell)} &amp;\leftarrow \vec b^{(\ell)} - \eta \frac{\partial J}{\partial \vec b^{(\ell)}}. \end{align*}\] <hr> <p><strong>6. One Full Training Step</strong></p> <p><em>Forward</em></p> \[\begin{align*} \matr A^{(0)} &amp;:= \matr X,\\ \matr Z^{(\ell)} &amp;:= \Wmatr^{(\ell)} \matr A^{(\ell-1)} + \vec b^{(\ell)}\vec 1^\top,\\ \matr A^{(\ell)} &amp;:= \sigma(\matr Z^{(\ell)}),\ \ \ell=1..L. \end{align*}\] <p><em>Backward</em></p> \[\begin{align*} \matr\Delta^{(L)} &amp;:= \text{(choose loss/activation-specific formula)},\\ \matr\Delta^{(\ell)} &amp;:= ((\Wmatr^{(\ell+1)})^\top \matr\Delta^{(\ell+1)})\otimes \sigma'(\matr Z^{(\ell)}),\ \ \ell=L-1..1. \end{align*}\] <p><em>Gradients + update</em></p> \[\begin{align*} \frac{\partial J}{\partial \Wmatr^{(\ell)}} &amp;:= \frac{1}{\batch}\matr\Delta^{(\ell)}(\matr A^{(\ell-1)})^\top,\\ \frac{\partial J}{\partial \vec b^{(\ell)}} &amp;:= \frac{1}{\batch}\matr\Delta^{(\ell)}\vec 1,\\ (\Wmatr^{(\ell)},\vec b^{(\ell)}) &amp;\leftarrow (\Wmatr^{(\ell)},\vec b^{(\ell)}) - \eta\left(\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \frac{\partial J}{\partial \vec b^{(\ell)}}\right). \end{align*}\] <hr> <h2 id="practical-considerations">Practical Considerations</h2> <p>The following topics are directly relevant when implementing and training neural networks in practice. They affect numerical stability, convergence behavior, and training efficiency.</p> <h3 id="gradient-checking-finite-differences">Gradient Checking (Finite Differences)</h3> <p>When implementing a neural network from scratch, one of the very first validation tools you should add is <strong>gradient checking</strong>. Backpropagation formulas are easy to derive on paper but surprisingly easy to implement incorrectly (sign errors, missing transposes, wrong broadcasting, etc.). Gradient checking provides a simple but powerful sanity check that your analytical gradients are correct.</p> <p>Gradient checking compares two quantities for the same parameter:</p> <ol> <li> <p><strong>Analytical gradient</strong><br> Computed via backpropagation: \(\frac{\partial J}{\partial \theta}\), where $\theta$ is a weight or bias parameter in the network.</p> </li> <li> <p><strong>Numerical gradient</strong><br> Approximated using finite differences applied directly to the scalar cost function $J(\Theta)$.</p> </li> </ol> <p>If both gradients agree (up to a small tolerance), your backpropagation implementation is very likely correct.</p> <p>Let $\theta$ be a single scalar parameter (e.g. one entry of $\Wmatr^{(\ell)}$ or $\vec b^{(\ell)}$). The numerical gradient is approximated by the central difference formula:</p> \[\frac{\partial J}{\partial \theta} \;\approx\; \frac{J(\Theta + \varepsilon\,\mathbf e_\theta) - J(\Theta - \varepsilon\,\mathbf e_\theta)} {2\varepsilon},\] <p>where:</p> <ul> <li>$\varepsilon &gt; 0$ is a small step size (e.g. $10^{-5}$),</li> <li>$\mathbf e_\theta$ denotes a perturbation that affects <em>only</em> parameter $\theta$,</li> <li>$J(\Theta)$ is the total cost of the network, i.e. \(J(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}\!\left(\vec y_n^*, \hat{\vec y}_n\right).\)</li> </ul> <p>Importantly, <strong>no backpropagation is used</strong> to compute this quantity; only forward passes.</p> <p>For a given layer $\ell$, gradient checking compares:</p> <ul> <li> <p><strong>Backprop gradient</strong> \(g_{\text{bp}}\)</p> \[\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \qquad \frac{\partial J}{\partial \vec b^{(\ell)}}\] </li> <li> <p><strong>Numerical gradient</strong> \(g_{\text{num}}\)</p> </li> </ul> \[\left(\frac{\partial J}{\partial \Wmatr^{(\ell)}}\right)^{\text{num}}, \qquad \left(\frac{\partial J}{\partial \vec b^{(\ell)}}\right)^{\text{num}}\] <p>The comparison is can be done using a <strong>relative error</strong> score:</p> \[\mathrm{rel\_err} = \frac{\lvert g_{\text{num}} - g_{\text{bp}} \rvert} {\lvert g_{\text{num}} \rvert + \lvert g_{\text{bp}} \rvert + \varepsilon_{\text{stab}}},\] <p>evaluated element-wise, where $\varepsilon_{\text{stab}}$ is a small value like $10^{-12}$ which is added to the denominator for stability reasons. A common rule of thumb for interpretation for \(\mathrm{rel\_err}\):</p> <table> <thead> <tr> <th>Relative error</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>$&lt; 10^{-7}$</td> <td>Excellent (almost certainly correct)</td> </tr> <tr> <td>$10^{-7}$ – $10^{-5}$</td> <td>Probably fine</td> </tr> <tr> <td>$10^{-5}$ – $10^{-3}$</td> <td>Suspicious</td> </tr> <tr> <td>$&gt; 10^{-3}$</td> <td>Very likely a bug</td> </tr> </tbody> </table> <p>Small absolute gradients are usually exempted from strict relative-error checks.</p> <p><strong>Practical Remarks</strong></p> <ul> <li>Gradient checking is slow: each parameter requires two forward passes. → Use it only for debugging, never during real training.</li> <li>Check one layer at a time, not the whole network at once.</li> <li>Disable regularization, dropout, and data shuffling while checking.</li> <li>Once gradient checking passes, turn it off permanently and trust backprop.</li> </ul> <hr> <p>In our example implementation, gradient checking is performed by perturbing individual entries of $\Wmatr^{(\ell)}$ or $\vec b^{(\ell)}$, recomputing the total network cost $J(\Theta)$, and comparing the resulting numerical gradients against the gradients produced by backpropagation. This clean separation (numerical gradients from forward passes only, analytical gradients from backprop) makes gradient checking a reliable correctness test.</p> <h3 id="batching-strategies">Batching Strategies</h3> <p>In practice, the cost function</p> \[J(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta)\] <p>is rarely evaluated over the entire training set at once. Instead, training data is processed in batches, leading to different optimization regimes.</p> <ul> <li> <p><strong>Full-batch gradient descent</strong><br> The gradient of $J(\Theta)$ is computed using all available training examples. This yields an exact descent direction but is computationally expensive and impractical for large datasets.</p> </li> <li> <p><strong>Mini-batch gradient descent</strong><br> The dataset is split into smaller batches of size $\batch \ll N_{\text{data}}$. Each update uses \(J_{\text{batch}}(\Theta) = \frac{1}{\batch}\sum_{n=1}^{\batch}\mathcal{L}_n(\Theta),\) providing a good trade-off between computational efficiency and gradient quality. This is the standard choice in modern deep learning.</p> </li> <li> <p><strong>Stochastic gradient descent (SGD)</strong><br> A special case of mini-batch training with $\batch = 1$. Updates are noisy but inexpensive and can help escape shallow local minima and saddle points.</p> </li> </ul> <p>All three methods share the same backpropagation equations; only the batch size $\batch$ differs. In our example below, we use the full-batch approach.</p> <h3 id="weight-initialization">Weight Initialization</h3> <p>Weight initialization plays a critical role in training neural networks. In particular, initializing all weights to zero is a bad idea, because it breaks learning entirely: if all weights in a layer start with the same value, then all neurons in that layer compute identical activations and receive identical gradients during backpropagation. As a result, they remain indistinguishable throughout training, and the network effectively behaves as if each layer had only a single neuron. Random initialization is therefore essential to break this symmetry between neurons.</p> <p>In our implementation, we use Glorot (Xavier) uniform initialization for the weights. For a layer with $d_{\ell-1}$ input units and $d_\ell$ output units, the weights are drawn from a uniform distribution</p> \[w_{i,j}^{(\ell)} \sim \mathcal{U}\!\left( -\sqrt{\frac{6}{d_{\ell-1}+d_\ell}}, \ \sqrt{\frac{6}{d_{\ell-1}+d_\ell}} \right).\] <p>This choice is motivated by the desire to keep the variance of activations and gradients approximately constant across layers, which helps to avoid vanishing or exploding signals during the forward and backward passes.</p> <p>Bias terms are initialized to zero,</p> \[\vec b^{(\ell)} = \vec 0,\] <p>which is common practice and does <em>not</em> cause symmetry issues, since symmetry is already broken by the random weight initialization.</p> <blockquote> <p><strong>Further reading.</strong><br> Other widely used initialization schemes include He initialization (for ReLU-based networks), orthogonal initialization, and data-dependent or adaptive initialization strategies. These approaches are discussed extensively in the literature.</p> </blockquote> <h3 id="learning-rate-step-size">Learning Rate (Step Size)</h3> <p>The <em>learning rate</em> $\eta&gt;0$ controls the size of the parameter update in each gradient descent step,</p> \[\Wmatr^{(\ell)} \leftarrow \Wmatr^{(\ell)} - \eta\,\frac{\partial J}{\partial \Wmatr^{(\ell)}}, \qquad \vec b^{(\ell)} \leftarrow \vec b^{(\ell)} - \eta\,\frac{\partial J}{\partial \vec b^{(\ell)}}.\] <p>Choosing an appropriate step size is crucial:</p> <ul> <li>If $\eta$ is <em>too small</em>, training progresses very slowly and may appear to stall.</li> <li>If $\eta$ is <em>too large</em>, the updates can overshoot minima, leading to oscillations or divergence of the cost function.</li> </ul> <p>In practice, $\eta$ is often treated as a hyperparameter that must be tuned experimentally. More advanced methods adapt the effective step size automatically during training; these are discussed in the further reading section.</p> <h3 id="input-normalization">Input Normalization</h3> <p>Neural networks are sensitive to the scale and distribution of input features. If different input dimensions have very different magnitudes, the optimization problem becomes poorly conditioned, which can significantly slow down training.</p> <p>A common remedy is <em>input normalization</em>, applied to the input matrix $\matr X = \matr A^{(0)}$ before training:</p> <ul> <li> <p><strong>Feature scaling</strong><br> Rescale each input feature to a comparable range (e.g. $[0,1]$), preventing some features from dominating others.</p> </li> <li> <p><strong>Zero-mean / unit-variance normalization</strong><br> Standardize each feature by subtracting its mean and dividing by its standard deviation, \(x_{i,n} \leftarrow \frac{x_{i,n} - \mu_i}{\sigma_i},\) where $\mu_i$ and $\sigma_i$ are computed over the training set.</p> </li> </ul> <p>Well-normalized inputs lead to more stable gradients and typically result in faster and more reliable convergence.</p> <h3 id="vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</h3> <p>In deep networks, gradients are propagated backward through many layers via repeated matrix multiplications and element-wise derivatives. If these factors are consistently smaller than one, gradients can <em>vanish</em>; if they are larger than one, gradients can <em>explode</em>. Both effects impair learning: vanishing gradients slow or stop updates in early layers, while exploding gradients lead to numerical instability.</p> <p>Common causes include poor weight initialization, unsuitable activation functions (e.g. sigmoid in very deep networks), and deep architectures without normalization.</p> <p><strong>Practical mitigation strategies</strong>:</p> <ul> <li>Careful weight initialization (e.g. Glorot initialization)</li> <li>Appropriate activation functions (sigmoid, as used in our example code might be a bad choice for deep nets)</li> <li>Input normalization and regularization</li> <li>Gradient clipping (to control exploding gradients)</li> </ul> <h3 id="convergence-issues">Convergence Issues</h3> <p>Even with correct gradients, optimization may behave poorly:</p> <ul> <li> <p><strong>Divergence</strong><br> Often caused by an excessively large learning rate, leading to overshooting and unstable updates.</p> </li> <li> <p><strong>Slow convergence</strong><br> Can result from poor conditioning of the optimization problem, unnormalized inputs, or an overly small learning rate.</p> </li> <li> <p><strong>Plateaus and saddle points</strong><br> Regions where gradients are close to zero but are not local minima, causing training to progress very slowly.</p> </li> </ul> <p>Understanding and diagnosing these behaviors is essential for successfully training neural networks in practice.</p> <h3 id="training-validation-and-test-data">Training, Validation, and Test Data</h3> <p>In practical machine learning workflows, the available data is typically split into three disjoint subsets:</p> <ul> <li> <p><strong>Training set</strong><br> Used to compute gradients and update the parameters $\Theta$ via backpropagation and gradient descent.</p> </li> <li> <p><strong>Validation set</strong><br> Used to monitor performance during training (e.g. for hyperparameter tuning, early stopping, or model selection) without influencing the learned parameters directly.</p> </li> <li> <p><strong>Test set</strong><br> Used only once, after all training decisions are finalized, to obtain an unbiased estimate of the model’s generalization performance.</p> </li> </ul> <p>This separation is crucial because evaluating a model on the same data used for training leads to <em>optimistically biased performance estimates</em> and can mask overfitting.</p> <p>In the example below, we deliberately train on the full dataset to keep the mathematical derivations and implementation as transparent as possible. Since the goal here is to understand <em>how</em> backpropagation works (not to assess generalization performance) this simplification is intentional. For any real-world application, however, proper train/validation/test splits are essential.</p> <h2 id="further-reading">Further Reading</h2> <p>The following topics extend (however, the list is still fairly incomplete) beyond the basic feed-forward network and are useful for deeper understanding or more advanced applications.</p> <ul> <li> <strong>Regularization</strong> <ul> <li>L2 (weight decay)</li> <li>L1 regularization</li> <li>Early stopping</li> </ul> </li> <li> <strong>Adaptive optimization methods</strong> <ul> <li>Momentum</li> <li>RMSProp</li> <li>Adam, AdamW</li> </ul> </li> <li> <strong>Advanced activation functions</strong> <ul> <li>ReLU, Leaky ReLU, ELU</li> <li>GELU, Swish</li> <li>Softmax (multiclass classification)</li> </ul> </li> <li> <strong>Loss functions</strong> <ul> <li>Binary cross-entropy</li> <li>Categorical cross-entropy</li> <li>Mean squared error (MSE)</li> <li>Mean absolute error (MAE)</li> <li>Huber loss</li> </ul> </li> <li> <strong>Numerical stability tricks</strong> <ul> <li>Log-sum-exp trick</li> <li>Stable softmax implementations</li> </ul> </li> <li><strong>Bias–variance tradeoff</strong></li> <li><strong>Overfitting vs. underfitting</strong></li> <li><strong>Initialization and symmetry breaking</strong></li> <li> <strong>Second-order methods</strong> <ul> <li>Newton’s method</li> <li>Quasi-Newton methods (L-BFGS)</li> </ul> </li> <li> <strong>Backpropagation efficiency</strong> <ul> <li>Computational graphs</li> <li>Automatic differentiation</li> </ul> </li> <li> <strong>Other neural network architectures</strong> <ul> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory (LSTM)</li> <li>Gated Recurrent Units (GRU)</li> <li>Transformers</li> </ul> </li> <li> <strong>Other training techniques</strong> <ul> <li>Batch normalization</li> <li>Layer normalization</li> <li>Dropout</li> </ul> </li> <li> <strong>Expressivity and depth</strong> <ul> <li>Shallow vs. deep networks</li> <li>Universal approximation theorem</li> </ul> </li> </ul> <p>These topics build on the foundations presented in this post and are good entry points for further study.</p> <h1 id="accompanying-jupyter-notebook-code-companion">Accompanying Jupyter Notebook (Code Companion)</h1> <style>.jupyter-child-ext{width:112%;position:relative;left:calc( - 10%)}</style> <div class="jupyter-child-ext"> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/MarkusThill.github.io-jupyter/2025_12_16_intro_neural_nets.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/online-batch-estimate-cov-mu/">Online and Batch-Incremental Estimation of Covariance Matrices and Means in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mahalanobis-distance/">The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/a-few-notes-on-the-runtime-complexity-of-latin-hypercube-sampling/">Notes on the Runtime Complexity of Latin Hypercube Sampling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mahalanobis-distance-implementations/">Implementing the Mahalanobis Distance in Python</a> </li> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Markus Thill. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a> and from <a href="https://www.freepik.com/" rel="external nofollow noopener" target="_blank">Freepik</a>. Last updated: December 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b608866baffe761c7f8f7670a3310d0f"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>